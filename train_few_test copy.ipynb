{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qin/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from CLIP.adaptercoop import CLIP_Inplanted\n",
    "from CLIP.clip import create_model\n",
    "\n",
    "from loss import FocalLoss, BinaryDiceLoss\n",
    "\n",
    "from dataset.medical_zero import MedTestDataset, MedTrainDataset\n",
    "from dataset.medical_few import MedDataset\n",
    "\n",
    "from utils import augment, cos_sim, encode_text_with_prompt_ensemble\n",
    "from prompt import REAL_NAME\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "CLASS_INDEX = {'Brain':3, 'Liver':2, 'Retina_RESC':1, 'Retina_OCT2017':-1, 'Chest':-2, 'Histopathology':-3}\n",
    "CLASS_INDEX_INV = {3:'Brain', 2:'Liver', 1:'Retina_RESC', -1:'Retina_OCT2017', -2:'Chest', -3:'Histopathology'}\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, test_loader, text_features, seg_mem_features, det_mem_features):\n",
    "    gt_list = []\n",
    "    gt_mask_list = []\n",
    "    logits_list = []\n",
    "\n",
    "    det_image_scores_zero = []\n",
    "    det_image_scores_few = []\n",
    "    \n",
    "    seg_score_map_zero = []\n",
    "    seg_score_map_few= []\n",
    "    \n",
    "    step = 0\n",
    "    for (image, y, mask) in tqdm(test_loader):\n",
    "        # step+=1\n",
    "        # if step < 300:\n",
    "        #     continue\n",
    "        image = image.to(device)\n",
    "        mask[mask > 0.5], mask[mask <= 0.5] = 1, 0\n",
    "        # print(\"mask.shape:\", mask.shape)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            _, _, seg_patch_tokens, det_patch_tokens, logits = model(image)\n",
    "            # 去掉cls token ，由于 biomedclip cls token 位置不同，此处需要对应改变\n",
    "            seg_patch_tokens = [p[:, 1:, :] for p in seg_patch_tokens]\n",
    "            det_patch_tokens = [p[:, 1:, :] for p in det_patch_tokens]\n",
    "   \n",
    "            if CLASS_INDEX[args.obj] > 0:\n",
    "\n",
    "                # few-shot, seg head\n",
    "                anomaly_maps_few_shot = []\n",
    "                for idx, p in enumerate(seg_patch_tokens):\n",
    "                    batch_cos_sim = []\n",
    "                    for b in range(p.shape[0]):\n",
    "                        cos = cos_sim(seg_mem_features[idx][b], p[b])\n",
    "                        height = int(np.sqrt(cos.shape[1]))\n",
    "                        # * 去掉cls_token\n",
    "                        anomaly_map_few_shot = torch.min((1 - cos), 0)[0].reshape(1, 1, height, height)\n",
    "                        anomaly_map_few_shot = F.interpolate(torch.tensor(anomaly_map_few_shot),\n",
    "                                                            size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                        # * 去掉\n",
    "                        batch_cos_sim.append(anomaly_map_few_shot[0].cpu().numpy())\n",
    "                        # print('batch_cos_sim.shape:', batch_cos_sim[0].shape)\n",
    "                    anomaly_maps_few_shot.append(np.stack(batch_cos_sim, axis=0))\n",
    "                    # print('anomaly_maps_few_shot.shape:', anomaly_maps_few_shot[0].shape)\n",
    "                score_map_few = np.sum(anomaly_maps_few_shot, axis=0)\n",
    "                seg_score_map_few.append(score_map_few)\n",
    "                # print('seg_score_map_few.shape:', seg_score_map_few[0].shape)\n",
    "                \n",
    "                # zero-shot, seg head\n",
    "                anomaly_maps = []\n",
    "                for layer in range(len(seg_patch_tokens)):\n",
    "                    seg_patch_tokens[layer] /= seg_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "                    # print(\"seg_patch_tokens[layer].shape:\", seg_patch_tokens[layer].shape)\n",
    "                    # print(\"text_features.shape:\", text_features.shape)\n",
    "                    \n",
    "                    anomaly_map = (100.0 * seg_patch_tokens[layer] @ text_features.t())\n",
    "                    B, L, C = anomaly_map.shape\n",
    "                    H = int(np.sqrt(L))\n",
    "                    anomaly_map = F.interpolate(anomaly_map.permute(0, 2, 1).view(B, 2, H, H),\n",
    "                                                size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                    # print('anomaly_map.shape:', anomaly_map.shape)\n",
    "                    # 4 2 224 224\n",
    "                    anomaly_map = torch.softmax(anomaly_map, dim=1)[:, 1:2, :, :]\n",
    "                    # 4 224 224 \n",
    "                    # print('anomaly_map.shape:', anomaly_map.shape)\n",
    "                    anomaly_maps.append(anomaly_map.cpu().numpy())\n",
    "                    # print('anomaly_map.shape:', anomaly_map[0].shape)\n",
    "                    # print(len(anomaly_maps))\n",
    "                    # print('anomaly_map.shape:', anomaly_maps[0].shape)\n",
    "                # print('anomaly_maps.shape:', len(anomaly_maps))\n",
    "                \n",
    "                # print('anomaly_maps[0].shape:', anomaly_maps[0].shape)\n",
    "                score_map_zero = np.sum(anomaly_maps, axis=0)\n",
    "                # print('score_map_zero.shape:', score_map_zero.shape)\n",
    "                # print('score_map_zero.shape:', score_map_zero.shape)\n",
    "                seg_score_map_zero.append(score_map_zero)\n",
    "                # print(len(seg_score_map_zero))\n",
    "                # print('seg_score_map_zero.shape:', seg_score_map_zero[0].shape)\n",
    "                # \n",
    "\n",
    "\n",
    "            else:\n",
    "                # few-shot, det head\n",
    "                anomaly_maps_few_shot = []\n",
    "                for idx, p in enumerate(seg_patch_tokens):\n",
    "                    batch_cos_sim = []\n",
    "                    for b in range(p.shape[0]):\n",
    "                        cos = cos_sim(seg_mem_features[idx][b], p[b])\n",
    "                        # print(\"cos_shape\",cos.shape)\n",
    "                        height = int(np.sqrt(cos.shape[1]))\n",
    "                        # * 提取cls——token\n",
    "                        anomaly_map_few_shot = torch.min((1 - cos), 0)[0].reshape(1, 1, height, height)\n",
    "                        # print(\"anoma\",anomaly_map_few_shot.shape)\n",
    "                        anomaly_map_few_shot = F.interpolate(torch.tensor(anomaly_map_few_shot),\n",
    "                                                            size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                        \n",
    "                        # print(\"anoma\",anomaly_map_few_shot[0].shape)\n",
    "                        #* 去除多余维度\n",
    "                        batch_cos_sim.append(anomaly_map_few_shot[0].cpu().numpy())\n",
    "                    #     print('batch_cos_sim.shape:', batch_cos_sim[0].shape)\n",
    "                    # print(\"len\",len(batch_cos_sim))\n",
    "                    # print(\"shape_batch\",batch_cos_sim[0].shape)\n",
    "                    anomaly_maps_few_shot.append(np.stack(batch_cos_sim, axis=0))\n",
    "                #     print('anomaly_maps_few_shot.shape:', len(anomaly_maps_few_shot))\n",
    "                # print(\"shape anomaly\",len(anomaly_maps_few_shot))\n",
    "                # print(\"shapt\", anomaly_maps_few_shot[0].shape)\n",
    "                \n",
    "                # anomaly_map_few_shot 4,4,1,244,244 各特征层求和\n",
    "                anomaly_map_few_shot = np.sum(anomaly_maps_few_shot, axis=0)\n",
    "\n",
    "                # anomaly_map_few_shot 4,1,244,244\n",
    "                # print(\"shape anomaly\",len(anomaly_map_few_shot))\n",
    "                # print(\"shapt\", anomaly_map_few_shot.shape)\n",
    "                \n",
    "                # \n",
    "                score_few_det = anomaly_map_few_shot.mean(axis=(1, 2,3))\n",
    "                # print('score_few_det.shape:', score_few_det.shape)\n",
    "                det_image_scores_few.append(score_few_det)\n",
    "                # print(len(det_image_scores_few))\n",
    "\n",
    "                # zero-shot, det head\n",
    "                anomaly_score = 0\n",
    "                for layer in range(len(det_patch_tokens)):\n",
    "                    det_patch_tokens[layer] /= det_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "                    anomaly_map = (100.0 * det_patch_tokens[layer] @ text_features.t())\n",
    "                    anomaly_map = torch.softmax(anomaly_map, dim=-1)[:, :, 1]\n",
    "                    # print(\"shap\",anomaly_map.shape)\n",
    "                    anomaly_score += anomaly_map.mean(dim=1)\n",
    "                    # print(\"shapt\",anomaly_map.mean(dim=1))\n",
    "                    \n",
    "                det_image_scores_zero.append(anomaly_score.cpu().numpy())\n",
    "\n",
    "            # 使用tensor将mask添加到gt_mask_list中\n",
    "            gt_mask_list.append(mask.cpu().detach().numpy())\n",
    "            gt_list.extend(y.cpu().detach().numpy())\n",
    "            \n",
    "            logits = torch.softmax(logits,dim=1)[:,1]\n",
    "            logits_list.extend(logits.cpu().detach().numpy())\n",
    "\n",
    "            \n",
    "    # 问题不在于gt_mask_list的维度，在于seg_score_map_zero的维度被缩减了\n",
    "    gt_list = np.array(gt_list)\n",
    "\n",
    "    print(gt_list.shape)\n",
    "    # print(logits_list.shape)    \n",
    "    logits_list  = np.array(logits_list)\n",
    "    \n",
    "    # roc_auc_im = roc_auc_score(gt_list, logits_list)\n",
    "    # print(\"auc\",roc_auc_im)\n",
    "    # return \n",
    "    # gt-list (932,)\n",
    "    \n",
    "    # print(\"len(gt_mask_list):\", len(gt_mask_list))\n",
    "    # print(\"shape(gt_mask_list):\", gt_mask_list[-1].shape)\n",
    "\n",
    "    # print(\"shape(gt_mask_list[-1]):\", gt_mask_list[0])\n",
    "    # print(\"shape(gt_mask_list[-1]):\", gt_mask_list[-1])\n",
    "    # print(gt_mask_list[-2])\n",
    "    # print(gt_mask_list[-1])\n",
    "    #! 最后只有一个时，维度会被压缩\n",
    "    # asarray batch_size设置使得最后的对象与前面的大小不统一会报错\n",
    "    gt_mask_list = [\n",
    "        gt_mask_list[j][i] if len(gt_mask_list[j].shape) > 2 else gt_mask_list[j]\n",
    "            for j in range(len(gt_mask_list))        # 先遍历元素索引 j\n",
    "            for i in range(gt_mask_list[j].shape[0])  # 再遍历元素内部的样本索引 i\n",
    "            ]\n",
    "    # print('gt_mask_list shape:', len(gt_mask_list))\n",
    "    # print('gt_mask_list[0].shape:', gt_mask_list[-2].shape)\n",
    "    # print('gt_mask_list[-1].shape:', gt_mask_list[-1].shape)\n",
    "\n",
    "    gt_mask_list = np.asarray(gt_mask_list)\n",
    "    gt_mask_list = (gt_mask_list>0).astype(np.int_)\n",
    "    # print('gt_mask_list shape:', gt_mask_list.shape)\n",
    "    \n",
    "    # gt_mask_list = gt_mask_list[:len_gt_mask_list]\n",
    "    # gt_mask_list.shape image_nums,batch_size,224,224\n",
    "\n",
    "    if CLASS_INDEX[args.obj] > 0:\n",
    "        # print(\"gt_mask_list shape:\", gt_mask_list.shape)\n",
    "        # print(\"seg_score_map_zero shape:\", len(seg_score_map_zero))\n",
    "        # print('seg_score_map_zero[0].shape:', seg_score_map_zero[0].shape)\n",
    "        seg_score_map_zero = [seg_score_map_zero[j][i] if len(seg_score_map_zero[j].shape) > 2 else seg_score_map_zero[j]\n",
    "            for j in range(len(seg_score_map_zero))        # 先遍历元素索引 j\n",
    "            for i in range(seg_score_map_zero[j].shape[0])  # 再遍历元素内部的样本索引 i\n",
    "            ]\n",
    "        seg_score_map_zero = np.array(seg_score_map_zero)\n",
    "        # print('seg_score_map_zero shape:', seg_score_map_zero.shape)\n",
    "        \n",
    "        seg_score_map_few = [seg_score_map_few[j][i] if len(seg_score_map_few[j].shape) > 2 else seg_score_map_few[j]\n",
    "            for j in range(len(seg_score_map_few))        # 先遍历元素索引 j\n",
    "            for i in range(seg_score_map_few[j].shape[0])  # 再遍历元素内部的样本索引 i\n",
    "            ]\n",
    "        seg_score_map_few = np.array(seg_score_map_few)\n",
    "        print(\"a\")\n",
    "\n",
    "        \n",
    "        # print(gt_mask_list.flatten())\n",
    "        # print(gt_mask_list.flatten().shape)\n",
    "        # print(seg_score_map_zero.flatten().shape)\n",
    "        # print(seg_score_map_few.flatten().shape)\n",
    "        # return\n",
    "\n",
    "        seg_score_map_zero = (seg_score_map_zero - seg_score_map_zero.min()) / (seg_score_map_zero.max() - seg_score_map_zero.min())\n",
    "        seg_score_map_few = (seg_score_map_few - seg_score_map_few.min()) / (seg_score_map_few.max() - seg_score_map_few.min())\n",
    "        segment_scores = 0.5 * seg_score_map_zero + 0.5 * seg_score_map_few\n",
    "\n",
    "        # seg_roc_auc = roc_auc_score(gt_mask_list.flatten(), segment_scores.flatten())\n",
    "        # print(f'{args.obj} pAUC : {round(seg_roc_auc,4)}')\n",
    "\n",
    "        # segment_scores size (238, 4, 1, 224, 224)\n",
    "        segment_scores_flatten = segment_scores.reshape(segment_scores.shape[0] * segment_scores.shape[1], -1)\n",
    "        # return segment_scores_flatten,gt_list\n",
    "    \n",
    "        roc_auc_im = roc_auc_score(gt_list, np.max(segment_scores_flatten, axis=1))\n",
    "        print(f'{args.obj} AUC : {round(roc_auc_im, 4)}')\n",
    "        print(f'{args.obj} AUC : {round(roc_auc_score(gt_list,logits_list), 4)}')\n",
    "        return \n",
    "        # return seg_roc_auc + roc_auc_im\n",
    "\n",
    "    else:\n",
    "        # * 多batch展平\n",
    "        # print(len(gt_mask_list))\n",
    "        # print('det_image_scores_zero shape:', len(det_image_scores_zero))\n",
    "        # print(\"shape(det_image_scores_zero):\", det_image_scores_zero[0].shape)\n",
    "        # print(\"shape(det_image_scores_zero):\", det_image_scores_zero[-1].shape)\n",
    "        # print('det_image_scores_few shape:', len(det_image_scores_few))\n",
    "        # print(\"shape(det_image_scores_few):\", det_image_scores_few[0].shape)\n",
    "        # print(\"shape(det_image_scores_few):\", det_image_scores_few[-1].shape)\n",
    "        # det_image_scores_zero = [det_image_scores_zero[j][i] if len(det_image_scores_zero[j].shape) > 2 else det_image_scores_zero[j]\n",
    "        #     for j in range(len(det_image_scores_zero))        # 先遍历元素索引 j\n",
    "        #     for i in range(det_image_scores_zero[j].shape[0])  # 再遍历元素内部的样本索引 i\n",
    "        #     ]\n",
    "        # det_image_scores_few = [det_image_scores_few[j][i] if len(det_image_scores_few[j].shape) > 2 else det_image_scores_few[j]\n",
    "        #     for j in range(len(det_image_scores_few))        # 先遍历元素索引 j\n",
    "        #     for i in range(det_image_scores_few[j].shape[0])  # 再遍历元素内部的样本索引 i\n",
    "        #     ]\n",
    "        det_image_scores_zero = np.concatenate(det_image_scores_zero)\n",
    "        det_image_scores_few = np.concatenate(det_image_scores_few)\n",
    "\n",
    "        det_image_scores_zero = np.array(det_image_scores_zero)\n",
    "        det_image_scores_few = np.array(det_image_scores_few)\n",
    "        # print(det_image_scores_few.shape)\n",
    "        # print(det_image_scores_zero.shape)\n",
    "        \n",
    "        det_image_scores_zero = (det_image_scores_zero - det_image_scores_zero.min()) / (det_image_scores_zero.max() - det_image_scores_zero.min())\n",
    "        det_image_scores_few = (det_image_scores_few - det_image_scores_few.min()) / (det_image_scores_few.max() - det_image_scores_few.min())\n",
    "    \n",
    "        image_scores = 0.5 * det_image_scores_zero + 0.5 * det_image_scores_few\n",
    "        # print(gt_list)\n",
    "        # print(\">>\",image_scores)\n",
    "        img_roc_auc_det = roc_auc_score(gt_list, image_scores)\n",
    "        print(f'{args.obj} AUC : {round(img_roc_auc_det,4)}')\n",
    "\n",
    "        return img_roc_auc_det\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Context vectors shape:  torch.Size([4, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coop\n",
    "parser = argparse.ArgumentParser(description='Testing')\n",
    "parser.add_argument('--model_name', type=str, default='biomedclip_local',)\n",
    "parser.add_argument('--pretrain', type=str, default='CLIP/ckpt/open_clip_pytorch_model.bin')\n",
    "parser.add_argument('--obj', type=str, default='Liver')\n",
    "parser.add_argument('--data_path', type=str, default='/root/data/')\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "parser.add_argument('--save_model', type=int, default=1)\n",
    "parser.add_argument('--save_path', type=str, default='./ckpt/few-shot/')\n",
    "parser.add_argument('--img_size', type=int, default=224)\n",
    "parser.add_argument(\"--epoch\", type=int, default=100, help=\"epochs\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.001, help=\"learning rate\")\n",
    "parser.add_argument(\"--features_list\", type=int, nargs=\"+\", default=[3,6,9,12], help=\"features used\")\n",
    "parser.add_argument('--seed', type=int, default=111)\n",
    "parser.add_argument('--shot', type=int, default=4)\n",
    "parser.add_argument('--iterate', type=int, default=0)\n",
    "args = parser.parse_args(args=['--obj', 'Brain',  '--shot', '4', '--batch_size', '4','--data_path','../MVFA-AD/data/'])\n",
    "\n",
    "setup_seed(args.seed)\n",
    "\n",
    "# fixed feature extractor\n",
    "biomedclip_model,tokenizer = create_model(model_name=args.model_name, \n",
    "                            force_image_size=args.img_size, \n",
    "                            device=device, \n",
    "                            pretrained=args.pretrain, \n",
    "                            require_pretrained=True)\n",
    "\n",
    "biomedclip_model.eval()\n",
    "\n",
    "# 模型添加适配器\n",
    "model = CLIP_Inplanted(clip_model=biomedclip_model,obj=args.obj,tokenizer=tokenizer, features=args.features_list).to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# optimizer for only adapters\n",
    "seg_optimizer = torch.optim.Adam(list(model.seg_adapters.parameters()), lr=args.learning_rate, betas=(0.5, 0.999))\n",
    "det_optimizer = torch.optim.Adam(list(model.det_adapters.parameters()), lr=args.learning_rate, betas=(0.5, 0.999))\n",
    "ctx_optimizer = torch.optim.Adam([model.prompt_learner.ctx], lr=args.learning_rate, betas=(0.5, 0.999))\n",
    " \n",
    "# losses\n",
    "loss_focal = FocalLoss()\n",
    "loss_dice = BinaryDiceLoss()\n",
    "loss_bce = torch.nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_prompts = model.prompt_learner.tokenized_prompts\n",
    "em = model.clipmodel.text.transformer.embeddings.word_embeddings(tokenized_prompts)\n",
    "# tokenized_prompts.shape\n",
    "em = em.mean(dim= 1)\n",
    "em.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 4\n",
    "args.shot = 4\n",
    "args.epoch = 100\n",
    "# args.obj = 'Liver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "kwargs = {'num_workers': 16, 'pin_memory': True} if use_cuda else {}\n",
    "test_dataset = MedDataset(args.data_path, args.obj, args.img_size, args.shot, args.iterate)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "# few-shot image augmentation\n",
    "augment_abnorm_img, augment_abnorm_mask = augment(test_dataset.fewshot_abnorm_img, test_dataset.fewshot_abnorm_mask)\n",
    "augment_normal_img, augment_normal_mask = augment(test_dataset.fewshot_norm_img)\n",
    "\n",
    "augment_fewshot_img = torch.cat([augment_abnorm_img, augment_normal_img], dim=0)\n",
    "augment_fewshot_mask = torch.cat([augment_abnorm_mask, augment_normal_mask], dim=0)\n",
    "\n",
    "augment_fewshot_label = torch.cat([torch.Tensor([1] * len(augment_abnorm_img)), torch.Tensor([0] * len(augment_normal_img))], dim=0)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(augment_fewshot_img, augment_fewshot_mask, augment_fewshot_label)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "# memory bank construction\n",
    "support_dataset = torch.utils.data.TensorDataset(augment_normal_img)\n",
    "support_loader = torch.utils.data.DataLoader(support_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 929/929 [01:02<00:00, 14.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3715,)\n",
      "a\n",
      "Brain AUC : 0.5242\n",
      "Brain AUC : 0.4973\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 156\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# seg_mem_features size =>  4, (image_nums * 197, embed_size) \u001b[39;00m\n\u001b[1;32m    155\u001b[0m result \u001b[38;5;241m=\u001b[39m test(args, model, test_loader, text_features, seg_mem_features, det_mem_features)\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m>\u001b[39m \u001b[43mbest_result\u001b[49m:\n\u001b[1;32m    157\u001b[0m     best_result \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest result\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_result' is not defined"
     ]
    }
   ],
   "source": [
    "from CLIP.adaptercoop import CLIP_Inplanted\n",
    "\n",
    "\n",
    "# for epoch in range(100):\n",
    "for epoch in range(args.epoch):\n",
    "    print('epoch ', epoch, ':')\n",
    "\n",
    "    # loss_list = []\n",
    "    # for (image, gt, label) in train_loader:\n",
    "    #     image = image.to(device)\n",
    "    #     with torch.cuda.amp.autocast():\n",
    "    #         image_features,text_features, seg_patch_tokens, det_patch_tokens,logits = model(image)\n",
    "    #         # seg_patch_tokens size { [batch_size,196,512] * 4} \n",
    "    #         seg_patch_tokens = [p[:, 1:, :] for p in seg_patch_tokens]\n",
    "    #         det_patch_tokens = [p[:, 1:, :] for p in det_patch_tokens]\n",
    "    #         # print(logits)\n",
    "    #         # det loss\n",
    "    #         det_loss = 0\n",
    "    #         image_label = label.to(device)\n",
    "    #         for layer in range(len(det_patch_tokens)):\n",
    "    #             det_patch_tokens[layer] = det_patch_tokens[layer] / det_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "    #             anomaly_map = (100.0 * det_patch_tokens[layer] @ text_features.t())    \n",
    "    #             anomaly_map = torch.softmax(anomaly_map, dim=-1)[:, :, 1]\n",
    "    #             anomaly_score = torch.mean(anomaly_map, dim=-1)\n",
    "    #             det_loss += loss_bce(anomaly_score, image_label)\n",
    "    #         # print(\"det_loss\",det_loss)\n",
    "            \n",
    "    #         loss_ce = F.cross_entropy(logits,image_label.long())\n",
    "    #         # print(\"loss_ce\",loss_ce)\n",
    "            \n",
    "    #         # Now calculate the frozen pre-trained features\n",
    "    #         fixed_embeddings =  model.prompt_learner.fixed_embeddings # precomputed pre-trained frozen textual features\n",
    "    #         fixed_embeddings = fixed_embeddings / fixed_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    #         zero_shot_features = model.prompt_learner.ZS_image_encoder(image)\n",
    "    #         zero_shot_features = zero_shot_features / zero_shot_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    #         scores = []\n",
    "    #         for i in range(fixed_embeddings.shape[1]):\n",
    "    #             temp_logits = model.logit_scale * image_features @ fixed_embeddings[:,i,:].cuda().t()\n",
    "    #             max_logits = torch.max(temp_logits, dim=1).values\n",
    "    #             sp = torch.mean(max_logits)\n",
    "    #             scores.append(sp.item())\n",
    "            \n",
    "    #         s_bar = torch.median(torch.tensor(scores))\n",
    "    #         d_bar = torch.median(torch.abs(torch.tensor(scores)-s_bar))\n",
    "    #         z = (torch.tensor(scores) - s_bar) / d_bar\n",
    "    #         tau = 1.5\n",
    "    #         mask = torch.abs((z - torch.mean(z))/torch.std(z)) <= tau\n",
    "    #         scores = torch.masked_select(torch.tensor(scores),mask)\n",
    "    #         scores = torch.tensor(scores).unsqueeze(1).unsqueeze(1).cuda()\n",
    "    #         selected_embeddings = fixed_embeddings[:,mask].mean(dim=1)\n",
    "    #         selected_embeddings = selected_embeddings / selected_embeddings.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "\n",
    "    #         fixed_embeddings = fixed_embeddings.mean(dim=1)\n",
    "    #         fixed_embeddings = fixed_embeddings / fixed_embeddings.norm(dim=-1, keepdim=True)\n",
    "    #         zero_shot_logits = model.logit_scale * zero_shot_features.cuda() @ selected_embeddings.cuda().t()\n",
    "            \n",
    "    #         loss_mse = torch.nn.MSELoss()\n",
    "    #         loss_sccm = loss_mse(text_features, fixed_embeddings.cuda()) * 1\n",
    "    #         # print(\"loss_sccm\",loss_sccm)\n",
    "            \n",
    "    #         loss_kdsp = F.kl_div(\n",
    "    #             F.log_softmax(logits, dim=1),\n",
    "    #             F.log_softmax(zero_shot_logits, dim=1),\n",
    "    #             reduction='sum',\n",
    "    #             log_target=True\n",
    "    #         ) / logits.numel()\n",
    "    #         loss_kdsp = loss_kdsp * 1\n",
    "    #         # print(\"loss_kdsp\",loss_kdsp)\n",
    "            \n",
    "\n",
    "    #         if CLASS_INDEX[args.obj] > 0:\n",
    "    #             # pixel level\n",
    "    #             seg_loss = 0\n",
    "    #             mask = gt.squeeze(0).to(device)\n",
    "    #             mask[mask > 0.5], mask[mask <= 0.5] = 1, 0\n",
    "    #             for layer in range(len(seg_patch_tokens)):\n",
    "    #                 seg_patch_tokens[layer] = seg_patch_tokens[layer] / seg_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "    #                 anomaly_map = (100.0 * seg_patch_tokens[layer] @ text_features.t())\n",
    "    #                 B, L, C = anomaly_map.shape\n",
    "    #                 H = int(np.sqrt(L))\n",
    "    #                 anomaly_map = F.interpolate(anomaly_map.permute(0, 2, 1).view(B, 2, H, H),\n",
    "    #                                             size=args.img_size, mode='bilinear', align_corners=True)\n",
    "    #                 anomaly_map = torch.softmax(anomaly_map, dim=1)\n",
    "    #                 seg_loss += loss_focal(anomaly_map, mask)\n",
    "    #                 seg_loss += loss_dice(anomaly_map[:, 1, :, :], mask)\n",
    "                \n",
    "    #             # print(\"set\",seg_loss + det_loss)\n",
    "    #             # print(\"ce\",loss_ce + loss_sccm + loss_kdsp)\n",
    "    #             loss = seg_loss  + det_loss + loss_ce * 5 + loss_sccm * 5  + loss_kdsp * 5\n",
    "    #             loss.requires_grad_(True)\n",
    "    #             seg_optimizer.zero_grad()\n",
    "    #             det_optimizer.zero_grad()\n",
    "    #             ctx_optimizer.zero_grad()\n",
    "    #             loss.backward()\n",
    "    #             ctx_optimizer.step()\n",
    "    #             seg_optimizer.step()\n",
    "    #             det_optimizer.step()\n",
    "                \n",
    "    #             # if epoch < args.epoch // 2:\n",
    "    #             #     loss_2 = loss_ce  + loss_sccm + loss_kdsp \n",
    "    #             #     loss_2.requires_grad_(True)\n",
    "    #             #     ctx_optimizer.zero_grad()\n",
    "    #             #     loss_2.backward()\n",
    "    #             #     ctx_optimizer.step()\n",
    "    #             #     loss = loss_2\n",
    "    #             # else:\n",
    "    #             #     loss_1 = seg_loss  + det_loss \n",
    "    #             #     loss_1.requires_grad_(True)\n",
    "    #             #     seg_optimizer.zero_grad()\n",
    "    #             #     det_optimizer.zero_grad()\n",
    "    #             #     loss_1.backward()\n",
    "    #             #     seg_optimizer.step()\n",
    "    #             #     det_optimizer.step()\n",
    "    #             #     lose = loss_1\n",
    "    #                 # loss = loss_1 + loss_2\n",
    "    #         else:\n",
    "    #             loss = det_loss + loss_ce + loss_sccm + loss_kdsp\n",
    "    #             loss.requires_grad_(True)\n",
    "    #             det_optimizer.zero_grad()\n",
    "    #             ctx_optimizer.zero_grad()\n",
    "    #             loss.backward()\n",
    "    #             ctx_optimizer.step()\n",
    "    #             det_optimizer.step()\n",
    "\n",
    "    #         # print(\"epoch {} over\", epoch)\n",
    "    #         loss_list.append(loss.item())\n",
    "    # print(\"Loss: \", np.mean(loss_list))\n",
    "\n",
    "\n",
    "\n",
    "    seg_features = []\n",
    "    det_features = []\n",
    "    for image in support_loader:\n",
    "        #batch_size = 4 :  { [4,3,224,224]  }\n",
    "        image = image[0].to(device)\n",
    "        with torch.no_grad():\n",
    "            _,text_features, seg_patch_tokens, det_patch_tokens,logits = model(image)\n",
    "            #? seg_patch_tokens size { [batch_size,197,512] * 4}\n",
    "            \n",
    "            #! 0 -> : , 仅改变batch_size维度， 不会改变其他维度\n",
    "            seg_patch_tokens = [p.contiguous() for p in seg_patch_tokens]\n",
    "            det_patch_tokens = [p.contiguous() for p in det_patch_tokens]\n",
    "            seg_features.append(seg_patch_tokens)\n",
    "            det_features.append(det_patch_tokens)\n",
    "    # batch_size = 1时， seg_features  image_nums， 4 ， [197,embed_size]\n",
    "    # batch_size = 2时， seg_features  {image_nums * { 4 * [2 ,197,embed_size] }  }\n",
    "    #! batch_size > 1 时， seg_features 维度会缩减！\n",
    "    seg_mem_features = [torch.cat([seg_features[j][i].view(-1,seg_features[j][i].shape[-2],seg_features[j][i].shape[-1]) for j in range(len(seg_features))], dim=0) for i in range(len(seg_features[0]))]\n",
    "    det_mem_features = [torch.cat([det_features[j][i].view(-1,det_features[j][i].shape[-2],det_features[j][i].shape[-1]) for j in range(len(det_features))], dim=0) for i in range(len(det_features[0]))]\n",
    "    # seg_mem_features size =>  4, (image_nums * 197, embed_size) \n",
    "    \n",
    "    result = test(args, model, test_loader, text_features, seg_mem_features, det_mem_features)\n",
    "    if result > best_result:\n",
    "        best_result = result\n",
    "        print(\"Best result\\n\")\n",
    "        if args.save_model == 1:\n",
    "            ckp_path = os.path.join(args.save_path, f'{args.obj}.pth')\n",
    "            torch.save({'seg_adapters': model.seg_adapters.state_dict(),\n",
    "                        'det_adapters': model.det_adapters.state_dict()}, \n",
    "                        ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logits_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mlogits_list\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logits_list' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 929/929 [01:26<00:00, 10.80it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "seg_features = []\n",
    "det_features = []\n",
    "for image in support_loader:\n",
    "    #batch_size = 4 :  { [4,3,224,224]  }\n",
    "    image = image[0].to(device)\n",
    "    with torch.no_grad():\n",
    "        _,_, seg_patch_tokens, det_patch_tokens,_ = model(image)\n",
    "        #? seg_patch_tokens size { [batch_size,197,512] * 4}\n",
    "        \n",
    "        #! 0 -> : , 仅改变batch_size维度， 不会改变其他维度\n",
    "        seg_patch_tokens = [p.contiguous() for p in seg_patch_tokens]\n",
    "        det_patch_tokens = [p.contiguous() for p in det_patch_tokens]\n",
    "        seg_features.append(seg_patch_tokens)\n",
    "        det_features.append(det_patch_tokens)\n",
    "# batch_size = 1时， seg_features  image_nums， 4 ， [197,embed_size]\n",
    "# batch_size = 2时， seg_features  {image_nums * { 4 * [2 ,197,embed_size] }  }\n",
    "#! batch_size > 1 时， seg_features 维度会缩减！\n",
    "seg_mem_features = [torch.cat([seg_features[j][i].view(-1,seg_features[j][i].shape[-2],seg_features[j][i].shape[-1]) for j in range(len(seg_features))], dim=0) for i in range(len(seg_features[0]))]\n",
    "det_mem_features = [torch.cat([det_features[j][i].view(-1,det_features[j][i].shape[-2],det_features[j][i].shape[-1]) for j in range(len(det_features))], dim=0) for i in range(len(det_features[0]))]\n",
    "# seg_mem_features size =>  4, (image_nums * 197, embed_size) \n",
    "\n",
    "\n",
    "result = test(args, model, test_loader, text_features, seg_mem_features, det_mem_features)\n",
    "if result > 0:\n",
    "    best_result = result\n",
    "    print(\"Best result\\n\")\n",
    "    if args.save_model == 1:\n",
    "        ckp_path = os.path.join(args.save_path, f'{args.obj}.pth')\n",
    "        torch.save({'seg_adapters': model.seg_adapters.state_dict(),\n",
    "                    'det_adapters': model.det_adapters.state_dict()}, \n",
    "                    ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([ 5.1199e-01,  6.7217e-02, -6.2822e-01,  2.0367e-01,  3.5700e-02,\n",
       "           1.2200e-02, -1.7521e-01,  4.4324e-04, -6.8688e-02, -1.1810e-01,\n",
       "          -3.5943e-02, -1.3074e-02,  5.2857e-02, -2.8560e-01,  1.5273e-01,\n",
       "          -2.8706e-01,  8.2016e-02, -1.2833e-01,  4.5680e-01,  3.5889e-01,\n",
       "          -1.1988e-01, -1.2642e-01, -1.9967e-01,  2.5266e-01,  3.5789e-01,\n",
       "          -1.8242e-02,  3.3421e-02, -3.7464e-01, -1.7979e-01,  2.7704e-01,\n",
       "           2.5598e-01,  2.5221e-01,  1.6680e-01,  4.9888e-02, -1.0321e-01,\n",
       "           2.2811e-01,  3.0461e-02,  3.0009e-02, -8.6265e-02,  7.8788e-02,\n",
       "           1.2989e-01, -1.0888e-01, -1.2430e-01, -4.4398e-02,  4.5020e-01,\n",
       "           2.6943e-01, -1.8908e-01,  7.7852e-02, -3.0184e-01, -1.7944e-01,\n",
       "           1.3145e-01,  1.8982e-01, -1.8014e-02,  2.6812e-01,  1.1116e-01,\n",
       "           1.8091e-01, -1.4851e-01,  7.7178e-02,  7.3749e-02,  1.7807e-01,\n",
       "          -2.0870e-01,  2.7113e-03,  3.7070e-01,  2.3924e-01,  2.9215e-01,\n",
       "          -1.9263e-03,  7.5360e-02,  9.1701e-02,  1.1798e-01, -3.8894e-02,\n",
       "           9.3340e-02,  2.8528e-01,  2.3927e-01,  1.1966e-02,  2.1036e-01,\n",
       "           6.3470e-03, -1.1353e-01,  2.0703e-01, -1.0335e-01, -1.5467e-01,\n",
       "           3.3455e-01, -1.4482e-01,  1.2664e-01, -3.2243e-01,  7.7489e-03,\n",
       "           2.2305e-01,  2.3832e-01, -2.3618e-01, -3.2453e-01,  1.3344e-01,\n",
       "           2.4659e-01, -1.7344e-03, -8.2906e-02,  1.6583e-01,  1.8659e-01,\n",
       "           8.1166e-02,  1.0294e-01, -3.1155e-01,  1.7839e+00,  8.6899e-02,\n",
       "           7.2251e-02, -1.7992e-01, -5.0660e-01,  5.6370e-02, -2.4217e-01,\n",
       "           3.4459e-01,  2.0594e-01, -4.3753e-02, -1.0069e-01, -1.7036e-01,\n",
       "          -1.6493e-01,  2.7466e-01,  5.7009e-02,  8.5543e-02, -1.8079e-01,\n",
       "           7.2932e-03,  3.3404e-02, -1.3353e-01,  1.9937e-01,  1.9785e-01,\n",
       "           5.2704e-02, -3.0257e-01, -1.1293e-01,  5.3753e-03, -5.9776e-02,\n",
       "          -2.4693e-01,  2.2928e-01, -3.4245e-01,  2.1027e-01, -3.4180e-01,\n",
       "           2.1754e-01,  2.7348e-02,  1.4005e-03,  2.6408e-01, -2.5132e-01,\n",
       "          -2.2898e-01,  1.4256e-01,  5.8775e-03, -1.6325e-01,  6.9918e-02,\n",
       "          -3.6745e-02, -1.4522e-02,  2.8980e-01, -2.7921e-02,  1.5737e-01,\n",
       "          -1.9639e-01, -1.9266e-01, -2.7696e-01,  1.7241e-01, -4.3368e-01,\n",
       "          -2.2806e-01,  2.2295e-02, -3.3013e-01,  1.2354e-01, -4.2327e-02,\n",
       "          -2.9068e-01,  6.3014e-02,  2.4697e-01,  9.3234e-03, -1.3259e-01,\n",
       "          -3.5415e-02, -2.0506e-03,  2.5672e-01, -1.8524e-01, -1.8568e-03,\n",
       "           1.3712e-01,  7.4625e-02, -2.1933e-02,  2.2335e-01,  1.7142e-01,\n",
       "          -3.5628e-01,  6.8933e-02,  9.1631e-02, -1.9857e-01, -1.9137e-01,\n",
       "           6.3319e-02, -1.7657e-01,  2.6075e-01, -8.5149e-02, -1.0594e-01,\n",
       "           1.1396e-01, -1.8425e-01, -7.1094e-02, -1.1425e-01,  3.4482e-01,\n",
       "           5.8817e-02, -1.0522e-01,  1.0239e-01, -1.4164e-01,  1.1239e-03,\n",
       "          -2.9062e-01,  3.7796e-01, -4.7504e-01, -2.4936e-01, -1.5992e-01,\n",
       "           2.4897e-01, -4.2938e-01, -9.5678e-02, -1.2342e-01, -2.8082e-02,\n",
       "          -2.4457e-01,  1.1679e-03,  1.2541e-01,  1.1309e-01,  1.7459e-01,\n",
       "          -5.4428e-02, -1.8425e-01,  8.9083e-02,  1.7859e-01,  3.5044e-01,\n",
       "          -1.4093e-01, -7.3836e-02, -1.8842e-01,  1.0473e-01,  1.1690e-01,\n",
       "           2.2390e-01,  2.6289e-01, -1.3752e-01, -2.0178e-01, -7.2094e-02,\n",
       "           2.2588e-01, -9.7305e-02,  1.9384e-01,  1.3331e-01,  2.9979e-01,\n",
       "          -2.1996e-01, -1.3837e-01, -8.6046e-02, -3.5572e-02,  8.3026e-02,\n",
       "          -3.7732e-01, -2.6070e-01, -7.1706e-02,  2.5786e-01, -7.4200e-02,\n",
       "          -1.7178e-01,  1.1079e-01,  7.7694e-02, -2.7876e-01, -2.3120e-01,\n",
       "          -2.6540e-02,  2.2528e+00, -5.8911e-02, -4.7647e-02,  1.7766e-02,\n",
       "           5.0872e-02,  1.2243e-01, -2.8358e-01, -5.8081e-02, -5.9568e-02,\n",
       "          -4.9095e-01,  9.5258e-02,  3.9939e-01, -3.3053e-01,  1.1624e-01,\n",
       "           7.4713e-02, -2.2068e-01,  3.9781e-01, -2.2691e-02,  2.0983e-01,\n",
       "          -3.5292e-01,  6.4390e-02,  1.3993e-01,  1.6917e-01, -9.5128e-01,\n",
       "          -8.9082e-02, -8.9976e-02,  1.4344e-01, -1.0179e-01,  1.4457e-01,\n",
       "           1.1434e-01,  3.8826e-01, -5.1193e-03, -7.2353e-02,  3.1514e-01,\n",
       "           6.9131e-01, -1.7789e-01, -1.2838e-01,  2.2159e-01, -4.9201e-02,\n",
       "          -1.7699e-01,  2.2713e-02, -1.9996e-01,  6.2268e-02,  5.7276e-02,\n",
       "           2.8217e-02,  1.2388e-01, -7.9870e-02, -9.3750e-02,  2.2735e-02,\n",
       "           3.2727e-01, -2.5552e-01,  2.5481e-01,  3.0350e-02,  1.0637e-01,\n",
       "           1.1261e-01, -1.7137e-01,  2.5317e-01,  1.7071e-01, -7.8992e-02,\n",
       "           2.0338e-01, -7.4795e-03,  5.6608e-01, -6.3063e-02,  1.1983e-01,\n",
       "          -3.8464e-01,  1.2004e-01, -7.8010e-02,  4.1221e-01, -1.6460e-01,\n",
       "          -3.3751e-01,  4.6604e-02,  3.6122e-01, -1.7714e-02,  6.2197e-02,\n",
       "          -2.3326e-02, -1.8835e-01,  4.0574e-02,  6.3516e-02, -1.2420e-01,\n",
       "          -1.4674e-01,  1.9444e-01, -9.1703e-02, -3.7505e-01,  4.4699e-02,\n",
       "           3.0831e-01, -8.2783e-02, -5.7083e-02,  8.3300e-02, -3.7451e-02,\n",
       "          -5.6810e-01,  3.3395e-01, -1.6767e-01,  6.6744e-01,  3.6663e-01,\n",
       "          -3.7911e-02, -2.4347e-01, -6.3793e-02, -2.1815e-02, -1.7477e-01,\n",
       "           8.1840e-02,  8.6021e-02,  1.7668e-01,  1.9756e-01, -2.5598e-01,\n",
       "          -1.1381e-01, -3.4735e-01,  1.2064e-01, -4.9369e-03,  3.7298e-01,\n",
       "           6.6049e-02,  1.7251e-01, -1.3744e-01,  2.2471e-01,  6.2586e-02,\n",
       "           3.3100e-01,  1.1605e-01, -7.6892e-02, -1.1532e-01,  2.6284e-02,\n",
       "          -1.9663e-01,  1.3052e-01, -3.9807e-02,  2.4890e-01, -1.4221e-01,\n",
       "          -6.9413e-03, -9.4677e-02,  6.7206e-02, -4.4246e-01,  2.0318e-01,\n",
       "          -5.4312e-02,  2.2955e-01, -1.1185e-01,  4.5975e-02, -3.0779e-01,\n",
       "          -2.0885e-01, -5.5737e-01, -2.5313e-01,  1.2569e-01, -2.0181e-01,\n",
       "          -2.5140e-01, -2.1319e-01, -2.3389e-01,  2.5118e-01,  1.0444e-01,\n",
       "          -9.3709e-02,  2.6567e-01, -9.2356e-03,  8.5706e-02,  2.9555e-02,\n",
       "          -1.0901e-03,  1.0854e-01,  1.4911e-01,  1.0911e-01,  4.6317e-03,\n",
       "          -4.5364e-01,  6.4951e-02,  5.1204e-01,  4.5070e-02,  3.5309e-01,\n",
       "          -2.6169e-01,  1.5892e-01, -1.8759e-01,  8.0931e-02, -1.5673e-01,\n",
       "           3.1781e-01,  2.4186e-01,  1.4195e-01, -5.7960e-02, -2.1879e-01,\n",
       "          -3.9357e-02,  2.9624e-01,  6.9045e-02,  4.2252e-02, -1.3590e-01,\n",
       "          -2.0413e-01, -2.9312e-01, -1.0235e-01,  1.8207e-02, -2.2969e-01,\n",
       "           4.6641e-01, -9.0372e-02,  3.6463e-02, -1.0181e-01,  1.4460e-01,\n",
       "          -1.8688e-01,  3.1896e-02, -4.6111e-02, -6.0641e-02,  1.0143e-01,\n",
       "          -9.1146e-02,  9.6946e-02,  2.2511e-01,  6.5458e-02,  3.0277e-02,\n",
       "           2.5575e-01, -1.4143e-03, -6.1967e-02,  1.6394e-01,  2.6592e-01,\n",
       "          -7.1706e-02,  1.2125e-01,  6.1986e-02, -5.4316e-02, -2.9374e-01,\n",
       "          -2.5845e-02,  3.6862e-02,  8.3491e-03,  1.1467e-01,  4.3105e-01,\n",
       "          -2.5261e-01, -8.2594e-02, -4.5061e-02, -3.9239e-02, -6.1770e-02,\n",
       "          -8.9522e-02,  1.6852e-01,  7.5903e-02, -6.2312e-02,  1.7686e-02,\n",
       "          -5.5122e-02, -1.3073e-01, -1.5270e-01, -5.3549e-02,  5.3696e-02,\n",
       "          -1.7208e-01,  4.1693e-01, -1.0629e-01,  2.4776e-01, -8.4640e-02,\n",
       "          -3.8824e-01,  2.6061e-01, -1.5364e-01, -3.6149e-02,  1.2991e-01,\n",
       "           1.6077e-01,  7.8555e-03, -1.7734e-01, -2.5661e-02, -2.8434e-02,\n",
       "           1.4266e-01,  6.3670e-02,  3.5782e-01,  3.5624e-01, -1.4273e-01,\n",
       "           4.0379e-01, -1.6081e-02,  3.6598e-03, -2.1655e-01, -2.2918e-01,\n",
       "           1.0139e-01,  1.5702e-01,  4.5970e-02, -2.2864e-01, -1.5470e-02,\n",
       "          -1.3869e-01,  7.9072e-02, -1.8118e-02, -1.2263e-01, -3.7886e-02,\n",
       "          -2.2483e-01,  3.2867e-01, -3.7717e-01,  3.4737e-02, -5.8808e-02,\n",
       "          -9.2526e-02, -1.8681e-01,  4.8722e-02, -2.5632e-01,  2.3552e-01,\n",
       "          -1.9595e-01,  1.7809e-01], device='cuda:0'),\n",
       "  tensor([ 5.2274e-01,  1.5641e-01, -1.0848e+00,  1.1177e-01,  7.5653e-02,\n",
       "           9.4285e-02, -1.8001e-01,  2.5528e-02, -2.4492e-01, -1.7891e-01,\n",
       "          -4.5921e-02,  2.5578e-02,  1.8064e-01, -4.5970e-01,  6.3076e-02,\n",
       "          -3.7772e-01,  1.5441e-01, -2.4951e-01,  4.7971e-01,  3.5050e-01,\n",
       "          -2.6991e-01, -9.2889e-02, -1.7606e-01,  1.7152e-01,  2.3409e-01,\n",
       "          -7.8755e-02, -4.0918e-02, -3.0925e-01, -1.6300e-01,  3.8787e-01,\n",
       "           2.5585e-01,  3.3782e-01,  1.5435e-01,  7.4610e-02, -2.3172e-02,\n",
       "           1.3508e-01,  2.6527e-02, -4.3903e-02, -6.8146e-02,  2.9305e-01,\n",
       "           2.3502e-02, -1.9274e-01, -7.0365e-02, -3.2197e-01,  5.4049e-01,\n",
       "           3.2196e-01, -2.7936e-01,  7.5554e-02, -1.5153e-01, -2.3615e-01,\n",
       "           2.3531e-01,  1.5285e-01, -2.6181e-01,  1.7080e-01,  2.8013e-01,\n",
       "           1.7029e-01, -1.4272e-02,  1.2108e-02, -1.6482e-01,  8.8154e-02,\n",
       "          -2.0103e-01, -1.1609e-01,  4.7431e-01,  3.1184e-01,  4.3271e-01,\n",
       "           9.0316e-02,  1.8462e-01,  1.6195e-01,  6.8520e-02, -1.0234e-01,\n",
       "          -9.5794e-02,  4.6709e-01,  4.8865e-01, -5.3075e-02,  1.7724e-01,\n",
       "          -1.0268e-01,  2.9243e-02,  3.3458e-01, -1.4299e-01, -1.4205e-01,\n",
       "           5.5196e-01, -1.6875e-01,  1.9743e-01, -2.6752e-01, -1.1521e-02,\n",
       "           1.7574e-01,  6.4551e-02, -5.0148e-01, -3.4393e-01,  2.6587e-01,\n",
       "           4.6449e-01, -5.0084e-04,  8.8365e-03,  2.4609e-01,  2.6972e-01,\n",
       "           8.8671e-05,  1.6340e-01, -3.4826e-01,  2.7051e+00,  2.1946e-01,\n",
       "           1.7937e-01, -2.9697e-01, -4.5259e-01,  2.4084e-01, -2.9354e-01,\n",
       "           4.4238e-01,  3.2129e-01, -7.7020e-02, -3.8553e-02, -1.4583e-01,\n",
       "          -3.7871e-01,  2.1882e-01,  1.4687e-01,  9.9550e-02, -1.0327e-01,\n",
       "           4.0365e-02,  1.8728e-01, -1.0975e-01,  4.5076e-01,  2.5217e-01,\n",
       "           1.8275e-01, -3.9115e-01, -3.5903e-01, -1.4331e-01, -1.1253e-01,\n",
       "          -2.5967e-01,  2.5023e-01, -4.1895e-01,  1.8815e-01, -2.4978e-01,\n",
       "           4.5059e-01,  1.3987e-01, -1.1408e-01,  2.5577e-01, -1.5558e-01,\n",
       "          -2.2749e-01,  2.1994e-01, -2.3140e-01, -4.2276e-01,  1.9363e-01,\n",
       "          -1.8481e-01, -9.6807e-03,  2.1645e-01,  1.5031e-01,  2.4152e-01,\n",
       "          -4.2557e-01, -3.0346e-01, -2.2863e-01,  2.3039e-01, -6.4430e-01,\n",
       "          -2.1962e-01, -7.2523e-02, -3.3888e-01,  4.6646e-02, -3.4023e-02,\n",
       "          -1.6126e-01,  1.8565e-01,  2.4995e-01, -7.5664e-02, -1.1935e-01,\n",
       "          -4.3957e-02,  8.8616e-02,  2.8147e-01, -1.9149e-01,  5.2789e-02,\n",
       "           1.3602e-01, -5.7536e-02, -1.0664e-01,  2.9895e-01,  9.0617e-02,\n",
       "          -2.4501e-01,  6.2611e-02,  9.3418e-02, -3.0951e-01, -1.2341e-01,\n",
       "           3.8067e-02, -1.7876e-01,  4.6486e-01, -1.7777e-01, -7.2212e-02,\n",
       "           3.2726e-01, -3.2004e-01, -2.6162e-02, -1.7617e-01,  4.4617e-01,\n",
       "          -3.3485e-02, -8.0312e-02,  1.2325e-01, -1.3496e-01,  2.0807e-01,\n",
       "          -6.2653e-01,  3.0064e-01, -2.3005e-01, -2.4519e-01, -1.9420e-01,\n",
       "           4.5950e-01, -6.5656e-01, -7.4801e-02, -1.3110e-01, -1.3971e-01,\n",
       "          -3.7797e-01,  5.9950e-02, -1.9938e-02,  1.0263e-01,  2.8390e-01,\n",
       "          -3.7863e-02, -1.5375e-01,  1.1312e-01,  2.1486e-01,  3.8871e-01,\n",
       "          -2.5118e-01,  1.1082e-01, -2.9860e-01, -6.2103e-02,  2.1795e-02,\n",
       "           2.4248e-01,  3.7907e-01, -1.0378e-01, -1.7678e-01, -7.0341e-02,\n",
       "           2.8019e-01, -1.2278e-01,  2.2996e-01,  1.3770e-01,  4.2115e-01,\n",
       "          -3.6226e-01, -6.6510e-01, -1.9985e-02, -1.3854e-01,  1.9808e-01,\n",
       "          -6.1745e-01, -3.9260e-01, -7.5302e-02,  2.5463e-01, -6.3912e-02,\n",
       "          -1.0777e-01,  2.0373e-01,  1.7897e-02, -5.0587e-01, -2.4789e-01,\n",
       "           1.0027e-01,  3.4695e+00, -1.2569e-01, -7.9449e-02,  8.5338e-02,\n",
       "           4.5396e-02,  1.8641e-01, -4.3182e-01, -2.8186e-02, -6.3559e-01,\n",
       "          -6.4887e-01, -6.9457e-02,  4.8287e-01, -5.8451e-01,  2.1489e-01,\n",
       "           1.6621e-01, -6.8065e-02,  4.4866e-01, -3.4670e-02,  3.2352e-01,\n",
       "          -3.7933e-01,  1.3323e-02,  3.2149e-01,  2.9157e-01, -1.3252e+00,\n",
       "          -4.1968e-03, -3.6521e-02,  2.1551e-01, -2.0766e-01,  3.7599e-01,\n",
       "           2.2564e-01,  5.2119e-01, -4.4355e-02, -1.8952e-01,  2.5574e-01,\n",
       "           1.1558e+00, -1.8362e-01, -2.5785e-01,  3.3655e-01, -5.9746e-03,\n",
       "          -2.3840e-01,  4.8814e-02, -3.9995e-01, -5.1703e-02,  9.0431e-02,\n",
       "          -1.4829e-01,  2.6324e-01, -2.1299e-01, -2.0745e-01,  2.1008e-01,\n",
       "           4.3979e-01, -4.1771e-01,  1.6865e-01, -1.4575e-03,  1.6424e-01,\n",
       "          -2.7560e-02, -2.9673e-01,  4.4356e-01,  3.8644e-01, -1.7177e-02,\n",
       "           3.0430e-01, -1.3036e-01,  5.8990e-01, -2.6672e-02,  1.3839e-01,\n",
       "          -2.1776e-01,  2.5886e-01, -1.0365e-01,  5.0708e-01,  7.1468e-02,\n",
       "          -2.8835e-01,  9.0630e-03,  5.4950e-01,  1.3183e-02,  7.8507e-02,\n",
       "          -2.4742e-02, -2.8229e-01,  6.9248e-02,  1.3588e-01, -8.1964e-02,\n",
       "          -1.2618e-01,  1.7700e-01, -1.8295e-01, -3.7349e-01, -3.0257e-02,\n",
       "           4.9901e-01, -4.6310e-02, -1.0667e-01,  2.5187e-01, -2.4974e-01,\n",
       "          -1.0153e+00,  2.5389e-01, -1.6947e-01,  9.5279e-01,  2.2441e-01,\n",
       "           1.8631e-02, -3.5385e-01, -3.7518e-03, -1.3832e-03, -1.4773e-01,\n",
       "           1.1633e-01, -1.4843e-02,  3.2812e-01,  7.6172e-02, -8.9335e-02,\n",
       "          -2.7072e-02, -3.8204e-01,  1.4398e-01, -2.2048e-01,  2.6724e-01,\n",
       "           1.7931e-01,  1.9831e-02, -3.5856e-02,  3.1362e-01, -5.6898e-02,\n",
       "           2.9256e-01,  1.3741e-01,  1.1482e-01, -2.2795e-01,  6.1906e-02,\n",
       "          -3.8382e-01,  9.9245e-02,  2.6809e-02,  1.2444e-01, -1.0488e-01,\n",
       "          -6.3618e-02, -1.7001e-01, -9.3777e-02, -4.4260e-01,  2.7408e-01,\n",
       "          -2.6038e-01,  5.7491e-02,  4.8378e-02,  2.6667e-02, -2.7939e-01,\n",
       "          -2.1347e-01, -5.7146e-01, -4.2780e-01,  3.9927e-01, -2.3433e-01,\n",
       "          -3.4036e-01, -2.6126e-01, -2.2133e-01,  3.1489e-01,  1.0589e-01,\n",
       "          -2.3049e-01,  3.3393e-01,  3.9580e-02,  6.9268e-02,  1.0786e-01,\n",
       "           4.3680e-02, -1.3661e-03,  4.0690e-02,  3.1856e-01,  4.8938e-02,\n",
       "          -4.7885e-01,  1.4441e-01,  9.0219e-01, -6.4605e-02,  3.9035e-01,\n",
       "          -1.3006e-01,  2.2442e-01, -1.6776e-01, -9.6538e-02, -3.2596e-01,\n",
       "           2.4414e-01,  1.7990e-01,  1.3210e-01, -2.7508e-02, -1.6708e-01,\n",
       "          -7.2453e-02,  3.6112e-01,  1.7543e-03,  2.1864e-01,  3.5058e-02,\n",
       "          -1.1467e-01, -2.9627e-01, -9.2460e-03,  1.7361e-02, -1.5156e-01,\n",
       "           4.3672e-01, -1.0810e-01,  2.1949e-01, -1.6031e-01,  7.9630e-02,\n",
       "          -2.6704e-01,  9.6436e-02, -9.1067e-02, -3.4006e-02, -1.0135e-01,\n",
       "          -1.3300e-01, -1.1336e-01,  2.4301e-01, -3.6827e-02, -7.1317e-03,\n",
       "           3.4798e-01,  2.6985e-03,  3.1138e-02,  1.8100e-01,  2.7475e-01,\n",
       "          -6.9054e-02, -6.8759e-02, -7.0484e-02,  6.8572e-02, -3.5036e-01,\n",
       "           2.1605e-02, -2.0104e-02, -1.4014e-03,  1.6381e-01,  5.8007e-01,\n",
       "          -5.6445e-01, -3.6321e-02, -1.2807e-01, -2.9857e-02, -1.2163e-01,\n",
       "          -1.0122e-01,  1.7037e-01,  1.1601e-01, -8.2954e-02,  2.6946e-03,\n",
       "          -1.1136e-01, -8.7299e-02, -2.0346e-01, -1.9766e-01, -2.6130e-02,\n",
       "          -4.3175e-01,  4.3006e-01, -1.5023e-01,  2.6643e-01,  1.0258e-01,\n",
       "          -5.2727e-01,  5.2501e-01, -2.9109e-01,  1.4633e-02,  5.9105e-02,\n",
       "           1.1609e-01,  2.1430e-02, -1.7106e-01, -1.4557e-01,  5.3713e-02,\n",
       "           2.6238e-01, -4.4832e-02,  2.7818e-01,  3.5873e-01, -1.1511e-01,\n",
       "           3.5144e-01, -4.1552e-01, -6.7309e-02, -2.8997e-01, -2.2453e-01,\n",
       "           3.9258e-02,  1.5291e-01, -2.7494e-02, -1.2718e-01,  1.2670e-01,\n",
       "          -2.5984e-01, -8.0735e-02,  2.9160e-01, -4.9139e-02, -2.7177e-02,\n",
       "          -1.5466e-01,  3.1755e-01, -4.1531e-01,  5.7373e-02,  3.9311e-02,\n",
       "          -3.1150e-02, -8.1021e-02,  1.2103e-01, -3.4716e-01,  3.1155e-01,\n",
       "          -2.5113e-01,  3.3502e-01], device='cuda:0')],\n",
       " [tensor([ 5.1199e-01,  6.7217e-02, -6.2822e-01,  2.0367e-01,  3.5700e-02,\n",
       "           1.2200e-02, -1.7521e-01,  4.4324e-04, -6.8688e-02, -1.1810e-01,\n",
       "          -3.5943e-02, -1.3074e-02,  5.2857e-02, -2.8560e-01,  1.5273e-01,\n",
       "          -2.8706e-01,  8.2016e-02, -1.2833e-01,  4.5680e-01,  3.5889e-01,\n",
       "          -1.1988e-01, -1.2642e-01, -1.9967e-01,  2.5266e-01,  3.5789e-01,\n",
       "          -1.8242e-02,  3.3421e-02, -3.7464e-01, -1.7979e-01,  2.7704e-01,\n",
       "           2.5598e-01,  2.5221e-01,  1.6680e-01,  4.9888e-02, -1.0321e-01,\n",
       "           2.2811e-01,  3.0461e-02,  3.0009e-02, -8.6265e-02,  7.8788e-02,\n",
       "           1.2989e-01, -1.0888e-01, -1.2430e-01, -4.4398e-02,  4.5020e-01,\n",
       "           2.6943e-01, -1.8908e-01,  7.7852e-02, -3.0184e-01, -1.7944e-01,\n",
       "           1.3145e-01,  1.8982e-01, -1.8014e-02,  2.6812e-01,  1.1116e-01,\n",
       "           1.8091e-01, -1.4851e-01,  7.7178e-02,  7.3749e-02,  1.7807e-01,\n",
       "          -2.0870e-01,  2.7113e-03,  3.7070e-01,  2.3924e-01,  2.9215e-01,\n",
       "          -1.9263e-03,  7.5360e-02,  9.1701e-02,  1.1798e-01, -3.8894e-02,\n",
       "           9.3340e-02,  2.8528e-01,  2.3927e-01,  1.1966e-02,  2.1036e-01,\n",
       "           6.3470e-03, -1.1353e-01,  2.0703e-01, -1.0335e-01, -1.5467e-01,\n",
       "           3.3455e-01, -1.4482e-01,  1.2664e-01, -3.2243e-01,  7.7489e-03,\n",
       "           2.2305e-01,  2.3832e-01, -2.3618e-01, -3.2453e-01,  1.3344e-01,\n",
       "           2.4659e-01, -1.7344e-03, -8.2906e-02,  1.6583e-01,  1.8659e-01,\n",
       "           8.1166e-02,  1.0294e-01, -3.1155e-01,  1.7839e+00,  8.6899e-02,\n",
       "           7.2251e-02, -1.7992e-01, -5.0660e-01,  5.6370e-02, -2.4217e-01,\n",
       "           3.4459e-01,  2.0594e-01, -4.3753e-02, -1.0069e-01, -1.7036e-01,\n",
       "          -1.6493e-01,  2.7466e-01,  5.7009e-02,  8.5543e-02, -1.8079e-01,\n",
       "           7.2932e-03,  3.3404e-02, -1.3353e-01,  1.9937e-01,  1.9785e-01,\n",
       "           5.2704e-02, -3.0257e-01, -1.1293e-01,  5.3753e-03, -5.9776e-02,\n",
       "          -2.4693e-01,  2.2928e-01, -3.4245e-01,  2.1027e-01, -3.4180e-01,\n",
       "           2.1754e-01,  2.7348e-02,  1.4005e-03,  2.6408e-01, -2.5132e-01,\n",
       "          -2.2898e-01,  1.4256e-01,  5.8775e-03, -1.6325e-01,  6.9918e-02,\n",
       "          -3.6745e-02, -1.4522e-02,  2.8980e-01, -2.7921e-02,  1.5737e-01,\n",
       "          -1.9639e-01, -1.9266e-01, -2.7696e-01,  1.7241e-01, -4.3368e-01,\n",
       "          -2.2806e-01,  2.2295e-02, -3.3013e-01,  1.2354e-01, -4.2327e-02,\n",
       "          -2.9068e-01,  6.3014e-02,  2.4697e-01,  9.3234e-03, -1.3259e-01,\n",
       "          -3.5415e-02, -2.0506e-03,  2.5672e-01, -1.8524e-01, -1.8568e-03,\n",
       "           1.3712e-01,  7.4625e-02, -2.1933e-02,  2.2335e-01,  1.7142e-01,\n",
       "          -3.5628e-01,  6.8933e-02,  9.1631e-02, -1.9857e-01, -1.9137e-01,\n",
       "           6.3319e-02, -1.7657e-01,  2.6075e-01, -8.5149e-02, -1.0594e-01,\n",
       "           1.1396e-01, -1.8425e-01, -7.1094e-02, -1.1425e-01,  3.4482e-01,\n",
       "           5.8817e-02, -1.0522e-01,  1.0239e-01, -1.4164e-01,  1.1239e-03,\n",
       "          -2.9062e-01,  3.7796e-01, -4.7504e-01, -2.4936e-01, -1.5992e-01,\n",
       "           2.4897e-01, -4.2938e-01, -9.5678e-02, -1.2342e-01, -2.8082e-02,\n",
       "          -2.4457e-01,  1.1679e-03,  1.2541e-01,  1.1309e-01,  1.7459e-01,\n",
       "          -5.4428e-02, -1.8425e-01,  8.9083e-02,  1.7859e-01,  3.5044e-01,\n",
       "          -1.4093e-01, -7.3836e-02, -1.8842e-01,  1.0473e-01,  1.1690e-01,\n",
       "           2.2390e-01,  2.6289e-01, -1.3752e-01, -2.0178e-01, -7.2094e-02,\n",
       "           2.2588e-01, -9.7305e-02,  1.9384e-01,  1.3331e-01,  2.9979e-01,\n",
       "          -2.1996e-01, -1.3837e-01, -8.6046e-02, -3.5572e-02,  8.3026e-02,\n",
       "          -3.7732e-01, -2.6070e-01, -7.1706e-02,  2.5786e-01, -7.4200e-02,\n",
       "          -1.7178e-01,  1.1079e-01,  7.7694e-02, -2.7876e-01, -2.3120e-01,\n",
       "          -2.6540e-02,  2.2528e+00, -5.8911e-02, -4.7647e-02,  1.7766e-02,\n",
       "           5.0872e-02,  1.2243e-01, -2.8358e-01, -5.8081e-02, -5.9568e-02,\n",
       "          -4.9095e-01,  9.5258e-02,  3.9939e-01, -3.3053e-01,  1.1624e-01,\n",
       "           7.4713e-02, -2.2068e-01,  3.9781e-01, -2.2691e-02,  2.0983e-01,\n",
       "          -3.5292e-01,  6.4390e-02,  1.3993e-01,  1.6917e-01, -9.5128e-01,\n",
       "          -8.9082e-02, -8.9976e-02,  1.4344e-01, -1.0179e-01,  1.4457e-01,\n",
       "           1.1434e-01,  3.8826e-01, -5.1193e-03, -7.2353e-02,  3.1514e-01,\n",
       "           6.9131e-01, -1.7789e-01, -1.2838e-01,  2.2159e-01, -4.9201e-02,\n",
       "          -1.7699e-01,  2.2713e-02, -1.9996e-01,  6.2268e-02,  5.7276e-02,\n",
       "           2.8217e-02,  1.2388e-01, -7.9870e-02, -9.3750e-02,  2.2735e-02,\n",
       "           3.2727e-01, -2.5552e-01,  2.5481e-01,  3.0350e-02,  1.0637e-01,\n",
       "           1.1261e-01, -1.7137e-01,  2.5317e-01,  1.7071e-01, -7.8992e-02,\n",
       "           2.0338e-01, -7.4795e-03,  5.6608e-01, -6.3063e-02,  1.1983e-01,\n",
       "          -3.8464e-01,  1.2004e-01, -7.8010e-02,  4.1221e-01, -1.6460e-01,\n",
       "          -3.3751e-01,  4.6604e-02,  3.6122e-01, -1.7714e-02,  6.2197e-02,\n",
       "          -2.3326e-02, -1.8835e-01,  4.0574e-02,  6.3516e-02, -1.2420e-01,\n",
       "          -1.4674e-01,  1.9444e-01, -9.1703e-02, -3.7505e-01,  4.4699e-02,\n",
       "           3.0831e-01, -8.2783e-02, -5.7083e-02,  8.3300e-02, -3.7451e-02,\n",
       "          -5.6810e-01,  3.3395e-01, -1.6767e-01,  6.6744e-01,  3.6663e-01,\n",
       "          -3.7911e-02, -2.4347e-01, -6.3793e-02, -2.1815e-02, -1.7477e-01,\n",
       "           8.1840e-02,  8.6021e-02,  1.7668e-01,  1.9756e-01, -2.5598e-01,\n",
       "          -1.1381e-01, -3.4735e-01,  1.2064e-01, -4.9369e-03,  3.7298e-01,\n",
       "           6.6049e-02,  1.7251e-01, -1.3744e-01,  2.2471e-01,  6.2586e-02,\n",
       "           3.3100e-01,  1.1605e-01, -7.6892e-02, -1.1532e-01,  2.6284e-02,\n",
       "          -1.9663e-01,  1.3052e-01, -3.9807e-02,  2.4890e-01, -1.4221e-01,\n",
       "          -6.9413e-03, -9.4677e-02,  6.7206e-02, -4.4246e-01,  2.0318e-01,\n",
       "          -5.4312e-02,  2.2955e-01, -1.1185e-01,  4.5975e-02, -3.0779e-01,\n",
       "          -2.0885e-01, -5.5737e-01, -2.5313e-01,  1.2569e-01, -2.0181e-01,\n",
       "          -2.5140e-01, -2.1319e-01, -2.3389e-01,  2.5118e-01,  1.0444e-01,\n",
       "          -9.3709e-02,  2.6567e-01, -9.2356e-03,  8.5706e-02,  2.9555e-02,\n",
       "          -1.0901e-03,  1.0854e-01,  1.4911e-01,  1.0911e-01,  4.6317e-03,\n",
       "          -4.5364e-01,  6.4951e-02,  5.1204e-01,  4.5070e-02,  3.5309e-01,\n",
       "          -2.6169e-01,  1.5892e-01, -1.8759e-01,  8.0931e-02, -1.5673e-01,\n",
       "           3.1781e-01,  2.4186e-01,  1.4195e-01, -5.7960e-02, -2.1879e-01,\n",
       "          -3.9357e-02,  2.9624e-01,  6.9045e-02,  4.2252e-02, -1.3590e-01,\n",
       "          -2.0413e-01, -2.9312e-01, -1.0235e-01,  1.8207e-02, -2.2969e-01,\n",
       "           4.6641e-01, -9.0372e-02,  3.6463e-02, -1.0181e-01,  1.4460e-01,\n",
       "          -1.8688e-01,  3.1896e-02, -4.6111e-02, -6.0641e-02,  1.0143e-01,\n",
       "          -9.1146e-02,  9.6946e-02,  2.2511e-01,  6.5458e-02,  3.0277e-02,\n",
       "           2.5575e-01, -1.4143e-03, -6.1967e-02,  1.6394e-01,  2.6592e-01,\n",
       "          -7.1706e-02,  1.2125e-01,  6.1986e-02, -5.4316e-02, -2.9374e-01,\n",
       "          -2.5845e-02,  3.6862e-02,  8.3491e-03,  1.1467e-01,  4.3105e-01,\n",
       "          -2.5261e-01, -8.2594e-02, -4.5061e-02, -3.9239e-02, -6.1770e-02,\n",
       "          -8.9522e-02,  1.6852e-01,  7.5903e-02, -6.2312e-02,  1.7686e-02,\n",
       "          -5.5122e-02, -1.3073e-01, -1.5270e-01, -5.3549e-02,  5.3696e-02,\n",
       "          -1.7208e-01,  4.1693e-01, -1.0629e-01,  2.4776e-01, -8.4640e-02,\n",
       "          -3.8824e-01,  2.6061e-01, -1.5364e-01, -3.6149e-02,  1.2991e-01,\n",
       "           1.6077e-01,  7.8555e-03, -1.7734e-01, -2.5661e-02, -2.8434e-02,\n",
       "           1.4266e-01,  6.3670e-02,  3.5782e-01,  3.5624e-01, -1.4273e-01,\n",
       "           4.0379e-01, -1.6081e-02,  3.6598e-03, -2.1655e-01, -2.2918e-01,\n",
       "           1.0139e-01,  1.5702e-01,  4.5970e-02, -2.2864e-01, -1.5470e-02,\n",
       "          -1.3869e-01,  7.9072e-02, -1.8118e-02, -1.2263e-01, -3.7886e-02,\n",
       "          -2.2483e-01,  3.2867e-01, -3.7717e-01,  3.4737e-02, -5.8808e-02,\n",
       "          -9.2526e-02, -1.8681e-01,  4.8722e-02, -2.5632e-01,  2.3552e-01,\n",
       "          -1.9595e-01,  1.7809e-01], device='cuda:0'),\n",
       "  tensor([ 5.2274e-01,  1.5641e-01, -1.0848e+00,  1.1177e-01,  7.5653e-02,\n",
       "           9.4285e-02, -1.8001e-01,  2.5528e-02, -2.4492e-01, -1.7891e-01,\n",
       "          -4.5921e-02,  2.5578e-02,  1.8064e-01, -4.5970e-01,  6.3076e-02,\n",
       "          -3.7772e-01,  1.5441e-01, -2.4951e-01,  4.7971e-01,  3.5050e-01,\n",
       "          -2.6991e-01, -9.2889e-02, -1.7606e-01,  1.7152e-01,  2.3409e-01,\n",
       "          -7.8755e-02, -4.0918e-02, -3.0925e-01, -1.6300e-01,  3.8787e-01,\n",
       "           2.5585e-01,  3.3782e-01,  1.5435e-01,  7.4610e-02, -2.3172e-02,\n",
       "           1.3508e-01,  2.6527e-02, -4.3903e-02, -6.8146e-02,  2.9305e-01,\n",
       "           2.3502e-02, -1.9274e-01, -7.0365e-02, -3.2197e-01,  5.4049e-01,\n",
       "           3.2196e-01, -2.7936e-01,  7.5554e-02, -1.5153e-01, -2.3615e-01,\n",
       "           2.3531e-01,  1.5285e-01, -2.6181e-01,  1.7080e-01,  2.8013e-01,\n",
       "           1.7029e-01, -1.4272e-02,  1.2108e-02, -1.6482e-01,  8.8154e-02,\n",
       "          -2.0103e-01, -1.1609e-01,  4.7431e-01,  3.1184e-01,  4.3271e-01,\n",
       "           9.0316e-02,  1.8462e-01,  1.6195e-01,  6.8520e-02, -1.0234e-01,\n",
       "          -9.5794e-02,  4.6709e-01,  4.8865e-01, -5.3075e-02,  1.7724e-01,\n",
       "          -1.0268e-01,  2.9243e-02,  3.3458e-01, -1.4299e-01, -1.4205e-01,\n",
       "           5.5196e-01, -1.6875e-01,  1.9743e-01, -2.6752e-01, -1.1521e-02,\n",
       "           1.7574e-01,  6.4551e-02, -5.0148e-01, -3.4393e-01,  2.6587e-01,\n",
       "           4.6449e-01, -5.0084e-04,  8.8365e-03,  2.4609e-01,  2.6972e-01,\n",
       "           8.8671e-05,  1.6340e-01, -3.4826e-01,  2.7051e+00,  2.1946e-01,\n",
       "           1.7937e-01, -2.9697e-01, -4.5259e-01,  2.4084e-01, -2.9354e-01,\n",
       "           4.4238e-01,  3.2129e-01, -7.7020e-02, -3.8553e-02, -1.4583e-01,\n",
       "          -3.7871e-01,  2.1882e-01,  1.4687e-01,  9.9550e-02, -1.0327e-01,\n",
       "           4.0365e-02,  1.8728e-01, -1.0975e-01,  4.5076e-01,  2.5217e-01,\n",
       "           1.8275e-01, -3.9115e-01, -3.5903e-01, -1.4331e-01, -1.1253e-01,\n",
       "          -2.5967e-01,  2.5023e-01, -4.1895e-01,  1.8815e-01, -2.4978e-01,\n",
       "           4.5059e-01,  1.3987e-01, -1.1408e-01,  2.5577e-01, -1.5558e-01,\n",
       "          -2.2749e-01,  2.1994e-01, -2.3140e-01, -4.2276e-01,  1.9363e-01,\n",
       "          -1.8481e-01, -9.6807e-03,  2.1645e-01,  1.5031e-01,  2.4152e-01,\n",
       "          -4.2557e-01, -3.0346e-01, -2.2863e-01,  2.3039e-01, -6.4430e-01,\n",
       "          -2.1962e-01, -7.2523e-02, -3.3888e-01,  4.6646e-02, -3.4023e-02,\n",
       "          -1.6126e-01,  1.8565e-01,  2.4995e-01, -7.5664e-02, -1.1935e-01,\n",
       "          -4.3957e-02,  8.8616e-02,  2.8147e-01, -1.9149e-01,  5.2789e-02,\n",
       "           1.3602e-01, -5.7536e-02, -1.0664e-01,  2.9895e-01,  9.0617e-02,\n",
       "          -2.4501e-01,  6.2611e-02,  9.3418e-02, -3.0951e-01, -1.2341e-01,\n",
       "           3.8067e-02, -1.7876e-01,  4.6486e-01, -1.7777e-01, -7.2212e-02,\n",
       "           3.2726e-01, -3.2004e-01, -2.6162e-02, -1.7617e-01,  4.4617e-01,\n",
       "          -3.3485e-02, -8.0312e-02,  1.2325e-01, -1.3496e-01,  2.0807e-01,\n",
       "          -6.2653e-01,  3.0064e-01, -2.3005e-01, -2.4519e-01, -1.9420e-01,\n",
       "           4.5950e-01, -6.5656e-01, -7.4801e-02, -1.3110e-01, -1.3971e-01,\n",
       "          -3.7797e-01,  5.9950e-02, -1.9938e-02,  1.0263e-01,  2.8390e-01,\n",
       "          -3.7863e-02, -1.5375e-01,  1.1312e-01,  2.1486e-01,  3.8871e-01,\n",
       "          -2.5118e-01,  1.1082e-01, -2.9860e-01, -6.2103e-02,  2.1795e-02,\n",
       "           2.4248e-01,  3.7907e-01, -1.0378e-01, -1.7678e-01, -7.0341e-02,\n",
       "           2.8019e-01, -1.2278e-01,  2.2996e-01,  1.3770e-01,  4.2115e-01,\n",
       "          -3.6226e-01, -6.6510e-01, -1.9985e-02, -1.3854e-01,  1.9808e-01,\n",
       "          -6.1745e-01, -3.9260e-01, -7.5302e-02,  2.5463e-01, -6.3912e-02,\n",
       "          -1.0777e-01,  2.0373e-01,  1.7897e-02, -5.0587e-01, -2.4789e-01,\n",
       "           1.0027e-01,  3.4695e+00, -1.2569e-01, -7.9449e-02,  8.5338e-02,\n",
       "           4.5396e-02,  1.8641e-01, -4.3182e-01, -2.8186e-02, -6.3559e-01,\n",
       "          -6.4887e-01, -6.9457e-02,  4.8287e-01, -5.8451e-01,  2.1489e-01,\n",
       "           1.6621e-01, -6.8065e-02,  4.4866e-01, -3.4670e-02,  3.2352e-01,\n",
       "          -3.7933e-01,  1.3323e-02,  3.2149e-01,  2.9157e-01, -1.3252e+00,\n",
       "          -4.1968e-03, -3.6521e-02,  2.1551e-01, -2.0766e-01,  3.7599e-01,\n",
       "           2.2564e-01,  5.2119e-01, -4.4355e-02, -1.8952e-01,  2.5574e-01,\n",
       "           1.1558e+00, -1.8362e-01, -2.5785e-01,  3.3655e-01, -5.9746e-03,\n",
       "          -2.3840e-01,  4.8814e-02, -3.9995e-01, -5.1703e-02,  9.0431e-02,\n",
       "          -1.4829e-01,  2.6324e-01, -2.1299e-01, -2.0745e-01,  2.1008e-01,\n",
       "           4.3979e-01, -4.1771e-01,  1.6865e-01, -1.4575e-03,  1.6424e-01,\n",
       "          -2.7560e-02, -2.9673e-01,  4.4356e-01,  3.8644e-01, -1.7177e-02,\n",
       "           3.0430e-01, -1.3036e-01,  5.8990e-01, -2.6672e-02,  1.3839e-01,\n",
       "          -2.1776e-01,  2.5886e-01, -1.0365e-01,  5.0708e-01,  7.1468e-02,\n",
       "          -2.8835e-01,  9.0630e-03,  5.4950e-01,  1.3183e-02,  7.8507e-02,\n",
       "          -2.4742e-02, -2.8229e-01,  6.9248e-02,  1.3588e-01, -8.1964e-02,\n",
       "          -1.2618e-01,  1.7700e-01, -1.8295e-01, -3.7349e-01, -3.0257e-02,\n",
       "           4.9901e-01, -4.6310e-02, -1.0667e-01,  2.5187e-01, -2.4974e-01,\n",
       "          -1.0153e+00,  2.5389e-01, -1.6947e-01,  9.5279e-01,  2.2441e-01,\n",
       "           1.8631e-02, -3.5385e-01, -3.7518e-03, -1.3832e-03, -1.4773e-01,\n",
       "           1.1633e-01, -1.4843e-02,  3.2812e-01,  7.6172e-02, -8.9335e-02,\n",
       "          -2.7072e-02, -3.8204e-01,  1.4398e-01, -2.2048e-01,  2.6724e-01,\n",
       "           1.7931e-01,  1.9831e-02, -3.5856e-02,  3.1362e-01, -5.6898e-02,\n",
       "           2.9256e-01,  1.3741e-01,  1.1482e-01, -2.2795e-01,  6.1906e-02,\n",
       "          -3.8382e-01,  9.9245e-02,  2.6809e-02,  1.2444e-01, -1.0488e-01,\n",
       "          -6.3618e-02, -1.7001e-01, -9.3777e-02, -4.4260e-01,  2.7408e-01,\n",
       "          -2.6038e-01,  5.7491e-02,  4.8378e-02,  2.6667e-02, -2.7939e-01,\n",
       "          -2.1347e-01, -5.7146e-01, -4.2780e-01,  3.9927e-01, -2.3433e-01,\n",
       "          -3.4036e-01, -2.6126e-01, -2.2133e-01,  3.1489e-01,  1.0589e-01,\n",
       "          -2.3049e-01,  3.3393e-01,  3.9580e-02,  6.9268e-02,  1.0786e-01,\n",
       "           4.3680e-02, -1.3661e-03,  4.0690e-02,  3.1856e-01,  4.8938e-02,\n",
       "          -4.7885e-01,  1.4441e-01,  9.0219e-01, -6.4605e-02,  3.9035e-01,\n",
       "          -1.3006e-01,  2.2442e-01, -1.6776e-01, -9.6538e-02, -3.2596e-01,\n",
       "           2.4414e-01,  1.7990e-01,  1.3210e-01, -2.7508e-02, -1.6708e-01,\n",
       "          -7.2453e-02,  3.6112e-01,  1.7543e-03,  2.1864e-01,  3.5058e-02,\n",
       "          -1.1467e-01, -2.9627e-01, -9.2460e-03,  1.7361e-02, -1.5156e-01,\n",
       "           4.3672e-01, -1.0810e-01,  2.1949e-01, -1.6031e-01,  7.9630e-02,\n",
       "          -2.6704e-01,  9.6436e-02, -9.1067e-02, -3.4006e-02, -1.0135e-01,\n",
       "          -1.3300e-01, -1.1336e-01,  2.4301e-01, -3.6827e-02, -7.1317e-03,\n",
       "           3.4798e-01,  2.6985e-03,  3.1138e-02,  1.8100e-01,  2.7475e-01,\n",
       "          -6.9054e-02, -6.8759e-02, -7.0484e-02,  6.8572e-02, -3.5036e-01,\n",
       "           2.1605e-02, -2.0104e-02, -1.4014e-03,  1.6381e-01,  5.8007e-01,\n",
       "          -5.6445e-01, -3.6321e-02, -1.2807e-01, -2.9857e-02, -1.2163e-01,\n",
       "          -1.0122e-01,  1.7037e-01,  1.1601e-01, -8.2954e-02,  2.6946e-03,\n",
       "          -1.1136e-01, -8.7299e-02, -2.0346e-01, -1.9766e-01, -2.6130e-02,\n",
       "          -4.3175e-01,  4.3006e-01, -1.5023e-01,  2.6643e-01,  1.0258e-01,\n",
       "          -5.2727e-01,  5.2501e-01, -2.9109e-01,  1.4633e-02,  5.9105e-02,\n",
       "           1.1609e-01,  2.1430e-02, -1.7106e-01, -1.4557e-01,  5.3713e-02,\n",
       "           2.6238e-01, -4.4832e-02,  2.7818e-01,  3.5873e-01, -1.1511e-01,\n",
       "           3.5144e-01, -4.1552e-01, -6.7309e-02, -2.8997e-01, -2.2453e-01,\n",
       "           3.9258e-02,  1.5291e-01, -2.7494e-02, -1.2718e-01,  1.2670e-01,\n",
       "          -2.5984e-01, -8.0735e-02,  2.9160e-01, -4.9139e-02, -2.7177e-02,\n",
       "          -1.5466e-01,  3.1755e-01, -4.1531e-01,  5.7373e-02,  3.9311e-02,\n",
       "          -3.1150e-02, -8.1021e-02,  1.2103e-01, -3.4716e-01,  3.1155e-01,\n",
       "          -2.5113e-01,  3.3502e-01], device='cuda:0')],\n",
       " [tensor([ 5.1199e-01,  6.7217e-02, -6.2822e-01,  2.0367e-01,  3.5700e-02,\n",
       "           1.2200e-02, -1.7521e-01,  4.4324e-04, -6.8688e-02, -1.1810e-01,\n",
       "          -3.5943e-02, -1.3074e-02,  5.2857e-02, -2.8560e-01,  1.5273e-01,\n",
       "          -2.8706e-01,  8.2016e-02, -1.2833e-01,  4.5680e-01,  3.5889e-01,\n",
       "          -1.1988e-01, -1.2642e-01, -1.9967e-01,  2.5266e-01,  3.5789e-01,\n",
       "          -1.8242e-02,  3.3421e-02, -3.7464e-01, -1.7979e-01,  2.7704e-01,\n",
       "           2.5598e-01,  2.5221e-01,  1.6680e-01,  4.9888e-02, -1.0321e-01,\n",
       "           2.2811e-01,  3.0461e-02,  3.0009e-02, -8.6265e-02,  7.8788e-02,\n",
       "           1.2989e-01, -1.0888e-01, -1.2430e-01, -4.4398e-02,  4.5020e-01,\n",
       "           2.6943e-01, -1.8908e-01,  7.7852e-02, -3.0184e-01, -1.7944e-01,\n",
       "           1.3145e-01,  1.8982e-01, -1.8014e-02,  2.6812e-01,  1.1116e-01,\n",
       "           1.8091e-01, -1.4851e-01,  7.7178e-02,  7.3749e-02,  1.7807e-01,\n",
       "          -2.0870e-01,  2.7113e-03,  3.7070e-01,  2.3924e-01,  2.9215e-01,\n",
       "          -1.9263e-03,  7.5360e-02,  9.1701e-02,  1.1798e-01, -3.8894e-02,\n",
       "           9.3340e-02,  2.8528e-01,  2.3927e-01,  1.1966e-02,  2.1036e-01,\n",
       "           6.3470e-03, -1.1353e-01,  2.0703e-01, -1.0335e-01, -1.5467e-01,\n",
       "           3.3455e-01, -1.4482e-01,  1.2664e-01, -3.2243e-01,  7.7489e-03,\n",
       "           2.2305e-01,  2.3832e-01, -2.3618e-01, -3.2453e-01,  1.3344e-01,\n",
       "           2.4659e-01, -1.7344e-03, -8.2906e-02,  1.6583e-01,  1.8659e-01,\n",
       "           8.1166e-02,  1.0294e-01, -3.1155e-01,  1.7839e+00,  8.6899e-02,\n",
       "           7.2251e-02, -1.7992e-01, -5.0660e-01,  5.6370e-02, -2.4217e-01,\n",
       "           3.4459e-01,  2.0594e-01, -4.3753e-02, -1.0069e-01, -1.7036e-01,\n",
       "          -1.6493e-01,  2.7466e-01,  5.7009e-02,  8.5543e-02, -1.8079e-01,\n",
       "           7.2932e-03,  3.3404e-02, -1.3353e-01,  1.9937e-01,  1.9785e-01,\n",
       "           5.2704e-02, -3.0257e-01, -1.1293e-01,  5.3753e-03, -5.9776e-02,\n",
       "          -2.4693e-01,  2.2928e-01, -3.4245e-01,  2.1027e-01, -3.4180e-01,\n",
       "           2.1754e-01,  2.7348e-02,  1.4005e-03,  2.6408e-01, -2.5132e-01,\n",
       "          -2.2898e-01,  1.4256e-01,  5.8775e-03, -1.6325e-01,  6.9918e-02,\n",
       "          -3.6745e-02, -1.4522e-02,  2.8980e-01, -2.7921e-02,  1.5737e-01,\n",
       "          -1.9639e-01, -1.9266e-01, -2.7696e-01,  1.7241e-01, -4.3368e-01,\n",
       "          -2.2806e-01,  2.2295e-02, -3.3013e-01,  1.2354e-01, -4.2327e-02,\n",
       "          -2.9068e-01,  6.3014e-02,  2.4697e-01,  9.3234e-03, -1.3259e-01,\n",
       "          -3.5415e-02, -2.0506e-03,  2.5672e-01, -1.8524e-01, -1.8568e-03,\n",
       "           1.3712e-01,  7.4625e-02, -2.1933e-02,  2.2335e-01,  1.7142e-01,\n",
       "          -3.5628e-01,  6.8933e-02,  9.1631e-02, -1.9857e-01, -1.9137e-01,\n",
       "           6.3319e-02, -1.7657e-01,  2.6075e-01, -8.5149e-02, -1.0594e-01,\n",
       "           1.1396e-01, -1.8425e-01, -7.1094e-02, -1.1425e-01,  3.4482e-01,\n",
       "           5.8817e-02, -1.0522e-01,  1.0239e-01, -1.4164e-01,  1.1239e-03,\n",
       "          -2.9062e-01,  3.7796e-01, -4.7504e-01, -2.4936e-01, -1.5992e-01,\n",
       "           2.4897e-01, -4.2938e-01, -9.5678e-02, -1.2342e-01, -2.8082e-02,\n",
       "          -2.4457e-01,  1.1679e-03,  1.2541e-01,  1.1309e-01,  1.7459e-01,\n",
       "          -5.4428e-02, -1.8425e-01,  8.9083e-02,  1.7859e-01,  3.5044e-01,\n",
       "          -1.4093e-01, -7.3836e-02, -1.8842e-01,  1.0473e-01,  1.1690e-01,\n",
       "           2.2390e-01,  2.6289e-01, -1.3752e-01, -2.0178e-01, -7.2094e-02,\n",
       "           2.2588e-01, -9.7305e-02,  1.9384e-01,  1.3331e-01,  2.9979e-01,\n",
       "          -2.1996e-01, -1.3837e-01, -8.6046e-02, -3.5572e-02,  8.3026e-02,\n",
       "          -3.7732e-01, -2.6070e-01, -7.1706e-02,  2.5786e-01, -7.4200e-02,\n",
       "          -1.7178e-01,  1.1079e-01,  7.7694e-02, -2.7876e-01, -2.3120e-01,\n",
       "          -2.6540e-02,  2.2528e+00, -5.8911e-02, -4.7647e-02,  1.7766e-02,\n",
       "           5.0872e-02,  1.2243e-01, -2.8358e-01, -5.8081e-02, -5.9568e-02,\n",
       "          -4.9095e-01,  9.5258e-02,  3.9939e-01, -3.3053e-01,  1.1624e-01,\n",
       "           7.4713e-02, -2.2068e-01,  3.9781e-01, -2.2691e-02,  2.0983e-01,\n",
       "          -3.5292e-01,  6.4390e-02,  1.3993e-01,  1.6917e-01, -9.5128e-01,\n",
       "          -8.9082e-02, -8.9976e-02,  1.4344e-01, -1.0179e-01,  1.4457e-01,\n",
       "           1.1434e-01,  3.8826e-01, -5.1193e-03, -7.2353e-02,  3.1514e-01,\n",
       "           6.9131e-01, -1.7789e-01, -1.2838e-01,  2.2159e-01, -4.9201e-02,\n",
       "          -1.7699e-01,  2.2713e-02, -1.9996e-01,  6.2268e-02,  5.7276e-02,\n",
       "           2.8217e-02,  1.2388e-01, -7.9870e-02, -9.3750e-02,  2.2735e-02,\n",
       "           3.2727e-01, -2.5552e-01,  2.5481e-01,  3.0350e-02,  1.0637e-01,\n",
       "           1.1261e-01, -1.7137e-01,  2.5317e-01,  1.7071e-01, -7.8992e-02,\n",
       "           2.0338e-01, -7.4795e-03,  5.6608e-01, -6.3063e-02,  1.1983e-01,\n",
       "          -3.8464e-01,  1.2004e-01, -7.8010e-02,  4.1221e-01, -1.6460e-01,\n",
       "          -3.3751e-01,  4.6604e-02,  3.6122e-01, -1.7714e-02,  6.2197e-02,\n",
       "          -2.3326e-02, -1.8835e-01,  4.0574e-02,  6.3516e-02, -1.2420e-01,\n",
       "          -1.4674e-01,  1.9444e-01, -9.1703e-02, -3.7505e-01,  4.4699e-02,\n",
       "           3.0831e-01, -8.2783e-02, -5.7083e-02,  8.3300e-02, -3.7451e-02,\n",
       "          -5.6810e-01,  3.3395e-01, -1.6767e-01,  6.6744e-01,  3.6663e-01,\n",
       "          -3.7911e-02, -2.4347e-01, -6.3793e-02, -2.1815e-02, -1.7477e-01,\n",
       "           8.1840e-02,  8.6021e-02,  1.7668e-01,  1.9756e-01, -2.5598e-01,\n",
       "          -1.1381e-01, -3.4735e-01,  1.2064e-01, -4.9369e-03,  3.7298e-01,\n",
       "           6.6049e-02,  1.7251e-01, -1.3744e-01,  2.2471e-01,  6.2586e-02,\n",
       "           3.3100e-01,  1.1605e-01, -7.6892e-02, -1.1532e-01,  2.6284e-02,\n",
       "          -1.9663e-01,  1.3052e-01, -3.9807e-02,  2.4890e-01, -1.4221e-01,\n",
       "          -6.9413e-03, -9.4677e-02,  6.7206e-02, -4.4246e-01,  2.0318e-01,\n",
       "          -5.4312e-02,  2.2955e-01, -1.1185e-01,  4.5975e-02, -3.0779e-01,\n",
       "          -2.0885e-01, -5.5737e-01, -2.5313e-01,  1.2569e-01, -2.0181e-01,\n",
       "          -2.5140e-01, -2.1319e-01, -2.3389e-01,  2.5118e-01,  1.0444e-01,\n",
       "          -9.3709e-02,  2.6567e-01, -9.2356e-03,  8.5706e-02,  2.9555e-02,\n",
       "          -1.0901e-03,  1.0854e-01,  1.4911e-01,  1.0911e-01,  4.6317e-03,\n",
       "          -4.5364e-01,  6.4951e-02,  5.1204e-01,  4.5070e-02,  3.5309e-01,\n",
       "          -2.6169e-01,  1.5892e-01, -1.8759e-01,  8.0931e-02, -1.5673e-01,\n",
       "           3.1781e-01,  2.4186e-01,  1.4195e-01, -5.7960e-02, -2.1879e-01,\n",
       "          -3.9357e-02,  2.9624e-01,  6.9045e-02,  4.2252e-02, -1.3590e-01,\n",
       "          -2.0413e-01, -2.9312e-01, -1.0235e-01,  1.8207e-02, -2.2969e-01,\n",
       "           4.6641e-01, -9.0372e-02,  3.6463e-02, -1.0181e-01,  1.4460e-01,\n",
       "          -1.8688e-01,  3.1896e-02, -4.6111e-02, -6.0641e-02,  1.0143e-01,\n",
       "          -9.1146e-02,  9.6946e-02,  2.2511e-01,  6.5458e-02,  3.0277e-02,\n",
       "           2.5575e-01, -1.4143e-03, -6.1967e-02,  1.6394e-01,  2.6592e-01,\n",
       "          -7.1706e-02,  1.2125e-01,  6.1986e-02, -5.4316e-02, -2.9374e-01,\n",
       "          -2.5845e-02,  3.6862e-02,  8.3491e-03,  1.1467e-01,  4.3105e-01,\n",
       "          -2.5261e-01, -8.2594e-02, -4.5061e-02, -3.9239e-02, -6.1770e-02,\n",
       "          -8.9522e-02,  1.6852e-01,  7.5903e-02, -6.2312e-02,  1.7686e-02,\n",
       "          -5.5122e-02, -1.3073e-01, -1.5270e-01, -5.3549e-02,  5.3696e-02,\n",
       "          -1.7208e-01,  4.1693e-01, -1.0629e-01,  2.4776e-01, -8.4640e-02,\n",
       "          -3.8824e-01,  2.6061e-01, -1.5364e-01, -3.6149e-02,  1.2991e-01,\n",
       "           1.6077e-01,  7.8555e-03, -1.7734e-01, -2.5661e-02, -2.8434e-02,\n",
       "           1.4266e-01,  6.3670e-02,  3.5782e-01,  3.5624e-01, -1.4273e-01,\n",
       "           4.0379e-01, -1.6081e-02,  3.6598e-03, -2.1655e-01, -2.2918e-01,\n",
       "           1.0139e-01,  1.5702e-01,  4.5970e-02, -2.2864e-01, -1.5470e-02,\n",
       "          -1.3869e-01,  7.9072e-02, -1.8118e-02, -1.2263e-01, -3.7886e-02,\n",
       "          -2.2483e-01,  3.2867e-01, -3.7717e-01,  3.4737e-02, -5.8808e-02,\n",
       "          -9.2526e-02, -1.8681e-01,  4.8722e-02, -2.5632e-01,  2.3552e-01,\n",
       "          -1.9595e-01,  1.7809e-01], device='cuda:0'),\n",
       "  tensor([ 5.2274e-01,  1.5641e-01, -1.0848e+00,  1.1177e-01,  7.5653e-02,\n",
       "           9.4285e-02, -1.8001e-01,  2.5528e-02, -2.4492e-01, -1.7891e-01,\n",
       "          -4.5921e-02,  2.5578e-02,  1.8064e-01, -4.5970e-01,  6.3076e-02,\n",
       "          -3.7772e-01,  1.5441e-01, -2.4951e-01,  4.7971e-01,  3.5050e-01,\n",
       "          -2.6991e-01, -9.2889e-02, -1.7606e-01,  1.7152e-01,  2.3409e-01,\n",
       "          -7.8755e-02, -4.0918e-02, -3.0925e-01, -1.6300e-01,  3.8787e-01,\n",
       "           2.5585e-01,  3.3782e-01,  1.5435e-01,  7.4610e-02, -2.3172e-02,\n",
       "           1.3508e-01,  2.6527e-02, -4.3903e-02, -6.8146e-02,  2.9305e-01,\n",
       "           2.3502e-02, -1.9274e-01, -7.0365e-02, -3.2197e-01,  5.4049e-01,\n",
       "           3.2196e-01, -2.7936e-01,  7.5554e-02, -1.5153e-01, -2.3615e-01,\n",
       "           2.3531e-01,  1.5285e-01, -2.6181e-01,  1.7080e-01,  2.8013e-01,\n",
       "           1.7029e-01, -1.4272e-02,  1.2108e-02, -1.6482e-01,  8.8154e-02,\n",
       "          -2.0103e-01, -1.1609e-01,  4.7431e-01,  3.1184e-01,  4.3271e-01,\n",
       "           9.0316e-02,  1.8462e-01,  1.6195e-01,  6.8520e-02, -1.0234e-01,\n",
       "          -9.5794e-02,  4.6709e-01,  4.8865e-01, -5.3075e-02,  1.7724e-01,\n",
       "          -1.0268e-01,  2.9243e-02,  3.3458e-01, -1.4299e-01, -1.4205e-01,\n",
       "           5.5196e-01, -1.6875e-01,  1.9743e-01, -2.6752e-01, -1.1521e-02,\n",
       "           1.7574e-01,  6.4551e-02, -5.0148e-01, -3.4393e-01,  2.6587e-01,\n",
       "           4.6449e-01, -5.0084e-04,  8.8365e-03,  2.4609e-01,  2.6972e-01,\n",
       "           8.8671e-05,  1.6340e-01, -3.4826e-01,  2.7051e+00,  2.1946e-01,\n",
       "           1.7937e-01, -2.9697e-01, -4.5259e-01,  2.4084e-01, -2.9354e-01,\n",
       "           4.4238e-01,  3.2129e-01, -7.7020e-02, -3.8553e-02, -1.4583e-01,\n",
       "          -3.7871e-01,  2.1882e-01,  1.4687e-01,  9.9550e-02, -1.0327e-01,\n",
       "           4.0365e-02,  1.8728e-01, -1.0975e-01,  4.5076e-01,  2.5217e-01,\n",
       "           1.8275e-01, -3.9115e-01, -3.5903e-01, -1.4331e-01, -1.1253e-01,\n",
       "          -2.5967e-01,  2.5023e-01, -4.1895e-01,  1.8815e-01, -2.4978e-01,\n",
       "           4.5059e-01,  1.3987e-01, -1.1408e-01,  2.5577e-01, -1.5558e-01,\n",
       "          -2.2749e-01,  2.1994e-01, -2.3140e-01, -4.2276e-01,  1.9363e-01,\n",
       "          -1.8481e-01, -9.6807e-03,  2.1645e-01,  1.5031e-01,  2.4152e-01,\n",
       "          -4.2557e-01, -3.0346e-01, -2.2863e-01,  2.3039e-01, -6.4430e-01,\n",
       "          -2.1962e-01, -7.2523e-02, -3.3888e-01,  4.6646e-02, -3.4023e-02,\n",
       "          -1.6126e-01,  1.8565e-01,  2.4995e-01, -7.5664e-02, -1.1935e-01,\n",
       "          -4.3957e-02,  8.8616e-02,  2.8147e-01, -1.9149e-01,  5.2789e-02,\n",
       "           1.3602e-01, -5.7536e-02, -1.0664e-01,  2.9895e-01,  9.0617e-02,\n",
       "          -2.4501e-01,  6.2611e-02,  9.3418e-02, -3.0951e-01, -1.2341e-01,\n",
       "           3.8067e-02, -1.7876e-01,  4.6486e-01, -1.7777e-01, -7.2212e-02,\n",
       "           3.2726e-01, -3.2004e-01, -2.6162e-02, -1.7617e-01,  4.4617e-01,\n",
       "          -3.3485e-02, -8.0312e-02,  1.2325e-01, -1.3496e-01,  2.0807e-01,\n",
       "          -6.2653e-01,  3.0064e-01, -2.3005e-01, -2.4519e-01, -1.9420e-01,\n",
       "           4.5950e-01, -6.5656e-01, -7.4801e-02, -1.3110e-01, -1.3971e-01,\n",
       "          -3.7797e-01,  5.9950e-02, -1.9938e-02,  1.0263e-01,  2.8390e-01,\n",
       "          -3.7863e-02, -1.5375e-01,  1.1312e-01,  2.1486e-01,  3.8871e-01,\n",
       "          -2.5118e-01,  1.1082e-01, -2.9860e-01, -6.2103e-02,  2.1795e-02,\n",
       "           2.4248e-01,  3.7907e-01, -1.0378e-01, -1.7678e-01, -7.0341e-02,\n",
       "           2.8019e-01, -1.2278e-01,  2.2996e-01,  1.3770e-01,  4.2115e-01,\n",
       "          -3.6226e-01, -6.6510e-01, -1.9985e-02, -1.3854e-01,  1.9808e-01,\n",
       "          -6.1745e-01, -3.9260e-01, -7.5302e-02,  2.5463e-01, -6.3912e-02,\n",
       "          -1.0777e-01,  2.0373e-01,  1.7897e-02, -5.0587e-01, -2.4789e-01,\n",
       "           1.0027e-01,  3.4695e+00, -1.2569e-01, -7.9449e-02,  8.5338e-02,\n",
       "           4.5396e-02,  1.8641e-01, -4.3182e-01, -2.8186e-02, -6.3559e-01,\n",
       "          -6.4887e-01, -6.9457e-02,  4.8287e-01, -5.8451e-01,  2.1489e-01,\n",
       "           1.6621e-01, -6.8065e-02,  4.4866e-01, -3.4670e-02,  3.2352e-01,\n",
       "          -3.7933e-01,  1.3323e-02,  3.2149e-01,  2.9157e-01, -1.3252e+00,\n",
       "          -4.1968e-03, -3.6521e-02,  2.1551e-01, -2.0766e-01,  3.7599e-01,\n",
       "           2.2564e-01,  5.2119e-01, -4.4355e-02, -1.8952e-01,  2.5574e-01,\n",
       "           1.1558e+00, -1.8362e-01, -2.5785e-01,  3.3655e-01, -5.9746e-03,\n",
       "          -2.3840e-01,  4.8814e-02, -3.9995e-01, -5.1703e-02,  9.0431e-02,\n",
       "          -1.4829e-01,  2.6324e-01, -2.1299e-01, -2.0745e-01,  2.1008e-01,\n",
       "           4.3979e-01, -4.1771e-01,  1.6865e-01, -1.4575e-03,  1.6424e-01,\n",
       "          -2.7560e-02, -2.9673e-01,  4.4356e-01,  3.8644e-01, -1.7177e-02,\n",
       "           3.0430e-01, -1.3036e-01,  5.8990e-01, -2.6672e-02,  1.3839e-01,\n",
       "          -2.1776e-01,  2.5886e-01, -1.0365e-01,  5.0708e-01,  7.1468e-02,\n",
       "          -2.8835e-01,  9.0630e-03,  5.4950e-01,  1.3183e-02,  7.8507e-02,\n",
       "          -2.4742e-02, -2.8229e-01,  6.9248e-02,  1.3588e-01, -8.1964e-02,\n",
       "          -1.2618e-01,  1.7700e-01, -1.8295e-01, -3.7349e-01, -3.0257e-02,\n",
       "           4.9901e-01, -4.6310e-02, -1.0667e-01,  2.5187e-01, -2.4974e-01,\n",
       "          -1.0153e+00,  2.5389e-01, -1.6947e-01,  9.5279e-01,  2.2441e-01,\n",
       "           1.8631e-02, -3.5385e-01, -3.7518e-03, -1.3832e-03, -1.4773e-01,\n",
       "           1.1633e-01, -1.4843e-02,  3.2812e-01,  7.6172e-02, -8.9335e-02,\n",
       "          -2.7072e-02, -3.8204e-01,  1.4398e-01, -2.2048e-01,  2.6724e-01,\n",
       "           1.7931e-01,  1.9831e-02, -3.5856e-02,  3.1362e-01, -5.6898e-02,\n",
       "           2.9256e-01,  1.3741e-01,  1.1482e-01, -2.2795e-01,  6.1906e-02,\n",
       "          -3.8382e-01,  9.9245e-02,  2.6809e-02,  1.2444e-01, -1.0488e-01,\n",
       "          -6.3618e-02, -1.7001e-01, -9.3777e-02, -4.4260e-01,  2.7408e-01,\n",
       "          -2.6038e-01,  5.7491e-02,  4.8378e-02,  2.6667e-02, -2.7939e-01,\n",
       "          -2.1347e-01, -5.7146e-01, -4.2780e-01,  3.9927e-01, -2.3433e-01,\n",
       "          -3.4036e-01, -2.6126e-01, -2.2133e-01,  3.1489e-01,  1.0589e-01,\n",
       "          -2.3049e-01,  3.3393e-01,  3.9580e-02,  6.9268e-02,  1.0786e-01,\n",
       "           4.3680e-02, -1.3661e-03,  4.0690e-02,  3.1856e-01,  4.8938e-02,\n",
       "          -4.7885e-01,  1.4441e-01,  9.0219e-01, -6.4605e-02,  3.9035e-01,\n",
       "          -1.3006e-01,  2.2442e-01, -1.6776e-01, -9.6538e-02, -3.2596e-01,\n",
       "           2.4414e-01,  1.7990e-01,  1.3210e-01, -2.7508e-02, -1.6708e-01,\n",
       "          -7.2453e-02,  3.6112e-01,  1.7543e-03,  2.1864e-01,  3.5058e-02,\n",
       "          -1.1467e-01, -2.9627e-01, -9.2460e-03,  1.7361e-02, -1.5156e-01,\n",
       "           4.3672e-01, -1.0810e-01,  2.1949e-01, -1.6031e-01,  7.9630e-02,\n",
       "          -2.6704e-01,  9.6436e-02, -9.1067e-02, -3.4006e-02, -1.0135e-01,\n",
       "          -1.3300e-01, -1.1336e-01,  2.4301e-01, -3.6827e-02, -7.1317e-03,\n",
       "           3.4798e-01,  2.6985e-03,  3.1138e-02,  1.8100e-01,  2.7475e-01,\n",
       "          -6.9054e-02, -6.8759e-02, -7.0484e-02,  6.8572e-02, -3.5036e-01,\n",
       "           2.1605e-02, -2.0104e-02, -1.4014e-03,  1.6381e-01,  5.8007e-01,\n",
       "          -5.6445e-01, -3.6321e-02, -1.2807e-01, -2.9857e-02, -1.2163e-01,\n",
       "          -1.0122e-01,  1.7037e-01,  1.1601e-01, -8.2954e-02,  2.6946e-03,\n",
       "          -1.1136e-01, -8.7299e-02, -2.0346e-01, -1.9766e-01, -2.6130e-02,\n",
       "          -4.3175e-01,  4.3006e-01, -1.5023e-01,  2.6643e-01,  1.0258e-01,\n",
       "          -5.2727e-01,  5.2501e-01, -2.9109e-01,  1.4633e-02,  5.9105e-02,\n",
       "           1.1609e-01,  2.1430e-02, -1.7106e-01, -1.4557e-01,  5.3713e-02,\n",
       "           2.6238e-01, -4.4832e-02,  2.7818e-01,  3.5873e-01, -1.1511e-01,\n",
       "           3.5144e-01, -4.1552e-01, -6.7309e-02, -2.8997e-01, -2.2453e-01,\n",
       "           3.9258e-02,  1.5291e-01, -2.7494e-02, -1.2718e-01,  1.2670e-01,\n",
       "          -2.5984e-01, -8.0735e-02,  2.9160e-01, -4.9139e-02, -2.7177e-02,\n",
       "          -1.5466e-01,  3.1755e-01, -4.1531e-01,  5.7373e-02,  3.9311e-02,\n",
       "          -3.1150e-02, -8.1021e-02,  1.2103e-01, -3.4716e-01,  3.1155e-01,\n",
       "          -2.5113e-01,  3.3502e-01], device='cuda:0')],\n",
       " [tensor([ 5.1199e-01,  6.7217e-02, -6.2822e-01,  2.0367e-01,  3.5700e-02,\n",
       "           1.2200e-02, -1.7521e-01,  4.4324e-04, -6.8688e-02, -1.1810e-01,\n",
       "          -3.5943e-02, -1.3074e-02,  5.2857e-02, -2.8560e-01,  1.5273e-01,\n",
       "          -2.8706e-01,  8.2016e-02, -1.2833e-01,  4.5680e-01,  3.5889e-01,\n",
       "          -1.1988e-01, -1.2642e-01, -1.9967e-01,  2.5266e-01,  3.5789e-01,\n",
       "          -1.8242e-02,  3.3421e-02, -3.7464e-01, -1.7979e-01,  2.7704e-01,\n",
       "           2.5598e-01,  2.5221e-01,  1.6680e-01,  4.9888e-02, -1.0321e-01,\n",
       "           2.2811e-01,  3.0461e-02,  3.0009e-02, -8.6265e-02,  7.8788e-02,\n",
       "           1.2989e-01, -1.0888e-01, -1.2430e-01, -4.4398e-02,  4.5020e-01,\n",
       "           2.6943e-01, -1.8908e-01,  7.7852e-02, -3.0184e-01, -1.7944e-01,\n",
       "           1.3145e-01,  1.8982e-01, -1.8014e-02,  2.6812e-01,  1.1116e-01,\n",
       "           1.8091e-01, -1.4851e-01,  7.7178e-02,  7.3749e-02,  1.7807e-01,\n",
       "          -2.0870e-01,  2.7113e-03,  3.7070e-01,  2.3924e-01,  2.9215e-01,\n",
       "          -1.9263e-03,  7.5360e-02,  9.1701e-02,  1.1798e-01, -3.8894e-02,\n",
       "           9.3340e-02,  2.8528e-01,  2.3927e-01,  1.1966e-02,  2.1036e-01,\n",
       "           6.3470e-03, -1.1353e-01,  2.0703e-01, -1.0335e-01, -1.5467e-01,\n",
       "           3.3455e-01, -1.4482e-01,  1.2664e-01, -3.2243e-01,  7.7489e-03,\n",
       "           2.2305e-01,  2.3832e-01, -2.3618e-01, -3.2453e-01,  1.3344e-01,\n",
       "           2.4659e-01, -1.7344e-03, -8.2906e-02,  1.6583e-01,  1.8659e-01,\n",
       "           8.1166e-02,  1.0294e-01, -3.1155e-01,  1.7839e+00,  8.6899e-02,\n",
       "           7.2251e-02, -1.7992e-01, -5.0660e-01,  5.6370e-02, -2.4217e-01,\n",
       "           3.4459e-01,  2.0594e-01, -4.3753e-02, -1.0069e-01, -1.7036e-01,\n",
       "          -1.6493e-01,  2.7466e-01,  5.7009e-02,  8.5543e-02, -1.8079e-01,\n",
       "           7.2932e-03,  3.3404e-02, -1.3353e-01,  1.9937e-01,  1.9785e-01,\n",
       "           5.2704e-02, -3.0257e-01, -1.1293e-01,  5.3753e-03, -5.9776e-02,\n",
       "          -2.4693e-01,  2.2928e-01, -3.4245e-01,  2.1027e-01, -3.4180e-01,\n",
       "           2.1754e-01,  2.7348e-02,  1.4005e-03,  2.6408e-01, -2.5132e-01,\n",
       "          -2.2898e-01,  1.4256e-01,  5.8775e-03, -1.6325e-01,  6.9918e-02,\n",
       "          -3.6745e-02, -1.4522e-02,  2.8980e-01, -2.7921e-02,  1.5737e-01,\n",
       "          -1.9639e-01, -1.9266e-01, -2.7696e-01,  1.7241e-01, -4.3368e-01,\n",
       "          -2.2806e-01,  2.2295e-02, -3.3013e-01,  1.2354e-01, -4.2327e-02,\n",
       "          -2.9068e-01,  6.3014e-02,  2.4697e-01,  9.3234e-03, -1.3259e-01,\n",
       "          -3.5415e-02, -2.0506e-03,  2.5672e-01, -1.8524e-01, -1.8568e-03,\n",
       "           1.3712e-01,  7.4625e-02, -2.1933e-02,  2.2335e-01,  1.7142e-01,\n",
       "          -3.5628e-01,  6.8933e-02,  9.1631e-02, -1.9857e-01, -1.9137e-01,\n",
       "           6.3319e-02, -1.7657e-01,  2.6075e-01, -8.5149e-02, -1.0594e-01,\n",
       "           1.1396e-01, -1.8425e-01, -7.1094e-02, -1.1425e-01,  3.4482e-01,\n",
       "           5.8817e-02, -1.0522e-01,  1.0239e-01, -1.4164e-01,  1.1239e-03,\n",
       "          -2.9062e-01,  3.7796e-01, -4.7504e-01, -2.4936e-01, -1.5992e-01,\n",
       "           2.4897e-01, -4.2938e-01, -9.5678e-02, -1.2342e-01, -2.8082e-02,\n",
       "          -2.4457e-01,  1.1679e-03,  1.2541e-01,  1.1309e-01,  1.7459e-01,\n",
       "          -5.4428e-02, -1.8425e-01,  8.9083e-02,  1.7859e-01,  3.5044e-01,\n",
       "          -1.4093e-01, -7.3836e-02, -1.8842e-01,  1.0473e-01,  1.1690e-01,\n",
       "           2.2390e-01,  2.6289e-01, -1.3752e-01, -2.0178e-01, -7.2094e-02,\n",
       "           2.2588e-01, -9.7305e-02,  1.9384e-01,  1.3331e-01,  2.9979e-01,\n",
       "          -2.1996e-01, -1.3837e-01, -8.6046e-02, -3.5572e-02,  8.3026e-02,\n",
       "          -3.7732e-01, -2.6070e-01, -7.1706e-02,  2.5786e-01, -7.4200e-02,\n",
       "          -1.7178e-01,  1.1079e-01,  7.7694e-02, -2.7876e-01, -2.3120e-01,\n",
       "          -2.6540e-02,  2.2528e+00, -5.8911e-02, -4.7647e-02,  1.7766e-02,\n",
       "           5.0872e-02,  1.2243e-01, -2.8358e-01, -5.8081e-02, -5.9568e-02,\n",
       "          -4.9095e-01,  9.5258e-02,  3.9939e-01, -3.3053e-01,  1.1624e-01,\n",
       "           7.4713e-02, -2.2068e-01,  3.9781e-01, -2.2691e-02,  2.0983e-01,\n",
       "          -3.5292e-01,  6.4390e-02,  1.3993e-01,  1.6917e-01, -9.5128e-01,\n",
       "          -8.9082e-02, -8.9976e-02,  1.4344e-01, -1.0179e-01,  1.4457e-01,\n",
       "           1.1434e-01,  3.8826e-01, -5.1193e-03, -7.2353e-02,  3.1514e-01,\n",
       "           6.9131e-01, -1.7789e-01, -1.2838e-01,  2.2159e-01, -4.9201e-02,\n",
       "          -1.7699e-01,  2.2713e-02, -1.9996e-01,  6.2268e-02,  5.7276e-02,\n",
       "           2.8217e-02,  1.2388e-01, -7.9870e-02, -9.3750e-02,  2.2735e-02,\n",
       "           3.2727e-01, -2.5552e-01,  2.5481e-01,  3.0350e-02,  1.0637e-01,\n",
       "           1.1261e-01, -1.7137e-01,  2.5317e-01,  1.7071e-01, -7.8992e-02,\n",
       "           2.0338e-01, -7.4795e-03,  5.6608e-01, -6.3063e-02,  1.1983e-01,\n",
       "          -3.8464e-01,  1.2004e-01, -7.8010e-02,  4.1221e-01, -1.6460e-01,\n",
       "          -3.3751e-01,  4.6604e-02,  3.6122e-01, -1.7714e-02,  6.2197e-02,\n",
       "          -2.3326e-02, -1.8835e-01,  4.0574e-02,  6.3516e-02, -1.2420e-01,\n",
       "          -1.4674e-01,  1.9444e-01, -9.1703e-02, -3.7505e-01,  4.4699e-02,\n",
       "           3.0831e-01, -8.2783e-02, -5.7083e-02,  8.3300e-02, -3.7451e-02,\n",
       "          -5.6810e-01,  3.3395e-01, -1.6767e-01,  6.6744e-01,  3.6663e-01,\n",
       "          -3.7911e-02, -2.4347e-01, -6.3793e-02, -2.1815e-02, -1.7477e-01,\n",
       "           8.1840e-02,  8.6021e-02,  1.7668e-01,  1.9756e-01, -2.5598e-01,\n",
       "          -1.1381e-01, -3.4735e-01,  1.2064e-01, -4.9369e-03,  3.7298e-01,\n",
       "           6.6049e-02,  1.7251e-01, -1.3744e-01,  2.2471e-01,  6.2586e-02,\n",
       "           3.3100e-01,  1.1605e-01, -7.6892e-02, -1.1532e-01,  2.6284e-02,\n",
       "          -1.9663e-01,  1.3052e-01, -3.9807e-02,  2.4890e-01, -1.4221e-01,\n",
       "          -6.9413e-03, -9.4677e-02,  6.7206e-02, -4.4246e-01,  2.0318e-01,\n",
       "          -5.4312e-02,  2.2955e-01, -1.1185e-01,  4.5975e-02, -3.0779e-01,\n",
       "          -2.0885e-01, -5.5737e-01, -2.5313e-01,  1.2569e-01, -2.0181e-01,\n",
       "          -2.5140e-01, -2.1319e-01, -2.3389e-01,  2.5118e-01,  1.0444e-01,\n",
       "          -9.3709e-02,  2.6567e-01, -9.2356e-03,  8.5706e-02,  2.9555e-02,\n",
       "          -1.0901e-03,  1.0854e-01,  1.4911e-01,  1.0911e-01,  4.6317e-03,\n",
       "          -4.5364e-01,  6.4951e-02,  5.1204e-01,  4.5070e-02,  3.5309e-01,\n",
       "          -2.6169e-01,  1.5892e-01, -1.8759e-01,  8.0931e-02, -1.5673e-01,\n",
       "           3.1781e-01,  2.4186e-01,  1.4195e-01, -5.7960e-02, -2.1879e-01,\n",
       "          -3.9357e-02,  2.9624e-01,  6.9045e-02,  4.2252e-02, -1.3590e-01,\n",
       "          -2.0413e-01, -2.9312e-01, -1.0235e-01,  1.8207e-02, -2.2969e-01,\n",
       "           4.6641e-01, -9.0372e-02,  3.6463e-02, -1.0181e-01,  1.4460e-01,\n",
       "          -1.8688e-01,  3.1896e-02, -4.6111e-02, -6.0641e-02,  1.0143e-01,\n",
       "          -9.1146e-02,  9.6946e-02,  2.2511e-01,  6.5458e-02,  3.0277e-02,\n",
       "           2.5575e-01, -1.4143e-03, -6.1967e-02,  1.6394e-01,  2.6592e-01,\n",
       "          -7.1706e-02,  1.2125e-01,  6.1986e-02, -5.4316e-02, -2.9374e-01,\n",
       "          -2.5845e-02,  3.6862e-02,  8.3491e-03,  1.1467e-01,  4.3105e-01,\n",
       "          -2.5261e-01, -8.2594e-02, -4.5061e-02, -3.9239e-02, -6.1770e-02,\n",
       "          -8.9522e-02,  1.6852e-01,  7.5903e-02, -6.2312e-02,  1.7686e-02,\n",
       "          -5.5122e-02, -1.3073e-01, -1.5270e-01, -5.3549e-02,  5.3696e-02,\n",
       "          -1.7208e-01,  4.1693e-01, -1.0629e-01,  2.4776e-01, -8.4640e-02,\n",
       "          -3.8824e-01,  2.6061e-01, -1.5364e-01, -3.6149e-02,  1.2991e-01,\n",
       "           1.6077e-01,  7.8555e-03, -1.7734e-01, -2.5661e-02, -2.8434e-02,\n",
       "           1.4266e-01,  6.3670e-02,  3.5782e-01,  3.5624e-01, -1.4273e-01,\n",
       "           4.0379e-01, -1.6081e-02,  3.6598e-03, -2.1655e-01, -2.2918e-01,\n",
       "           1.0139e-01,  1.5702e-01,  4.5970e-02, -2.2864e-01, -1.5470e-02,\n",
       "          -1.3869e-01,  7.9072e-02, -1.8118e-02, -1.2263e-01, -3.7886e-02,\n",
       "          -2.2483e-01,  3.2867e-01, -3.7717e-01,  3.4737e-02, -5.8808e-02,\n",
       "          -9.2526e-02, -1.8681e-01,  4.8722e-02, -2.5632e-01,  2.3552e-01,\n",
       "          -1.9595e-01,  1.7809e-01], device='cuda:0'),\n",
       "  tensor([ 5.2274e-01,  1.5641e-01, -1.0848e+00,  1.1177e-01,  7.5653e-02,\n",
       "           9.4285e-02, -1.8001e-01,  2.5528e-02, -2.4492e-01, -1.7891e-01,\n",
       "          -4.5921e-02,  2.5578e-02,  1.8064e-01, -4.5970e-01,  6.3076e-02,\n",
       "          -3.7772e-01,  1.5441e-01, -2.4951e-01,  4.7971e-01,  3.5050e-01,\n",
       "          -2.6991e-01, -9.2889e-02, -1.7606e-01,  1.7152e-01,  2.3409e-01,\n",
       "          -7.8755e-02, -4.0918e-02, -3.0925e-01, -1.6300e-01,  3.8787e-01,\n",
       "           2.5585e-01,  3.3782e-01,  1.5435e-01,  7.4610e-02, -2.3172e-02,\n",
       "           1.3508e-01,  2.6527e-02, -4.3903e-02, -6.8146e-02,  2.9305e-01,\n",
       "           2.3502e-02, -1.9274e-01, -7.0365e-02, -3.2197e-01,  5.4049e-01,\n",
       "           3.2196e-01, -2.7936e-01,  7.5554e-02, -1.5153e-01, -2.3615e-01,\n",
       "           2.3531e-01,  1.5285e-01, -2.6181e-01,  1.7080e-01,  2.8013e-01,\n",
       "           1.7029e-01, -1.4272e-02,  1.2108e-02, -1.6482e-01,  8.8154e-02,\n",
       "          -2.0103e-01, -1.1609e-01,  4.7431e-01,  3.1184e-01,  4.3271e-01,\n",
       "           9.0316e-02,  1.8462e-01,  1.6195e-01,  6.8520e-02, -1.0234e-01,\n",
       "          -9.5794e-02,  4.6709e-01,  4.8865e-01, -5.3075e-02,  1.7724e-01,\n",
       "          -1.0268e-01,  2.9243e-02,  3.3458e-01, -1.4299e-01, -1.4205e-01,\n",
       "           5.5196e-01, -1.6875e-01,  1.9743e-01, -2.6752e-01, -1.1521e-02,\n",
       "           1.7574e-01,  6.4551e-02, -5.0148e-01, -3.4393e-01,  2.6587e-01,\n",
       "           4.6449e-01, -5.0084e-04,  8.8365e-03,  2.4609e-01,  2.6972e-01,\n",
       "           8.8671e-05,  1.6340e-01, -3.4826e-01,  2.7051e+00,  2.1946e-01,\n",
       "           1.7937e-01, -2.9697e-01, -4.5259e-01,  2.4084e-01, -2.9354e-01,\n",
       "           4.4238e-01,  3.2129e-01, -7.7020e-02, -3.8553e-02, -1.4583e-01,\n",
       "          -3.7871e-01,  2.1882e-01,  1.4687e-01,  9.9550e-02, -1.0327e-01,\n",
       "           4.0365e-02,  1.8728e-01, -1.0975e-01,  4.5076e-01,  2.5217e-01,\n",
       "           1.8275e-01, -3.9115e-01, -3.5903e-01, -1.4331e-01, -1.1253e-01,\n",
       "          -2.5967e-01,  2.5023e-01, -4.1895e-01,  1.8815e-01, -2.4978e-01,\n",
       "           4.5059e-01,  1.3987e-01, -1.1408e-01,  2.5577e-01, -1.5558e-01,\n",
       "          -2.2749e-01,  2.1994e-01, -2.3140e-01, -4.2276e-01,  1.9363e-01,\n",
       "          -1.8481e-01, -9.6807e-03,  2.1645e-01,  1.5031e-01,  2.4152e-01,\n",
       "          -4.2557e-01, -3.0346e-01, -2.2863e-01,  2.3039e-01, -6.4430e-01,\n",
       "          -2.1962e-01, -7.2523e-02, -3.3888e-01,  4.6646e-02, -3.4023e-02,\n",
       "          -1.6126e-01,  1.8565e-01,  2.4995e-01, -7.5664e-02, -1.1935e-01,\n",
       "          -4.3957e-02,  8.8616e-02,  2.8147e-01, -1.9149e-01,  5.2789e-02,\n",
       "           1.3602e-01, -5.7536e-02, -1.0664e-01,  2.9895e-01,  9.0617e-02,\n",
       "          -2.4501e-01,  6.2611e-02,  9.3418e-02, -3.0951e-01, -1.2341e-01,\n",
       "           3.8067e-02, -1.7876e-01,  4.6486e-01, -1.7777e-01, -7.2212e-02,\n",
       "           3.2726e-01, -3.2004e-01, -2.6162e-02, -1.7617e-01,  4.4617e-01,\n",
       "          -3.3485e-02, -8.0312e-02,  1.2325e-01, -1.3496e-01,  2.0807e-01,\n",
       "          -6.2653e-01,  3.0064e-01, -2.3005e-01, -2.4519e-01, -1.9420e-01,\n",
       "           4.5950e-01, -6.5656e-01, -7.4801e-02, -1.3110e-01, -1.3971e-01,\n",
       "          -3.7797e-01,  5.9950e-02, -1.9938e-02,  1.0263e-01,  2.8390e-01,\n",
       "          -3.7863e-02, -1.5375e-01,  1.1312e-01,  2.1486e-01,  3.8871e-01,\n",
       "          -2.5118e-01,  1.1082e-01, -2.9860e-01, -6.2103e-02,  2.1795e-02,\n",
       "           2.4248e-01,  3.7907e-01, -1.0378e-01, -1.7678e-01, -7.0341e-02,\n",
       "           2.8019e-01, -1.2278e-01,  2.2996e-01,  1.3770e-01,  4.2115e-01,\n",
       "          -3.6226e-01, -6.6510e-01, -1.9985e-02, -1.3854e-01,  1.9808e-01,\n",
       "          -6.1745e-01, -3.9260e-01, -7.5302e-02,  2.5463e-01, -6.3912e-02,\n",
       "          -1.0777e-01,  2.0373e-01,  1.7897e-02, -5.0587e-01, -2.4789e-01,\n",
       "           1.0027e-01,  3.4695e+00, -1.2569e-01, -7.9449e-02,  8.5338e-02,\n",
       "           4.5396e-02,  1.8641e-01, -4.3182e-01, -2.8186e-02, -6.3559e-01,\n",
       "          -6.4887e-01, -6.9457e-02,  4.8287e-01, -5.8451e-01,  2.1489e-01,\n",
       "           1.6621e-01, -6.8065e-02,  4.4866e-01, -3.4670e-02,  3.2352e-01,\n",
       "          -3.7933e-01,  1.3323e-02,  3.2149e-01,  2.9157e-01, -1.3252e+00,\n",
       "          -4.1968e-03, -3.6521e-02,  2.1551e-01, -2.0766e-01,  3.7599e-01,\n",
       "           2.2564e-01,  5.2119e-01, -4.4355e-02, -1.8952e-01,  2.5574e-01,\n",
       "           1.1558e+00, -1.8362e-01, -2.5785e-01,  3.3655e-01, -5.9746e-03,\n",
       "          -2.3840e-01,  4.8814e-02, -3.9995e-01, -5.1703e-02,  9.0431e-02,\n",
       "          -1.4829e-01,  2.6324e-01, -2.1299e-01, -2.0745e-01,  2.1008e-01,\n",
       "           4.3979e-01, -4.1771e-01,  1.6865e-01, -1.4575e-03,  1.6424e-01,\n",
       "          -2.7560e-02, -2.9673e-01,  4.4356e-01,  3.8644e-01, -1.7177e-02,\n",
       "           3.0430e-01, -1.3036e-01,  5.8990e-01, -2.6672e-02,  1.3839e-01,\n",
       "          -2.1776e-01,  2.5886e-01, -1.0365e-01,  5.0708e-01,  7.1468e-02,\n",
       "          -2.8835e-01,  9.0630e-03,  5.4950e-01,  1.3183e-02,  7.8507e-02,\n",
       "          -2.4742e-02, -2.8229e-01,  6.9248e-02,  1.3588e-01, -8.1964e-02,\n",
       "          -1.2618e-01,  1.7700e-01, -1.8295e-01, -3.7349e-01, -3.0257e-02,\n",
       "           4.9901e-01, -4.6310e-02, -1.0667e-01,  2.5187e-01, -2.4974e-01,\n",
       "          -1.0153e+00,  2.5389e-01, -1.6947e-01,  9.5279e-01,  2.2441e-01,\n",
       "           1.8631e-02, -3.5385e-01, -3.7518e-03, -1.3832e-03, -1.4773e-01,\n",
       "           1.1633e-01, -1.4843e-02,  3.2812e-01,  7.6172e-02, -8.9335e-02,\n",
       "          -2.7072e-02, -3.8204e-01,  1.4398e-01, -2.2048e-01,  2.6724e-01,\n",
       "           1.7931e-01,  1.9831e-02, -3.5856e-02,  3.1362e-01, -5.6898e-02,\n",
       "           2.9256e-01,  1.3741e-01,  1.1482e-01, -2.2795e-01,  6.1906e-02,\n",
       "          -3.8382e-01,  9.9245e-02,  2.6809e-02,  1.2444e-01, -1.0488e-01,\n",
       "          -6.3618e-02, -1.7001e-01, -9.3777e-02, -4.4260e-01,  2.7408e-01,\n",
       "          -2.6038e-01,  5.7491e-02,  4.8378e-02,  2.6667e-02, -2.7939e-01,\n",
       "          -2.1347e-01, -5.7146e-01, -4.2780e-01,  3.9927e-01, -2.3433e-01,\n",
       "          -3.4036e-01, -2.6126e-01, -2.2133e-01,  3.1489e-01,  1.0589e-01,\n",
       "          -2.3049e-01,  3.3393e-01,  3.9580e-02,  6.9268e-02,  1.0786e-01,\n",
       "           4.3680e-02, -1.3661e-03,  4.0690e-02,  3.1856e-01,  4.8938e-02,\n",
       "          -4.7885e-01,  1.4441e-01,  9.0219e-01, -6.4605e-02,  3.9035e-01,\n",
       "          -1.3006e-01,  2.2442e-01, -1.6776e-01, -9.6538e-02, -3.2596e-01,\n",
       "           2.4414e-01,  1.7990e-01,  1.3210e-01, -2.7508e-02, -1.6708e-01,\n",
       "          -7.2453e-02,  3.6112e-01,  1.7543e-03,  2.1864e-01,  3.5058e-02,\n",
       "          -1.1467e-01, -2.9627e-01, -9.2460e-03,  1.7361e-02, -1.5156e-01,\n",
       "           4.3672e-01, -1.0810e-01,  2.1949e-01, -1.6031e-01,  7.9630e-02,\n",
       "          -2.6704e-01,  9.6436e-02, -9.1067e-02, -3.4006e-02, -1.0135e-01,\n",
       "          -1.3300e-01, -1.1336e-01,  2.4301e-01, -3.6827e-02, -7.1317e-03,\n",
       "           3.4798e-01,  2.6985e-03,  3.1138e-02,  1.8100e-01,  2.7475e-01,\n",
       "          -6.9054e-02, -6.8759e-02, -7.0484e-02,  6.8572e-02, -3.5036e-01,\n",
       "           2.1605e-02, -2.0104e-02, -1.4014e-03,  1.6381e-01,  5.8007e-01,\n",
       "          -5.6445e-01, -3.6321e-02, -1.2807e-01, -2.9857e-02, -1.2163e-01,\n",
       "          -1.0122e-01,  1.7037e-01,  1.1601e-01, -8.2954e-02,  2.6946e-03,\n",
       "          -1.1136e-01, -8.7299e-02, -2.0346e-01, -1.9766e-01, -2.6130e-02,\n",
       "          -4.3175e-01,  4.3006e-01, -1.5023e-01,  2.6643e-01,  1.0258e-01,\n",
       "          -5.2727e-01,  5.2501e-01, -2.9109e-01,  1.4633e-02,  5.9105e-02,\n",
       "           1.1609e-01,  2.1430e-02, -1.7106e-01, -1.4557e-01,  5.3713e-02,\n",
       "           2.6238e-01, -4.4832e-02,  2.7818e-01,  3.5873e-01, -1.1511e-01,\n",
       "           3.5144e-01, -4.1552e-01, -6.7309e-02, -2.8997e-01, -2.2453e-01,\n",
       "           3.9258e-02,  1.5291e-01, -2.7494e-02, -1.2718e-01,  1.2670e-01,\n",
       "          -2.5984e-01, -8.0735e-02,  2.9160e-01, -4.9139e-02, -2.7177e-02,\n",
       "          -1.5466e-01,  3.1755e-01, -4.1531e-01,  5.7373e-02,  3.9311e-02,\n",
       "          -3.1150e-02, -8.1021e-02,  1.2103e-01, -3.4716e-01,  3.1155e-01,\n",
       "          -2.5113e-01,  3.3502e-01], device='cuda:0')],\n",
       " [tensor([ 5.1199e-01,  6.7217e-02, -6.2822e-01,  2.0367e-01,  3.5700e-02,\n",
       "           1.2200e-02, -1.7521e-01,  4.4324e-04, -6.8688e-02, -1.1810e-01,\n",
       "          -3.5943e-02, -1.3074e-02,  5.2857e-02, -2.8560e-01,  1.5273e-01,\n",
       "          -2.8706e-01,  8.2016e-02, -1.2833e-01,  4.5680e-01,  3.5889e-01,\n",
       "          -1.1988e-01, -1.2642e-01, -1.9967e-01,  2.5266e-01,  3.5789e-01,\n",
       "          -1.8242e-02,  3.3421e-02, -3.7464e-01, -1.7979e-01,  2.7704e-01,\n",
       "           2.5598e-01,  2.5221e-01,  1.6680e-01,  4.9888e-02, -1.0321e-01,\n",
       "           2.2811e-01,  3.0461e-02,  3.0009e-02, -8.6265e-02,  7.8788e-02,\n",
       "           1.2989e-01, -1.0888e-01, -1.2430e-01, -4.4398e-02,  4.5020e-01,\n",
       "           2.6943e-01, -1.8908e-01,  7.7852e-02, -3.0184e-01, -1.7944e-01,\n",
       "           1.3145e-01,  1.8982e-01, -1.8014e-02,  2.6812e-01,  1.1116e-01,\n",
       "           1.8091e-01, -1.4851e-01,  7.7178e-02,  7.3749e-02,  1.7807e-01,\n",
       "          -2.0870e-01,  2.7113e-03,  3.7070e-01,  2.3924e-01,  2.9215e-01,\n",
       "          -1.9263e-03,  7.5360e-02,  9.1701e-02,  1.1798e-01, -3.8894e-02,\n",
       "           9.3340e-02,  2.8528e-01,  2.3927e-01,  1.1966e-02,  2.1036e-01,\n",
       "           6.3470e-03, -1.1353e-01,  2.0703e-01, -1.0335e-01, -1.5467e-01,\n",
       "           3.3455e-01, -1.4482e-01,  1.2664e-01, -3.2243e-01,  7.7489e-03,\n",
       "           2.2305e-01,  2.3832e-01, -2.3618e-01, -3.2453e-01,  1.3344e-01,\n",
       "           2.4659e-01, -1.7344e-03, -8.2906e-02,  1.6583e-01,  1.8659e-01,\n",
       "           8.1166e-02,  1.0294e-01, -3.1155e-01,  1.7839e+00,  8.6899e-02,\n",
       "           7.2251e-02, -1.7992e-01, -5.0660e-01,  5.6370e-02, -2.4217e-01,\n",
       "           3.4459e-01,  2.0594e-01, -4.3753e-02, -1.0069e-01, -1.7036e-01,\n",
       "          -1.6493e-01,  2.7466e-01,  5.7009e-02,  8.5543e-02, -1.8079e-01,\n",
       "           7.2932e-03,  3.3404e-02, -1.3353e-01,  1.9937e-01,  1.9785e-01,\n",
       "           5.2704e-02, -3.0257e-01, -1.1293e-01,  5.3753e-03, -5.9776e-02,\n",
       "          -2.4693e-01,  2.2928e-01, -3.4245e-01,  2.1027e-01, -3.4180e-01,\n",
       "           2.1754e-01,  2.7348e-02,  1.4005e-03,  2.6408e-01, -2.5132e-01,\n",
       "          -2.2898e-01,  1.4256e-01,  5.8775e-03, -1.6325e-01,  6.9918e-02,\n",
       "          -3.6745e-02, -1.4522e-02,  2.8980e-01, -2.7921e-02,  1.5737e-01,\n",
       "          -1.9639e-01, -1.9266e-01, -2.7696e-01,  1.7241e-01, -4.3368e-01,\n",
       "          -2.2806e-01,  2.2295e-02, -3.3013e-01,  1.2354e-01, -4.2327e-02,\n",
       "          -2.9068e-01,  6.3014e-02,  2.4697e-01,  9.3234e-03, -1.3259e-01,\n",
       "          -3.5415e-02, -2.0506e-03,  2.5672e-01, -1.8524e-01, -1.8568e-03,\n",
       "           1.3712e-01,  7.4625e-02, -2.1933e-02,  2.2335e-01,  1.7142e-01,\n",
       "          -3.5628e-01,  6.8933e-02,  9.1631e-02, -1.9857e-01, -1.9137e-01,\n",
       "           6.3319e-02, -1.7657e-01,  2.6075e-01, -8.5149e-02, -1.0594e-01,\n",
       "           1.1396e-01, -1.8425e-01, -7.1094e-02, -1.1425e-01,  3.4482e-01,\n",
       "           5.8817e-02, -1.0522e-01,  1.0239e-01, -1.4164e-01,  1.1239e-03,\n",
       "          -2.9062e-01,  3.7796e-01, -4.7504e-01, -2.4936e-01, -1.5992e-01,\n",
       "           2.4897e-01, -4.2938e-01, -9.5678e-02, -1.2342e-01, -2.8082e-02,\n",
       "          -2.4457e-01,  1.1679e-03,  1.2541e-01,  1.1309e-01,  1.7459e-01,\n",
       "          -5.4428e-02, -1.8425e-01,  8.9083e-02,  1.7859e-01,  3.5044e-01,\n",
       "          -1.4093e-01, -7.3836e-02, -1.8842e-01,  1.0473e-01,  1.1690e-01,\n",
       "           2.2390e-01,  2.6289e-01, -1.3752e-01, -2.0178e-01, -7.2094e-02,\n",
       "           2.2588e-01, -9.7305e-02,  1.9384e-01,  1.3331e-01,  2.9979e-01,\n",
       "          -2.1996e-01, -1.3837e-01, -8.6046e-02, -3.5572e-02,  8.3026e-02,\n",
       "          -3.7732e-01, -2.6070e-01, -7.1706e-02,  2.5786e-01, -7.4200e-02,\n",
       "          -1.7178e-01,  1.1079e-01,  7.7694e-02, -2.7876e-01, -2.3120e-01,\n",
       "          -2.6540e-02,  2.2528e+00, -5.8911e-02, -4.7647e-02,  1.7766e-02,\n",
       "           5.0872e-02,  1.2243e-01, -2.8358e-01, -5.8081e-02, -5.9568e-02,\n",
       "          -4.9095e-01,  9.5258e-02,  3.9939e-01, -3.3053e-01,  1.1624e-01,\n",
       "           7.4713e-02, -2.2068e-01,  3.9781e-01, -2.2691e-02,  2.0983e-01,\n",
       "          -3.5292e-01,  6.4390e-02,  1.3993e-01,  1.6917e-01, -9.5128e-01,\n",
       "          -8.9082e-02, -8.9976e-02,  1.4344e-01, -1.0179e-01,  1.4457e-01,\n",
       "           1.1434e-01,  3.8826e-01, -5.1193e-03, -7.2353e-02,  3.1514e-01,\n",
       "           6.9131e-01, -1.7789e-01, -1.2838e-01,  2.2159e-01, -4.9201e-02,\n",
       "          -1.7699e-01,  2.2713e-02, -1.9996e-01,  6.2268e-02,  5.7276e-02,\n",
       "           2.8217e-02,  1.2388e-01, -7.9870e-02, -9.3750e-02,  2.2735e-02,\n",
       "           3.2727e-01, -2.5552e-01,  2.5481e-01,  3.0350e-02,  1.0637e-01,\n",
       "           1.1261e-01, -1.7137e-01,  2.5317e-01,  1.7071e-01, -7.8992e-02,\n",
       "           2.0338e-01, -7.4795e-03,  5.6608e-01, -6.3063e-02,  1.1983e-01,\n",
       "          -3.8464e-01,  1.2004e-01, -7.8010e-02,  4.1221e-01, -1.6460e-01,\n",
       "          -3.3751e-01,  4.6604e-02,  3.6122e-01, -1.7714e-02,  6.2197e-02,\n",
       "          -2.3326e-02, -1.8835e-01,  4.0574e-02,  6.3516e-02, -1.2420e-01,\n",
       "          -1.4674e-01,  1.9444e-01, -9.1703e-02, -3.7505e-01,  4.4699e-02,\n",
       "           3.0831e-01, -8.2783e-02, -5.7083e-02,  8.3300e-02, -3.7451e-02,\n",
       "          -5.6810e-01,  3.3395e-01, -1.6767e-01,  6.6744e-01,  3.6663e-01,\n",
       "          -3.7911e-02, -2.4347e-01, -6.3793e-02, -2.1815e-02, -1.7477e-01,\n",
       "           8.1840e-02,  8.6021e-02,  1.7668e-01,  1.9756e-01, -2.5598e-01,\n",
       "          -1.1381e-01, -3.4735e-01,  1.2064e-01, -4.9369e-03,  3.7298e-01,\n",
       "           6.6049e-02,  1.7251e-01, -1.3744e-01,  2.2471e-01,  6.2586e-02,\n",
       "           3.3100e-01,  1.1605e-01, -7.6892e-02, -1.1532e-01,  2.6284e-02,\n",
       "          -1.9663e-01,  1.3052e-01, -3.9807e-02,  2.4890e-01, -1.4221e-01,\n",
       "          -6.9413e-03, -9.4677e-02,  6.7206e-02, -4.4246e-01,  2.0318e-01,\n",
       "          -5.4312e-02,  2.2955e-01, -1.1185e-01,  4.5975e-02, -3.0779e-01,\n",
       "          -2.0885e-01, -5.5737e-01, -2.5313e-01,  1.2569e-01, -2.0181e-01,\n",
       "          -2.5140e-01, -2.1319e-01, -2.3389e-01,  2.5118e-01,  1.0444e-01,\n",
       "          -9.3709e-02,  2.6567e-01, -9.2356e-03,  8.5706e-02,  2.9555e-02,\n",
       "          -1.0901e-03,  1.0854e-01,  1.4911e-01,  1.0911e-01,  4.6317e-03,\n",
       "          -4.5364e-01,  6.4951e-02,  5.1204e-01,  4.5070e-02,  3.5309e-01,\n",
       "          -2.6169e-01,  1.5892e-01, -1.8759e-01,  8.0931e-02, -1.5673e-01,\n",
       "           3.1781e-01,  2.4186e-01,  1.4195e-01, -5.7960e-02, -2.1879e-01,\n",
       "          -3.9357e-02,  2.9624e-01,  6.9045e-02,  4.2252e-02, -1.3590e-01,\n",
       "          -2.0413e-01, -2.9312e-01, -1.0235e-01,  1.8207e-02, -2.2969e-01,\n",
       "           4.6641e-01, -9.0372e-02,  3.6463e-02, -1.0181e-01,  1.4460e-01,\n",
       "          -1.8688e-01,  3.1896e-02, -4.6111e-02, -6.0641e-02,  1.0143e-01,\n",
       "          -9.1146e-02,  9.6946e-02,  2.2511e-01,  6.5458e-02,  3.0277e-02,\n",
       "           2.5575e-01, -1.4143e-03, -6.1967e-02,  1.6394e-01,  2.6592e-01,\n",
       "          -7.1706e-02,  1.2125e-01,  6.1986e-02, -5.4316e-02, -2.9374e-01,\n",
       "          -2.5845e-02,  3.6862e-02,  8.3491e-03,  1.1467e-01,  4.3105e-01,\n",
       "          -2.5261e-01, -8.2594e-02, -4.5061e-02, -3.9239e-02, -6.1770e-02,\n",
       "          -8.9522e-02,  1.6852e-01,  7.5903e-02, -6.2312e-02,  1.7686e-02,\n",
       "          -5.5122e-02, -1.3073e-01, -1.5270e-01, -5.3549e-02,  5.3696e-02,\n",
       "          -1.7208e-01,  4.1693e-01, -1.0629e-01,  2.4776e-01, -8.4640e-02,\n",
       "          -3.8824e-01,  2.6061e-01, -1.5364e-01, -3.6149e-02,  1.2991e-01,\n",
       "           1.6077e-01,  7.8555e-03, -1.7734e-01, -2.5661e-02, -2.8434e-02,\n",
       "           1.4266e-01,  6.3670e-02,  3.5782e-01,  3.5624e-01, -1.4273e-01,\n",
       "           4.0379e-01, -1.6081e-02,  3.6598e-03, -2.1655e-01, -2.2918e-01,\n",
       "           1.0139e-01,  1.5702e-01,  4.5970e-02, -2.2864e-01, -1.5470e-02,\n",
       "          -1.3869e-01,  7.9072e-02, -1.8118e-02, -1.2263e-01, -3.7886e-02,\n",
       "          -2.2483e-01,  3.2867e-01, -3.7717e-01,  3.4737e-02, -5.8808e-02,\n",
       "          -9.2526e-02, -1.8681e-01,  4.8722e-02, -2.5632e-01,  2.3552e-01,\n",
       "          -1.9595e-01,  1.7809e-01], device='cuda:0'),\n",
       "  tensor([ 5.2274e-01,  1.5641e-01, -1.0848e+00,  1.1177e-01,  7.5653e-02,\n",
       "           9.4285e-02, -1.8001e-01,  2.5528e-02, -2.4492e-01, -1.7891e-01,\n",
       "          -4.5921e-02,  2.5578e-02,  1.8064e-01, -4.5970e-01,  6.3076e-02,\n",
       "          -3.7772e-01,  1.5441e-01, -2.4951e-01,  4.7971e-01,  3.5050e-01,\n",
       "          -2.6991e-01, -9.2889e-02, -1.7606e-01,  1.7152e-01,  2.3409e-01,\n",
       "          -7.8755e-02, -4.0918e-02, -3.0925e-01, -1.6300e-01,  3.8787e-01,\n",
       "           2.5585e-01,  3.3782e-01,  1.5435e-01,  7.4610e-02, -2.3172e-02,\n",
       "           1.3508e-01,  2.6527e-02, -4.3903e-02, -6.8146e-02,  2.9305e-01,\n",
       "           2.3502e-02, -1.9274e-01, -7.0365e-02, -3.2197e-01,  5.4049e-01,\n",
       "           3.2196e-01, -2.7936e-01,  7.5554e-02, -1.5153e-01, -2.3615e-01,\n",
       "           2.3531e-01,  1.5285e-01, -2.6181e-01,  1.7080e-01,  2.8013e-01,\n",
       "           1.7029e-01, -1.4272e-02,  1.2108e-02, -1.6482e-01,  8.8154e-02,\n",
       "          -2.0103e-01, -1.1609e-01,  4.7431e-01,  3.1184e-01,  4.3271e-01,\n",
       "           9.0316e-02,  1.8462e-01,  1.6195e-01,  6.8520e-02, -1.0234e-01,\n",
       "          -9.5794e-02,  4.6709e-01,  4.8865e-01, -5.3075e-02,  1.7724e-01,\n",
       "          -1.0268e-01,  2.9243e-02,  3.3458e-01, -1.4299e-01, -1.4205e-01,\n",
       "           5.5196e-01, -1.6875e-01,  1.9743e-01, -2.6752e-01, -1.1521e-02,\n",
       "           1.7574e-01,  6.4551e-02, -5.0148e-01, -3.4393e-01,  2.6587e-01,\n",
       "           4.6449e-01, -5.0084e-04,  8.8365e-03,  2.4609e-01,  2.6972e-01,\n",
       "           8.8671e-05,  1.6340e-01, -3.4826e-01,  2.7051e+00,  2.1946e-01,\n",
       "           1.7937e-01, -2.9697e-01, -4.5259e-01,  2.4084e-01, -2.9354e-01,\n",
       "           4.4238e-01,  3.2129e-01, -7.7020e-02, -3.8553e-02, -1.4583e-01,\n",
       "          -3.7871e-01,  2.1882e-01,  1.4687e-01,  9.9550e-02, -1.0327e-01,\n",
       "           4.0365e-02,  1.8728e-01, -1.0975e-01,  4.5076e-01,  2.5217e-01,\n",
       "           1.8275e-01, -3.9115e-01, -3.5903e-01, -1.4331e-01, -1.1253e-01,\n",
       "          -2.5967e-01,  2.5023e-01, -4.1895e-01,  1.8815e-01, -2.4978e-01,\n",
       "           4.5059e-01,  1.3987e-01, -1.1408e-01,  2.5577e-01, -1.5558e-01,\n",
       "          -2.2749e-01,  2.1994e-01, -2.3140e-01, -4.2276e-01,  1.9363e-01,\n",
       "          -1.8481e-01, -9.6807e-03,  2.1645e-01,  1.5031e-01,  2.4152e-01,\n",
       "          -4.2557e-01, -3.0346e-01, -2.2863e-01,  2.3039e-01, -6.4430e-01,\n",
       "          -2.1962e-01, -7.2523e-02, -3.3888e-01,  4.6646e-02, -3.4023e-02,\n",
       "          -1.6126e-01,  1.8565e-01,  2.4995e-01, -7.5664e-02, -1.1935e-01,\n",
       "          -4.3957e-02,  8.8616e-02,  2.8147e-01, -1.9149e-01,  5.2789e-02,\n",
       "           1.3602e-01, -5.7536e-02, -1.0664e-01,  2.9895e-01,  9.0617e-02,\n",
       "          -2.4501e-01,  6.2611e-02,  9.3418e-02, -3.0951e-01, -1.2341e-01,\n",
       "           3.8067e-02, -1.7876e-01,  4.6486e-01, -1.7777e-01, -7.2212e-02,\n",
       "           3.2726e-01, -3.2004e-01, -2.6162e-02, -1.7617e-01,  4.4617e-01,\n",
       "          -3.3485e-02, -8.0312e-02,  1.2325e-01, -1.3496e-01,  2.0807e-01,\n",
       "          -6.2653e-01,  3.0064e-01, -2.3005e-01, -2.4519e-01, -1.9420e-01,\n",
       "           4.5950e-01, -6.5656e-01, -7.4801e-02, -1.3110e-01, -1.3971e-01,\n",
       "          -3.7797e-01,  5.9950e-02, -1.9938e-02,  1.0263e-01,  2.8390e-01,\n",
       "          -3.7863e-02, -1.5375e-01,  1.1312e-01,  2.1486e-01,  3.8871e-01,\n",
       "          -2.5118e-01,  1.1082e-01, -2.9860e-01, -6.2103e-02,  2.1795e-02,\n",
       "           2.4248e-01,  3.7907e-01, -1.0378e-01, -1.7678e-01, -7.0341e-02,\n",
       "           2.8019e-01, -1.2278e-01,  2.2996e-01,  1.3770e-01,  4.2115e-01,\n",
       "          -3.6226e-01, -6.6510e-01, -1.9985e-02, -1.3854e-01,  1.9808e-01,\n",
       "          -6.1745e-01, -3.9260e-01, -7.5302e-02,  2.5463e-01, -6.3912e-02,\n",
       "          -1.0777e-01,  2.0373e-01,  1.7897e-02, -5.0587e-01, -2.4789e-01,\n",
       "           1.0027e-01,  3.4695e+00, -1.2569e-01, -7.9449e-02,  8.5338e-02,\n",
       "           4.5396e-02,  1.8641e-01, -4.3182e-01, -2.8186e-02, -6.3559e-01,\n",
       "          -6.4887e-01, -6.9457e-02,  4.8287e-01, -5.8451e-01,  2.1489e-01,\n",
       "           1.6621e-01, -6.8065e-02,  4.4866e-01, -3.4670e-02,  3.2352e-01,\n",
       "          -3.7933e-01,  1.3323e-02,  3.2149e-01,  2.9157e-01, -1.3252e+00,\n",
       "          -4.1968e-03, -3.6521e-02,  2.1551e-01, -2.0766e-01,  3.7599e-01,\n",
       "           2.2564e-01,  5.2119e-01, -4.4355e-02, -1.8952e-01,  2.5574e-01,\n",
       "           1.1558e+00, -1.8362e-01, -2.5785e-01,  3.3655e-01, -5.9746e-03,\n",
       "          -2.3840e-01,  4.8814e-02, -3.9995e-01, -5.1703e-02,  9.0431e-02,\n",
       "          -1.4829e-01,  2.6324e-01, -2.1299e-01, -2.0745e-01,  2.1008e-01,\n",
       "           4.3979e-01, -4.1771e-01,  1.6865e-01, -1.4575e-03,  1.6424e-01,\n",
       "          -2.7560e-02, -2.9673e-01,  4.4356e-01,  3.8644e-01, -1.7177e-02,\n",
       "           3.0430e-01, -1.3036e-01,  5.8990e-01, -2.6672e-02,  1.3839e-01,\n",
       "          -2.1776e-01,  2.5886e-01, -1.0365e-01,  5.0708e-01,  7.1468e-02,\n",
       "          -2.8835e-01,  9.0630e-03,  5.4950e-01,  1.3183e-02,  7.8507e-02,\n",
       "          -2.4742e-02, -2.8229e-01,  6.9248e-02,  1.3588e-01, -8.1964e-02,\n",
       "          -1.2618e-01,  1.7700e-01, -1.8295e-01, -3.7349e-01, -3.0257e-02,\n",
       "           4.9901e-01, -4.6310e-02, -1.0667e-01,  2.5187e-01, -2.4974e-01,\n",
       "          -1.0153e+00,  2.5389e-01, -1.6947e-01,  9.5279e-01,  2.2441e-01,\n",
       "           1.8631e-02, -3.5385e-01, -3.7518e-03, -1.3832e-03, -1.4773e-01,\n",
       "           1.1633e-01, -1.4843e-02,  3.2812e-01,  7.6172e-02, -8.9335e-02,\n",
       "          -2.7072e-02, -3.8204e-01,  1.4398e-01, -2.2048e-01,  2.6724e-01,\n",
       "           1.7931e-01,  1.9831e-02, -3.5856e-02,  3.1362e-01, -5.6898e-02,\n",
       "           2.9256e-01,  1.3741e-01,  1.1482e-01, -2.2795e-01,  6.1906e-02,\n",
       "          -3.8382e-01,  9.9245e-02,  2.6809e-02,  1.2444e-01, -1.0488e-01,\n",
       "          -6.3618e-02, -1.7001e-01, -9.3777e-02, -4.4260e-01,  2.7408e-01,\n",
       "          -2.6038e-01,  5.7491e-02,  4.8378e-02,  2.6667e-02, -2.7939e-01,\n",
       "          -2.1347e-01, -5.7146e-01, -4.2780e-01,  3.9927e-01, -2.3433e-01,\n",
       "          -3.4036e-01, -2.6126e-01, -2.2133e-01,  3.1489e-01,  1.0589e-01,\n",
       "          -2.3049e-01,  3.3393e-01,  3.9580e-02,  6.9268e-02,  1.0786e-01,\n",
       "           4.3680e-02, -1.3661e-03,  4.0690e-02,  3.1856e-01,  4.8938e-02,\n",
       "          -4.7885e-01,  1.4441e-01,  9.0219e-01, -6.4605e-02,  3.9035e-01,\n",
       "          -1.3006e-01,  2.2442e-01, -1.6776e-01, -9.6538e-02, -3.2596e-01,\n",
       "           2.4414e-01,  1.7990e-01,  1.3210e-01, -2.7508e-02, -1.6708e-01,\n",
       "          -7.2453e-02,  3.6112e-01,  1.7543e-03,  2.1864e-01,  3.5058e-02,\n",
       "          -1.1467e-01, -2.9627e-01, -9.2460e-03,  1.7361e-02, -1.5156e-01,\n",
       "           4.3672e-01, -1.0810e-01,  2.1949e-01, -1.6031e-01,  7.9630e-02,\n",
       "          -2.6704e-01,  9.6436e-02, -9.1067e-02, -3.4006e-02, -1.0135e-01,\n",
       "          -1.3300e-01, -1.1336e-01,  2.4301e-01, -3.6827e-02, -7.1317e-03,\n",
       "           3.4798e-01,  2.6985e-03,  3.1138e-02,  1.8100e-01,  2.7475e-01,\n",
       "          -6.9054e-02, -6.8759e-02, -7.0484e-02,  6.8572e-02, -3.5036e-01,\n",
       "           2.1605e-02, -2.0104e-02, -1.4014e-03,  1.6381e-01,  5.8007e-01,\n",
       "          -5.6445e-01, -3.6321e-02, -1.2807e-01, -2.9857e-02, -1.2163e-01,\n",
       "          -1.0122e-01,  1.7037e-01,  1.1601e-01, -8.2954e-02,  2.6946e-03,\n",
       "          -1.1136e-01, -8.7299e-02, -2.0346e-01, -1.9766e-01, -2.6130e-02,\n",
       "          -4.3175e-01,  4.3006e-01, -1.5023e-01,  2.6643e-01,  1.0258e-01,\n",
       "          -5.2727e-01,  5.2501e-01, -2.9109e-01,  1.4633e-02,  5.9105e-02,\n",
       "           1.1609e-01,  2.1430e-02, -1.7106e-01, -1.4557e-01,  5.3713e-02,\n",
       "           2.6238e-01, -4.4832e-02,  2.7818e-01,  3.5873e-01, -1.1511e-01,\n",
       "           3.5144e-01, -4.1552e-01, -6.7309e-02, -2.8997e-01, -2.2453e-01,\n",
       "           3.9258e-02,  1.5291e-01, -2.7494e-02, -1.2718e-01,  1.2670e-01,\n",
       "          -2.5984e-01, -8.0735e-02,  2.9160e-01, -4.9139e-02, -2.7177e-02,\n",
       "          -1.5466e-01,  3.1755e-01, -4.1531e-01,  5.7373e-02,  3.9311e-02,\n",
       "          -3.1150e-02, -8.1021e-02,  1.2103e-01, -3.4716e-01,  3.1155e-01,\n",
       "          -2.5113e-01,  3.3502e-01], device='cuda:0')],\n",
       " [tensor([ 5.1199e-01,  6.7217e-02, -6.2822e-01,  2.0367e-01,  3.5700e-02,\n",
       "           1.2200e-02, -1.7521e-01,  4.4324e-04, -6.8688e-02, -1.1810e-01,\n",
       "          -3.5943e-02, -1.3074e-02,  5.2857e-02, -2.8560e-01,  1.5273e-01,\n",
       "          -2.8706e-01,  8.2016e-02, -1.2833e-01,  4.5680e-01,  3.5889e-01,\n",
       "          -1.1988e-01, -1.2642e-01, -1.9967e-01,  2.5266e-01,  3.5789e-01,\n",
       "          -1.8242e-02,  3.3421e-02, -3.7464e-01, -1.7979e-01,  2.7704e-01,\n",
       "           2.5598e-01,  2.5221e-01,  1.6680e-01,  4.9888e-02, -1.0321e-01,\n",
       "           2.2811e-01,  3.0461e-02,  3.0009e-02, -8.6265e-02,  7.8788e-02,\n",
       "           1.2989e-01, -1.0888e-01, -1.2430e-01, -4.4398e-02,  4.5020e-01,\n",
       "           2.6943e-01, -1.8908e-01,  7.7852e-02, -3.0184e-01, -1.7944e-01,\n",
       "           1.3145e-01,  1.8982e-01, -1.8014e-02,  2.6812e-01,  1.1116e-01,\n",
       "           1.8091e-01, -1.4851e-01,  7.7178e-02,  7.3749e-02,  1.7807e-01,\n",
       "          -2.0870e-01,  2.7113e-03,  3.7070e-01,  2.3924e-01,  2.9215e-01,\n",
       "          -1.9263e-03,  7.5360e-02,  9.1701e-02,  1.1798e-01, -3.8894e-02,\n",
       "           9.3340e-02,  2.8528e-01,  2.3927e-01,  1.1966e-02,  2.1036e-01,\n",
       "           6.3470e-03, -1.1353e-01,  2.0703e-01, -1.0335e-01, -1.5467e-01,\n",
       "           3.3455e-01, -1.4482e-01,  1.2664e-01, -3.2243e-01,  7.7489e-03,\n",
       "           2.2305e-01,  2.3832e-01, -2.3618e-01, -3.2453e-01,  1.3344e-01,\n",
       "           2.4659e-01, -1.7344e-03, -8.2906e-02,  1.6583e-01,  1.8659e-01,\n",
       "           8.1166e-02,  1.0294e-01, -3.1155e-01,  1.7839e+00,  8.6899e-02,\n",
       "           7.2251e-02, -1.7992e-01, -5.0660e-01,  5.6370e-02, -2.4217e-01,\n",
       "           3.4459e-01,  2.0594e-01, -4.3753e-02, -1.0069e-01, -1.7036e-01,\n",
       "          -1.6493e-01,  2.7466e-01,  5.7009e-02,  8.5543e-02, -1.8079e-01,\n",
       "           7.2932e-03,  3.3404e-02, -1.3353e-01,  1.9937e-01,  1.9785e-01,\n",
       "           5.2704e-02, -3.0257e-01, -1.1293e-01,  5.3753e-03, -5.9776e-02,\n",
       "          -2.4693e-01,  2.2928e-01, -3.4245e-01,  2.1027e-01, -3.4180e-01,\n",
       "           2.1754e-01,  2.7348e-02,  1.4005e-03,  2.6408e-01, -2.5132e-01,\n",
       "          -2.2898e-01,  1.4256e-01,  5.8775e-03, -1.6325e-01,  6.9918e-02,\n",
       "          -3.6745e-02, -1.4522e-02,  2.8980e-01, -2.7921e-02,  1.5737e-01,\n",
       "          -1.9639e-01, -1.9266e-01, -2.7696e-01,  1.7241e-01, -4.3368e-01,\n",
       "          -2.2806e-01,  2.2295e-02, -3.3013e-01,  1.2354e-01, -4.2327e-02,\n",
       "          -2.9068e-01,  6.3014e-02,  2.4697e-01,  9.3234e-03, -1.3259e-01,\n",
       "          -3.5415e-02, -2.0506e-03,  2.5672e-01, -1.8524e-01, -1.8568e-03,\n",
       "           1.3712e-01,  7.4625e-02, -2.1933e-02,  2.2335e-01,  1.7142e-01,\n",
       "          -3.5628e-01,  6.8933e-02,  9.1631e-02, -1.9857e-01, -1.9137e-01,\n",
       "           6.3319e-02, -1.7657e-01,  2.6075e-01, -8.5149e-02, -1.0594e-01,\n",
       "           1.1396e-01, -1.8425e-01, -7.1094e-02, -1.1425e-01,  3.4482e-01,\n",
       "           5.8817e-02, -1.0522e-01,  1.0239e-01, -1.4164e-01,  1.1239e-03,\n",
       "          -2.9062e-01,  3.7796e-01, -4.7504e-01, -2.4936e-01, -1.5992e-01,\n",
       "           2.4897e-01, -4.2938e-01, -9.5678e-02, -1.2342e-01, -2.8082e-02,\n",
       "          -2.4457e-01,  1.1679e-03,  1.2541e-01,  1.1309e-01,  1.7459e-01,\n",
       "          -5.4428e-02, -1.8425e-01,  8.9083e-02,  1.7859e-01,  3.5044e-01,\n",
       "          -1.4093e-01, -7.3836e-02, -1.8842e-01,  1.0473e-01,  1.1690e-01,\n",
       "           2.2390e-01,  2.6289e-01, -1.3752e-01, -2.0178e-01, -7.2094e-02,\n",
       "           2.2588e-01, -9.7305e-02,  1.9384e-01,  1.3331e-01,  2.9979e-01,\n",
       "          -2.1996e-01, -1.3837e-01, -8.6046e-02, -3.5572e-02,  8.3026e-02,\n",
       "          -3.7732e-01, -2.6070e-01, -7.1706e-02,  2.5786e-01, -7.4200e-02,\n",
       "          -1.7178e-01,  1.1079e-01,  7.7694e-02, -2.7876e-01, -2.3120e-01,\n",
       "          -2.6540e-02,  2.2528e+00, -5.8911e-02, -4.7647e-02,  1.7766e-02,\n",
       "           5.0872e-02,  1.2243e-01, -2.8358e-01, -5.8081e-02, -5.9568e-02,\n",
       "          -4.9095e-01,  9.5258e-02,  3.9939e-01, -3.3053e-01,  1.1624e-01,\n",
       "           7.4713e-02, -2.2068e-01,  3.9781e-01, -2.2691e-02,  2.0983e-01,\n",
       "          -3.5292e-01,  6.4390e-02,  1.3993e-01,  1.6917e-01, -9.5128e-01,\n",
       "          -8.9082e-02, -8.9976e-02,  1.4344e-01, -1.0179e-01,  1.4457e-01,\n",
       "           1.1434e-01,  3.8826e-01, -5.1193e-03, -7.2353e-02,  3.1514e-01,\n",
       "           6.9131e-01, -1.7789e-01, -1.2838e-01,  2.2159e-01, -4.9201e-02,\n",
       "          -1.7699e-01,  2.2713e-02, -1.9996e-01,  6.2268e-02,  5.7276e-02,\n",
       "           2.8217e-02,  1.2388e-01, -7.9870e-02, -9.3750e-02,  2.2735e-02,\n",
       "           3.2727e-01, -2.5552e-01,  2.5481e-01,  3.0350e-02,  1.0637e-01,\n",
       "           1.1261e-01, -1.7137e-01,  2.5317e-01,  1.7071e-01, -7.8992e-02,\n",
       "           2.0338e-01, -7.4795e-03,  5.6608e-01, -6.3063e-02,  1.1983e-01,\n",
       "          -3.8464e-01,  1.2004e-01, -7.8010e-02,  4.1221e-01, -1.6460e-01,\n",
       "          -3.3751e-01,  4.6604e-02,  3.6122e-01, -1.7714e-02,  6.2197e-02,\n",
       "          -2.3326e-02, -1.8835e-01,  4.0574e-02,  6.3516e-02, -1.2420e-01,\n",
       "          -1.4674e-01,  1.9444e-01, -9.1703e-02, -3.7505e-01,  4.4699e-02,\n",
       "           3.0831e-01, -8.2783e-02, -5.7083e-02,  8.3300e-02, -3.7451e-02,\n",
       "          -5.6810e-01,  3.3395e-01, -1.6767e-01,  6.6744e-01,  3.6663e-01,\n",
       "          -3.7911e-02, -2.4347e-01, -6.3793e-02, -2.1815e-02, -1.7477e-01,\n",
       "           8.1840e-02,  8.6021e-02,  1.7668e-01,  1.9756e-01, -2.5598e-01,\n",
       "          -1.1381e-01, -3.4735e-01,  1.2064e-01, -4.9369e-03,  3.7298e-01,\n",
       "           6.6049e-02,  1.7251e-01, -1.3744e-01,  2.2471e-01,  6.2586e-02,\n",
       "           3.3100e-01,  1.1605e-01, -7.6892e-02, -1.1532e-01,  2.6284e-02,\n",
       "          -1.9663e-01,  1.3052e-01, -3.9807e-02,  2.4890e-01, -1.4221e-01,\n",
       "          -6.9413e-03, -9.4677e-02,  6.7206e-02, -4.4246e-01,  2.0318e-01,\n",
       "          -5.4312e-02,  2.2955e-01, -1.1185e-01,  4.5975e-02, -3.0779e-01,\n",
       "          -2.0885e-01, -5.5737e-01, -2.5313e-01,  1.2569e-01, -2.0181e-01,\n",
       "          -2.5140e-01, -2.1319e-01, -2.3389e-01,  2.5118e-01,  1.0444e-01,\n",
       "          -9.3709e-02,  2.6567e-01, -9.2356e-03,  8.5706e-02,  2.9555e-02,\n",
       "          -1.0901e-03,  1.0854e-01,  1.4911e-01,  1.0911e-01,  4.6317e-03,\n",
       "          -4.5364e-01,  6.4951e-02,  5.1204e-01,  4.5070e-02,  3.5309e-01,\n",
       "          -2.6169e-01,  1.5892e-01, -1.8759e-01,  8.0931e-02, -1.5673e-01,\n",
       "           3.1781e-01,  2.4186e-01,  1.4195e-01, -5.7960e-02, -2.1879e-01,\n",
       "          -3.9357e-02,  2.9624e-01,  6.9045e-02,  4.2252e-02, -1.3590e-01,\n",
       "          -2.0413e-01, -2.9312e-01, -1.0235e-01,  1.8207e-02, -2.2969e-01,\n",
       "           4.6641e-01, -9.0372e-02,  3.6463e-02, -1.0181e-01,  1.4460e-01,\n",
       "          -1.8688e-01,  3.1896e-02, -4.6111e-02, -6.0641e-02,  1.0143e-01,\n",
       "          -9.1146e-02,  9.6946e-02,  2.2511e-01,  6.5458e-02,  3.0277e-02,\n",
       "           2.5575e-01, -1.4143e-03, -6.1967e-02,  1.6394e-01,  2.6592e-01,\n",
       "          -7.1706e-02,  1.2125e-01,  6.1986e-02, -5.4316e-02, -2.9374e-01,\n",
       "          -2.5845e-02,  3.6862e-02,  8.3491e-03,  1.1467e-01,  4.3105e-01,\n",
       "          -2.5261e-01, -8.2594e-02, -4.5061e-02, -3.9239e-02, -6.1770e-02,\n",
       "          -8.9522e-02,  1.6852e-01,  7.5903e-02, -6.2312e-02,  1.7686e-02,\n",
       "          -5.5122e-02, -1.3073e-01, -1.5270e-01, -5.3549e-02,  5.3696e-02,\n",
       "          -1.7208e-01,  4.1693e-01, -1.0629e-01,  2.4776e-01, -8.4640e-02,\n",
       "          -3.8824e-01,  2.6061e-01, -1.5364e-01, -3.6149e-02,  1.2991e-01,\n",
       "           1.6077e-01,  7.8555e-03, -1.7734e-01, -2.5661e-02, -2.8434e-02,\n",
       "           1.4266e-01,  6.3670e-02,  3.5782e-01,  3.5624e-01, -1.4273e-01,\n",
       "           4.0379e-01, -1.6081e-02,  3.6598e-03, -2.1655e-01, -2.2918e-01,\n",
       "           1.0139e-01,  1.5702e-01,  4.5970e-02, -2.2864e-01, -1.5470e-02,\n",
       "          -1.3869e-01,  7.9072e-02, -1.8118e-02, -1.2263e-01, -3.7886e-02,\n",
       "          -2.2483e-01,  3.2867e-01, -3.7717e-01,  3.4737e-02, -5.8808e-02,\n",
       "          -9.2526e-02, -1.8681e-01,  4.8722e-02, -2.5632e-01,  2.3552e-01,\n",
       "          -1.9595e-01,  1.7809e-01], device='cuda:0'),\n",
       "  tensor([ 5.2274e-01,  1.5641e-01, -1.0848e+00,  1.1177e-01,  7.5653e-02,\n",
       "           9.4285e-02, -1.8001e-01,  2.5528e-02, -2.4492e-01, -1.7891e-01,\n",
       "          -4.5921e-02,  2.5578e-02,  1.8064e-01, -4.5970e-01,  6.3076e-02,\n",
       "          -3.7772e-01,  1.5441e-01, -2.4951e-01,  4.7971e-01,  3.5050e-01,\n",
       "          -2.6991e-01, -9.2889e-02, -1.7606e-01,  1.7152e-01,  2.3409e-01,\n",
       "          -7.8755e-02, -4.0918e-02, -3.0925e-01, -1.6300e-01,  3.8787e-01,\n",
       "           2.5585e-01,  3.3782e-01,  1.5435e-01,  7.4610e-02, -2.3172e-02,\n",
       "           1.3508e-01,  2.6527e-02, -4.3903e-02, -6.8146e-02,  2.9305e-01,\n",
       "           2.3502e-02, -1.9274e-01, -7.0365e-02, -3.2197e-01,  5.4049e-01,\n",
       "           3.2196e-01, -2.7936e-01,  7.5554e-02, -1.5153e-01, -2.3615e-01,\n",
       "           2.3531e-01,  1.5285e-01, -2.6181e-01,  1.7080e-01,  2.8013e-01,\n",
       "           1.7029e-01, -1.4272e-02,  1.2108e-02, -1.6482e-01,  8.8154e-02,\n",
       "          -2.0103e-01, -1.1609e-01,  4.7431e-01,  3.1184e-01,  4.3271e-01,\n",
       "           9.0316e-02,  1.8462e-01,  1.6195e-01,  6.8520e-02, -1.0234e-01,\n",
       "          -9.5794e-02,  4.6709e-01,  4.8865e-01, -5.3075e-02,  1.7724e-01,\n",
       "          -1.0268e-01,  2.9243e-02,  3.3458e-01, -1.4299e-01, -1.4205e-01,\n",
       "           5.5196e-01, -1.6875e-01,  1.9743e-01, -2.6752e-01, -1.1521e-02,\n",
       "           1.7574e-01,  6.4551e-02, -5.0148e-01, -3.4393e-01,  2.6587e-01,\n",
       "           4.6449e-01, -5.0084e-04,  8.8365e-03,  2.4609e-01,  2.6972e-01,\n",
       "           8.8671e-05,  1.6340e-01, -3.4826e-01,  2.7051e+00,  2.1946e-01,\n",
       "           1.7937e-01, -2.9697e-01, -4.5259e-01,  2.4084e-01, -2.9354e-01,\n",
       "           4.4238e-01,  3.2129e-01, -7.7020e-02, -3.8553e-02, -1.4583e-01,\n",
       "          -3.7871e-01,  2.1882e-01,  1.4687e-01,  9.9550e-02, -1.0327e-01,\n",
       "           4.0365e-02,  1.8728e-01, -1.0975e-01,  4.5076e-01,  2.5217e-01,\n",
       "           1.8275e-01, -3.9115e-01, -3.5903e-01, -1.4331e-01, -1.1253e-01,\n",
       "          -2.5967e-01,  2.5023e-01, -4.1895e-01,  1.8815e-01, -2.4978e-01,\n",
       "           4.5059e-01,  1.3987e-01, -1.1408e-01,  2.5577e-01, -1.5558e-01,\n",
       "          -2.2749e-01,  2.1994e-01, -2.3140e-01, -4.2276e-01,  1.9363e-01,\n",
       "          -1.8481e-01, -9.6807e-03,  2.1645e-01,  1.5031e-01,  2.4152e-01,\n",
       "          -4.2557e-01, -3.0346e-01, -2.2863e-01,  2.3039e-01, -6.4430e-01,\n",
       "          -2.1962e-01, -7.2523e-02, -3.3888e-01,  4.6646e-02, -3.4023e-02,\n",
       "          -1.6126e-01,  1.8565e-01,  2.4995e-01, -7.5664e-02, -1.1935e-01,\n",
       "          -4.3957e-02,  8.8616e-02,  2.8147e-01, -1.9149e-01,  5.2789e-02,\n",
       "           1.3602e-01, -5.7536e-02, -1.0664e-01,  2.9895e-01,  9.0617e-02,\n",
       "          -2.4501e-01,  6.2611e-02,  9.3418e-02, -3.0951e-01, -1.2341e-01,\n",
       "           3.8067e-02, -1.7876e-01,  4.6486e-01, -1.7777e-01, -7.2212e-02,\n",
       "           3.2726e-01, -3.2004e-01, -2.6162e-02, -1.7617e-01,  4.4617e-01,\n",
       "          -3.3485e-02, -8.0312e-02,  1.2325e-01, -1.3496e-01,  2.0807e-01,\n",
       "          -6.2653e-01,  3.0064e-01, -2.3005e-01, -2.4519e-01, -1.9420e-01,\n",
       "           4.5950e-01, -6.5656e-01, -7.4801e-02, -1.3110e-01, -1.3971e-01,\n",
       "          -3.7797e-01,  5.9950e-02, -1.9938e-02,  1.0263e-01,  2.8390e-01,\n",
       "          -3.7863e-02, -1.5375e-01,  1.1312e-01,  2.1486e-01,  3.8871e-01,\n",
       "          -2.5118e-01,  1.1082e-01, -2.9860e-01, -6.2103e-02,  2.1795e-02,\n",
       "           2.4248e-01,  3.7907e-01, -1.0378e-01, -1.7678e-01, -7.0341e-02,\n",
       "           2.8019e-01, -1.2278e-01,  2.2996e-01,  1.3770e-01,  4.2115e-01,\n",
       "          -3.6226e-01, -6.6510e-01, -1.9985e-02, -1.3854e-01,  1.9808e-01,\n",
       "          -6.1745e-01, -3.9260e-01, -7.5302e-02,  2.5463e-01, -6.3912e-02,\n",
       "          -1.0777e-01,  2.0373e-01,  1.7897e-02, -5.0587e-01, -2.4789e-01,\n",
       "           1.0027e-01,  3.4695e+00, -1.2569e-01, -7.9449e-02,  8.5338e-02,\n",
       "           4.5396e-02,  1.8641e-01, -4.3182e-01, -2.8186e-02, -6.3559e-01,\n",
       "          -6.4887e-01, -6.9457e-02,  4.8287e-01, -5.8451e-01,  2.1489e-01,\n",
       "           1.6621e-01, -6.8065e-02,  4.4866e-01, -3.4670e-02,  3.2352e-01,\n",
       "          -3.7933e-01,  1.3323e-02,  3.2149e-01,  2.9157e-01, -1.3252e+00,\n",
       "          -4.1968e-03, -3.6521e-02,  2.1551e-01, -2.0766e-01,  3.7599e-01,\n",
       "           2.2564e-01,  5.2119e-01, -4.4355e-02, -1.8952e-01,  2.5574e-01,\n",
       "           1.1558e+00, -1.8362e-01, -2.5785e-01,  3.3655e-01, -5.9746e-03,\n",
       "          -2.3840e-01,  4.8814e-02, -3.9995e-01, -5.1703e-02,  9.0431e-02,\n",
       "          -1.4829e-01,  2.6324e-01, -2.1299e-01, -2.0745e-01,  2.1008e-01,\n",
       "           4.3979e-01, -4.1771e-01,  1.6865e-01, -1.4575e-03,  1.6424e-01,\n",
       "          -2.7560e-02, -2.9673e-01,  4.4356e-01,  3.8644e-01, -1.7177e-02,\n",
       "           3.0430e-01, -1.3036e-01,  5.8990e-01, -2.6672e-02,  1.3839e-01,\n",
       "          -2.1776e-01,  2.5886e-01, -1.0365e-01,  5.0708e-01,  7.1468e-02,\n",
       "          -2.8835e-01,  9.0630e-03,  5.4950e-01,  1.3183e-02,  7.8507e-02,\n",
       "          -2.4742e-02, -2.8229e-01,  6.9248e-02,  1.3588e-01, -8.1964e-02,\n",
       "          -1.2618e-01,  1.7700e-01, -1.8295e-01, -3.7349e-01, -3.0257e-02,\n",
       "           4.9901e-01, -4.6310e-02, -1.0667e-01,  2.5187e-01, -2.4974e-01,\n",
       "          -1.0153e+00,  2.5389e-01, -1.6947e-01,  9.5279e-01,  2.2441e-01,\n",
       "           1.8631e-02, -3.5385e-01, -3.7518e-03, -1.3832e-03, -1.4773e-01,\n",
       "           1.1633e-01, -1.4843e-02,  3.2812e-01,  7.6172e-02, -8.9335e-02,\n",
       "          -2.7072e-02, -3.8204e-01,  1.4398e-01, -2.2048e-01,  2.6724e-01,\n",
       "           1.7931e-01,  1.9831e-02, -3.5856e-02,  3.1362e-01, -5.6898e-02,\n",
       "           2.9256e-01,  1.3741e-01,  1.1482e-01, -2.2795e-01,  6.1906e-02,\n",
       "          -3.8382e-01,  9.9245e-02,  2.6809e-02,  1.2444e-01, -1.0488e-01,\n",
       "          -6.3618e-02, -1.7001e-01, -9.3777e-02, -4.4260e-01,  2.7408e-01,\n",
       "          -2.6038e-01,  5.7491e-02,  4.8378e-02,  2.6667e-02, -2.7939e-01,\n",
       "          -2.1347e-01, -5.7146e-01, -4.2780e-01,  3.9927e-01, -2.3433e-01,\n",
       "          -3.4036e-01, -2.6126e-01, -2.2133e-01,  3.1489e-01,  1.0589e-01,\n",
       "          -2.3049e-01,  3.3393e-01,  3.9580e-02,  6.9268e-02,  1.0786e-01,\n",
       "           4.3680e-02, -1.3661e-03,  4.0690e-02,  3.1856e-01,  4.8938e-02,\n",
       "          -4.7885e-01,  1.4441e-01,  9.0219e-01, -6.4605e-02,  3.9035e-01,\n",
       "          -1.3006e-01,  2.2442e-01, -1.6776e-01, -9.6538e-02, -3.2596e-01,\n",
       "           2.4414e-01,  1.7990e-01,  1.3210e-01, -2.7508e-02, -1.6708e-01,\n",
       "          -7.2453e-02,  3.6112e-01,  1.7543e-03,  2.1864e-01,  3.5058e-02,\n",
       "          -1.1467e-01, -2.9627e-01, -9.2460e-03,  1.7361e-02, -1.5156e-01,\n",
       "           4.3672e-01, -1.0810e-01,  2.1949e-01, -1.6031e-01,  7.9630e-02,\n",
       "          -2.6704e-01,  9.6436e-02, -9.1067e-02, -3.4006e-02, -1.0135e-01,\n",
       "          -1.3300e-01, -1.1336e-01,  2.4301e-01, -3.6827e-02, -7.1317e-03,\n",
       "           3.4798e-01,  2.6985e-03,  3.1138e-02,  1.8100e-01,  2.7475e-01,\n",
       "          -6.9054e-02, -6.8759e-02, -7.0484e-02,  6.8572e-02, -3.5036e-01,\n",
       "           2.1605e-02, -2.0104e-02, -1.4014e-03,  1.6381e-01,  5.8007e-01,\n",
       "          -5.6445e-01, -3.6321e-02, -1.2807e-01, -2.9857e-02, -1.2163e-01,\n",
       "          -1.0122e-01,  1.7037e-01,  1.1601e-01, -8.2954e-02,  2.6946e-03,\n",
       "          -1.1136e-01, -8.7299e-02, -2.0346e-01, -1.9766e-01, -2.6130e-02,\n",
       "          -4.3175e-01,  4.3006e-01, -1.5023e-01,  2.6643e-01,  1.0258e-01,\n",
       "          -5.2727e-01,  5.2501e-01, -2.9109e-01,  1.4633e-02,  5.9105e-02,\n",
       "           1.1609e-01,  2.1430e-02, -1.7106e-01, -1.4557e-01,  5.3713e-02,\n",
       "           2.6238e-01, -4.4832e-02,  2.7818e-01,  3.5873e-01, -1.1511e-01,\n",
       "           3.5144e-01, -4.1552e-01, -6.7309e-02, -2.8997e-01, -2.2453e-01,\n",
       "           3.9258e-02,  1.5291e-01, -2.7494e-02, -1.2718e-01,  1.2670e-01,\n",
       "          -2.5984e-01, -8.0735e-02,  2.9160e-01, -4.9139e-02, -2.7177e-02,\n",
       "          -1.5466e-01,  3.1755e-01, -4.1531e-01,  5.7373e-02,  3.9311e-02,\n",
       "          -3.1150e-02, -8.1021e-02,  1.2103e-01, -3.4716e-01,  3.1155e-01,\n",
       "          -2.5113e-01,  3.3502e-01], device='cuda:0')],\n",
       " [tensor([ 5.1199e-01,  6.7217e-02, -6.2822e-01,  2.0367e-01,  3.5700e-02,\n",
       "           1.2200e-02, -1.7521e-01,  4.4324e-04, -6.8688e-02, -1.1810e-01,\n",
       "          -3.5943e-02, -1.3074e-02,  5.2857e-02, -2.8560e-01,  1.5273e-01,\n",
       "          -2.8706e-01,  8.2016e-02, -1.2833e-01,  4.5680e-01,  3.5889e-01,\n",
       "          -1.1988e-01, -1.2642e-01, -1.9967e-01,  2.5266e-01,  3.5789e-01,\n",
       "          -1.8242e-02,  3.3421e-02, -3.7464e-01, -1.7979e-01,  2.7704e-01,\n",
       "           2.5598e-01,  2.5221e-01,  1.6680e-01,  4.9888e-02, -1.0321e-01,\n",
       "           2.2811e-01,  3.0461e-02,  3.0009e-02, -8.6265e-02,  7.8788e-02,\n",
       "           1.2989e-01, -1.0888e-01, -1.2430e-01, -4.4398e-02,  4.5020e-01,\n",
       "           2.6943e-01, -1.8908e-01,  7.7852e-02, -3.0184e-01, -1.7944e-01,\n",
       "           1.3145e-01,  1.8982e-01, -1.8014e-02,  2.6812e-01,  1.1116e-01,\n",
       "           1.8091e-01, -1.4851e-01,  7.7178e-02,  7.3749e-02,  1.7807e-01,\n",
       "          -2.0870e-01,  2.7113e-03,  3.7070e-01,  2.3924e-01,  2.9215e-01,\n",
       "          -1.9263e-03,  7.5360e-02,  9.1701e-02,  1.1798e-01, -3.8894e-02,\n",
       "           9.3340e-02,  2.8528e-01,  2.3927e-01,  1.1966e-02,  2.1036e-01,\n",
       "           6.3470e-03, -1.1353e-01,  2.0703e-01, -1.0335e-01, -1.5467e-01,\n",
       "           3.3455e-01, -1.4482e-01,  1.2664e-01, -3.2243e-01,  7.7489e-03,\n",
       "           2.2305e-01,  2.3832e-01, -2.3618e-01, -3.2453e-01,  1.3344e-01,\n",
       "           2.4659e-01, -1.7344e-03, -8.2906e-02,  1.6583e-01,  1.8659e-01,\n",
       "           8.1166e-02,  1.0294e-01, -3.1155e-01,  1.7839e+00,  8.6899e-02,\n",
       "           7.2251e-02, -1.7992e-01, -5.0660e-01,  5.6370e-02, -2.4217e-01,\n",
       "           3.4459e-01,  2.0594e-01, -4.3753e-02, -1.0069e-01, -1.7036e-01,\n",
       "          -1.6493e-01,  2.7466e-01,  5.7009e-02,  8.5543e-02, -1.8079e-01,\n",
       "           7.2932e-03,  3.3404e-02, -1.3353e-01,  1.9937e-01,  1.9785e-01,\n",
       "           5.2704e-02, -3.0257e-01, -1.1293e-01,  5.3753e-03, -5.9776e-02,\n",
       "          -2.4693e-01,  2.2928e-01, -3.4245e-01,  2.1027e-01, -3.4180e-01,\n",
       "           2.1754e-01,  2.7348e-02,  1.4005e-03,  2.6408e-01, -2.5132e-01,\n",
       "          -2.2898e-01,  1.4256e-01,  5.8775e-03, -1.6325e-01,  6.9918e-02,\n",
       "          -3.6745e-02, -1.4522e-02,  2.8980e-01, -2.7921e-02,  1.5737e-01,\n",
       "          -1.9639e-01, -1.9266e-01, -2.7696e-01,  1.7241e-01, -4.3368e-01,\n",
       "          -2.2806e-01,  2.2295e-02, -3.3013e-01,  1.2354e-01, -4.2327e-02,\n",
       "          -2.9068e-01,  6.3014e-02,  2.4697e-01,  9.3234e-03, -1.3259e-01,\n",
       "          -3.5415e-02, -2.0506e-03,  2.5672e-01, -1.8524e-01, -1.8568e-03,\n",
       "           1.3712e-01,  7.4625e-02, -2.1933e-02,  2.2335e-01,  1.7142e-01,\n",
       "          -3.5628e-01,  6.8933e-02,  9.1631e-02, -1.9857e-01, -1.9137e-01,\n",
       "           6.3319e-02, -1.7657e-01,  2.6075e-01, -8.5149e-02, -1.0594e-01,\n",
       "           1.1396e-01, -1.8425e-01, -7.1094e-02, -1.1425e-01,  3.4482e-01,\n",
       "           5.8817e-02, -1.0522e-01,  1.0239e-01, -1.4164e-01,  1.1239e-03,\n",
       "          -2.9062e-01,  3.7796e-01, -4.7504e-01, -2.4936e-01, -1.5992e-01,\n",
       "           2.4897e-01, -4.2938e-01, -9.5678e-02, -1.2342e-01, -2.8082e-02,\n",
       "          -2.4457e-01,  1.1679e-03,  1.2541e-01,  1.1309e-01,  1.7459e-01,\n",
       "          -5.4428e-02, -1.8425e-01,  8.9083e-02,  1.7859e-01,  3.5044e-01,\n",
       "          -1.4093e-01, -7.3836e-02, -1.8842e-01,  1.0473e-01,  1.1690e-01,\n",
       "           2.2390e-01,  2.6289e-01, -1.3752e-01, -2.0178e-01, -7.2094e-02,\n",
       "           2.2588e-01, -9.7305e-02,  1.9384e-01,  1.3331e-01,  2.9979e-01,\n",
       "          -2.1996e-01, -1.3837e-01, -8.6046e-02, -3.5572e-02,  8.3026e-02,\n",
       "          -3.7732e-01, -2.6070e-01, -7.1706e-02,  2.5786e-01, -7.4200e-02,\n",
       "          -1.7178e-01,  1.1079e-01,  7.7694e-02, -2.7876e-01, -2.3120e-01,\n",
       "          -2.6540e-02,  2.2528e+00, -5.8911e-02, -4.7647e-02,  1.7766e-02,\n",
       "           5.0872e-02,  1.2243e-01, -2.8358e-01, -5.8081e-02, -5.9568e-02,\n",
       "          -4.9095e-01,  9.5258e-02,  3.9939e-01, -3.3053e-01,  1.1624e-01,\n",
       "           7.4713e-02, -2.2068e-01,  3.9781e-01, -2.2691e-02,  2.0983e-01,\n",
       "          -3.5292e-01,  6.4390e-02,  1.3993e-01,  1.6917e-01, -9.5128e-01,\n",
       "          -8.9082e-02, -8.9976e-02,  1.4344e-01, -1.0179e-01,  1.4457e-01,\n",
       "           1.1434e-01,  3.8826e-01, -5.1193e-03, -7.2353e-02,  3.1514e-01,\n",
       "           6.9131e-01, -1.7789e-01, -1.2838e-01,  2.2159e-01, -4.9201e-02,\n",
       "          -1.7699e-01,  2.2713e-02, -1.9996e-01,  6.2268e-02,  5.7276e-02,\n",
       "           2.8217e-02,  1.2388e-01, -7.9870e-02, -9.3750e-02,  2.2735e-02,\n",
       "           3.2727e-01, -2.5552e-01,  2.5481e-01,  3.0350e-02,  1.0637e-01,\n",
       "           1.1261e-01, -1.7137e-01,  2.5317e-01,  1.7071e-01, -7.8992e-02,\n",
       "           2.0338e-01, -7.4795e-03,  5.6608e-01, -6.3063e-02,  1.1983e-01,\n",
       "          -3.8464e-01,  1.2004e-01, -7.8010e-02,  4.1221e-01, -1.6460e-01,\n",
       "          -3.3751e-01,  4.6604e-02,  3.6122e-01, -1.7714e-02,  6.2197e-02,\n",
       "          -2.3326e-02, -1.8835e-01,  4.0574e-02,  6.3516e-02, -1.2420e-01,\n",
       "          -1.4674e-01,  1.9444e-01, -9.1703e-02, -3.7505e-01,  4.4699e-02,\n",
       "           3.0831e-01, -8.2783e-02, -5.7083e-02,  8.3300e-02, -3.7451e-02,\n",
       "          -5.6810e-01,  3.3395e-01, -1.6767e-01,  6.6744e-01,  3.6663e-01,\n",
       "          -3.7911e-02, -2.4347e-01, -6.3793e-02, -2.1815e-02, -1.7477e-01,\n",
       "           8.1840e-02,  8.6021e-02,  1.7668e-01,  1.9756e-01, -2.5598e-01,\n",
       "          -1.1381e-01, -3.4735e-01,  1.2064e-01, -4.9369e-03,  3.7298e-01,\n",
       "           6.6049e-02,  1.7251e-01, -1.3744e-01,  2.2471e-01,  6.2586e-02,\n",
       "           3.3100e-01,  1.1605e-01, -7.6892e-02, -1.1532e-01,  2.6284e-02,\n",
       "          -1.9663e-01,  1.3052e-01, -3.9807e-02,  2.4890e-01, -1.4221e-01,\n",
       "          -6.9413e-03, -9.4677e-02,  6.7206e-02, -4.4246e-01,  2.0318e-01,\n",
       "          -5.4312e-02,  2.2955e-01, -1.1185e-01,  4.5975e-02, -3.0779e-01,\n",
       "          -2.0885e-01, -5.5737e-01, -2.5313e-01,  1.2569e-01, -2.0181e-01,\n",
       "          -2.5140e-01, -2.1319e-01, -2.3389e-01,  2.5118e-01,  1.0444e-01,\n",
       "          -9.3709e-02,  2.6567e-01, -9.2356e-03,  8.5706e-02,  2.9555e-02,\n",
       "          -1.0901e-03,  1.0854e-01,  1.4911e-01,  1.0911e-01,  4.6317e-03,\n",
       "          -4.5364e-01,  6.4951e-02,  5.1204e-01,  4.5070e-02,  3.5309e-01,\n",
       "          -2.6169e-01,  1.5892e-01, -1.8759e-01,  8.0931e-02, -1.5673e-01,\n",
       "           3.1781e-01,  2.4186e-01,  1.4195e-01, -5.7960e-02, -2.1879e-01,\n",
       "          -3.9357e-02,  2.9624e-01,  6.9045e-02,  4.2252e-02, -1.3590e-01,\n",
       "          -2.0413e-01, -2.9312e-01, -1.0235e-01,  1.8207e-02, -2.2969e-01,\n",
       "           4.6641e-01, -9.0372e-02,  3.6463e-02, -1.0181e-01,  1.4460e-01,\n",
       "          -1.8688e-01,  3.1896e-02, -4.6111e-02, -6.0641e-02,  1.0143e-01,\n",
       "          -9.1146e-02,  9.6946e-02,  2.2511e-01,  6.5458e-02,  3.0277e-02,\n",
       "           2.5575e-01, -1.4143e-03, -6.1967e-02,  1.6394e-01,  2.6592e-01,\n",
       "          -7.1706e-02,  1.2125e-01,  6.1986e-02, -5.4316e-02, -2.9374e-01,\n",
       "          -2.5845e-02,  3.6862e-02,  8.3491e-03,  1.1467e-01,  4.3105e-01,\n",
       "          -2.5261e-01, -8.2594e-02, -4.5061e-02, -3.9239e-02, -6.1770e-02,\n",
       "          -8.9522e-02,  1.6852e-01,  7.5903e-02, -6.2312e-02,  1.7686e-02,\n",
       "          -5.5122e-02, -1.3073e-01, -1.5270e-01, -5.3549e-02,  5.3696e-02,\n",
       "          -1.7208e-01,  4.1693e-01, -1.0629e-01,  2.4776e-01, -8.4640e-02,\n",
       "          -3.8824e-01,  2.6061e-01, -1.5364e-01, -3.6149e-02,  1.2991e-01,\n",
       "           1.6077e-01,  7.8555e-03, -1.7734e-01, -2.5661e-02, -2.8434e-02,\n",
       "           1.4266e-01,  6.3670e-02,  3.5782e-01,  3.5624e-01, -1.4273e-01,\n",
       "           4.0379e-01, -1.6081e-02,  3.6598e-03, -2.1655e-01, -2.2918e-01,\n",
       "           1.0139e-01,  1.5702e-01,  4.5970e-02, -2.2864e-01, -1.5470e-02,\n",
       "          -1.3869e-01,  7.9072e-02, -1.8118e-02, -1.2263e-01, -3.7886e-02,\n",
       "          -2.2483e-01,  3.2867e-01, -3.7717e-01,  3.4737e-02, -5.8808e-02,\n",
       "          -9.2526e-02, -1.8681e-01,  4.8722e-02, -2.5632e-01,  2.3552e-01,\n",
       "          -1.9595e-01,  1.7809e-01], device='cuda:0'),\n",
       "  tensor([ 5.2274e-01,  1.5641e-01, -1.0848e+00,  1.1177e-01,  7.5653e-02,\n",
       "           9.4285e-02, -1.8001e-01,  2.5528e-02, -2.4492e-01, -1.7891e-01,\n",
       "          -4.5921e-02,  2.5578e-02,  1.8064e-01, -4.5970e-01,  6.3076e-02,\n",
       "          -3.7772e-01,  1.5441e-01, -2.4951e-01,  4.7971e-01,  3.5050e-01,\n",
       "          -2.6991e-01, -9.2889e-02, -1.7606e-01,  1.7152e-01,  2.3409e-01,\n",
       "          -7.8755e-02, -4.0918e-02, -3.0925e-01, -1.6300e-01,  3.8787e-01,\n",
       "           2.5585e-01,  3.3782e-01,  1.5435e-01,  7.4610e-02, -2.3172e-02,\n",
       "           1.3508e-01,  2.6527e-02, -4.3903e-02, -6.8146e-02,  2.9305e-01,\n",
       "           2.3502e-02, -1.9274e-01, -7.0365e-02, -3.2197e-01,  5.4049e-01,\n",
       "           3.2196e-01, -2.7936e-01,  7.5554e-02, -1.5153e-01, -2.3615e-01,\n",
       "           2.3531e-01,  1.5285e-01, -2.6181e-01,  1.7080e-01,  2.8013e-01,\n",
       "           1.7029e-01, -1.4272e-02,  1.2108e-02, -1.6482e-01,  8.8154e-02,\n",
       "          -2.0103e-01, -1.1609e-01,  4.7431e-01,  3.1184e-01,  4.3271e-01,\n",
       "           9.0316e-02,  1.8462e-01,  1.6195e-01,  6.8520e-02, -1.0234e-01,\n",
       "          -9.5794e-02,  4.6709e-01,  4.8865e-01, -5.3075e-02,  1.7724e-01,\n",
       "          -1.0268e-01,  2.9243e-02,  3.3458e-01, -1.4299e-01, -1.4205e-01,\n",
       "           5.5196e-01, -1.6875e-01,  1.9743e-01, -2.6752e-01, -1.1521e-02,\n",
       "           1.7574e-01,  6.4551e-02, -5.0148e-01, -3.4393e-01,  2.6587e-01,\n",
       "           4.6449e-01, -5.0084e-04,  8.8365e-03,  2.4609e-01,  2.6972e-01,\n",
       "           8.8671e-05,  1.6340e-01, -3.4826e-01,  2.7051e+00,  2.1946e-01,\n",
       "           1.7937e-01, -2.9697e-01, -4.5259e-01,  2.4084e-01, -2.9354e-01,\n",
       "           4.4238e-01,  3.2129e-01, -7.7020e-02, -3.8553e-02, -1.4583e-01,\n",
       "          -3.7871e-01,  2.1882e-01,  1.4687e-01,  9.9550e-02, -1.0327e-01,\n",
       "           4.0365e-02,  1.8728e-01, -1.0975e-01,  4.5076e-01,  2.5217e-01,\n",
       "           1.8275e-01, -3.9115e-01, -3.5903e-01, -1.4331e-01, -1.1253e-01,\n",
       "          -2.5967e-01,  2.5023e-01, -4.1895e-01,  1.8815e-01, -2.4978e-01,\n",
       "           4.5059e-01,  1.3987e-01, -1.1408e-01,  2.5577e-01, -1.5558e-01,\n",
       "          -2.2749e-01,  2.1994e-01, -2.3140e-01, -4.2276e-01,  1.9363e-01,\n",
       "          -1.8481e-01, -9.6807e-03,  2.1645e-01,  1.5031e-01,  2.4152e-01,\n",
       "          -4.2557e-01, -3.0346e-01, -2.2863e-01,  2.3039e-01, -6.4430e-01,\n",
       "          -2.1962e-01, -7.2523e-02, -3.3888e-01,  4.6646e-02, -3.4023e-02,\n",
       "          -1.6126e-01,  1.8565e-01,  2.4995e-01, -7.5664e-02, -1.1935e-01,\n",
       "          -4.3957e-02,  8.8616e-02,  2.8147e-01, -1.9149e-01,  5.2789e-02,\n",
       "           1.3602e-01, -5.7536e-02, -1.0664e-01,  2.9895e-01,  9.0617e-02,\n",
       "          -2.4501e-01,  6.2611e-02,  9.3418e-02, -3.0951e-01, -1.2341e-01,\n",
       "           3.8067e-02, -1.7876e-01,  4.6486e-01, -1.7777e-01, -7.2212e-02,\n",
       "           3.2726e-01, -3.2004e-01, -2.6162e-02, -1.7617e-01,  4.4617e-01,\n",
       "          -3.3485e-02, -8.0312e-02,  1.2325e-01, -1.3496e-01,  2.0807e-01,\n",
       "          -6.2653e-01,  3.0064e-01, -2.3005e-01, -2.4519e-01, -1.9420e-01,\n",
       "           4.5950e-01, -6.5656e-01, -7.4801e-02, -1.3110e-01, -1.3971e-01,\n",
       "          -3.7797e-01,  5.9950e-02, -1.9938e-02,  1.0263e-01,  2.8390e-01,\n",
       "          -3.7863e-02, -1.5375e-01,  1.1312e-01,  2.1486e-01,  3.8871e-01,\n",
       "          -2.5118e-01,  1.1082e-01, -2.9860e-01, -6.2103e-02,  2.1795e-02,\n",
       "           2.4248e-01,  3.7907e-01, -1.0378e-01, -1.7678e-01, -7.0341e-02,\n",
       "           2.8019e-01, -1.2278e-01,  2.2996e-01,  1.3770e-01,  4.2115e-01,\n",
       "          -3.6226e-01, -6.6510e-01, -1.9985e-02, -1.3854e-01,  1.9808e-01,\n",
       "          -6.1745e-01, -3.9260e-01, -7.5302e-02,  2.5463e-01, -6.3912e-02,\n",
       "          -1.0777e-01,  2.0373e-01,  1.7897e-02, -5.0587e-01, -2.4789e-01,\n",
       "           1.0027e-01,  3.4695e+00, -1.2569e-01, -7.9449e-02,  8.5338e-02,\n",
       "           4.5396e-02,  1.8641e-01, -4.3182e-01, -2.8186e-02, -6.3559e-01,\n",
       "          -6.4887e-01, -6.9457e-02,  4.8287e-01, -5.8451e-01,  2.1489e-01,\n",
       "           1.6621e-01, -6.8065e-02,  4.4866e-01, -3.4670e-02,  3.2352e-01,\n",
       "          -3.7933e-01,  1.3323e-02,  3.2149e-01,  2.9157e-01, -1.3252e+00,\n",
       "          -4.1968e-03, -3.6521e-02,  2.1551e-01, -2.0766e-01,  3.7599e-01,\n",
       "           2.2564e-01,  5.2119e-01, -4.4355e-02, -1.8952e-01,  2.5574e-01,\n",
       "           1.1558e+00, -1.8362e-01, -2.5785e-01,  3.3655e-01, -5.9746e-03,\n",
       "          -2.3840e-01,  4.8814e-02, -3.9995e-01, -5.1703e-02,  9.0431e-02,\n",
       "          -1.4829e-01,  2.6324e-01, -2.1299e-01, -2.0745e-01,  2.1008e-01,\n",
       "           4.3979e-01, -4.1771e-01,  1.6865e-01, -1.4575e-03,  1.6424e-01,\n",
       "          -2.7560e-02, -2.9673e-01,  4.4356e-01,  3.8644e-01, -1.7177e-02,\n",
       "           3.0430e-01, -1.3036e-01,  5.8990e-01, -2.6672e-02,  1.3839e-01,\n",
       "          -2.1776e-01,  2.5886e-01, -1.0365e-01,  5.0708e-01,  7.1468e-02,\n",
       "          -2.8835e-01,  9.0630e-03,  5.4950e-01,  1.3183e-02,  7.8507e-02,\n",
       "          -2.4742e-02, -2.8229e-01,  6.9248e-02,  1.3588e-01, -8.1964e-02,\n",
       "          -1.2618e-01,  1.7700e-01, -1.8295e-01, -3.7349e-01, -3.0257e-02,\n",
       "           4.9901e-01, -4.6310e-02, -1.0667e-01,  2.5187e-01, -2.4974e-01,\n",
       "          -1.0153e+00,  2.5389e-01, -1.6947e-01,  9.5279e-01,  2.2441e-01,\n",
       "           1.8631e-02, -3.5385e-01, -3.7518e-03, -1.3832e-03, -1.4773e-01,\n",
       "           1.1633e-01, -1.4843e-02,  3.2812e-01,  7.6172e-02, -8.9335e-02,\n",
       "          -2.7072e-02, -3.8204e-01,  1.4398e-01, -2.2048e-01,  2.6724e-01,\n",
       "           1.7931e-01,  1.9831e-02, -3.5856e-02,  3.1362e-01, -5.6898e-02,\n",
       "           2.9256e-01,  1.3741e-01,  1.1482e-01, -2.2795e-01,  6.1906e-02,\n",
       "          -3.8382e-01,  9.9245e-02,  2.6809e-02,  1.2444e-01, -1.0488e-01,\n",
       "          -6.3618e-02, -1.7001e-01, -9.3777e-02, -4.4260e-01,  2.7408e-01,\n",
       "          -2.6038e-01,  5.7491e-02,  4.8378e-02,  2.6667e-02, -2.7939e-01,\n",
       "          -2.1347e-01, -5.7146e-01, -4.2780e-01,  3.9927e-01, -2.3433e-01,\n",
       "          -3.4036e-01, -2.6126e-01, -2.2133e-01,  3.1489e-01,  1.0589e-01,\n",
       "          -2.3049e-01,  3.3393e-01,  3.9580e-02,  6.9268e-02,  1.0786e-01,\n",
       "           4.3680e-02, -1.3661e-03,  4.0690e-02,  3.1856e-01,  4.8938e-02,\n",
       "          -4.7885e-01,  1.4441e-01,  9.0219e-01, -6.4605e-02,  3.9035e-01,\n",
       "          -1.3006e-01,  2.2442e-01, -1.6776e-01, -9.6538e-02, -3.2596e-01,\n",
       "           2.4414e-01,  1.7990e-01,  1.3210e-01, -2.7508e-02, -1.6708e-01,\n",
       "          -7.2453e-02,  3.6112e-01,  1.7543e-03,  2.1864e-01,  3.5058e-02,\n",
       "          -1.1467e-01, -2.9627e-01, -9.2460e-03,  1.7361e-02, -1.5156e-01,\n",
       "           4.3672e-01, -1.0810e-01,  2.1949e-01, -1.6031e-01,  7.9630e-02,\n",
       "          -2.6704e-01,  9.6436e-02, -9.1067e-02, -3.4006e-02, -1.0135e-01,\n",
       "          -1.3300e-01, -1.1336e-01,  2.4301e-01, -3.6827e-02, -7.1317e-03,\n",
       "           3.4798e-01,  2.6985e-03,  3.1138e-02,  1.8100e-01,  2.7475e-01,\n",
       "          -6.9054e-02, -6.8759e-02, -7.0484e-02,  6.8572e-02, -3.5036e-01,\n",
       "           2.1605e-02, -2.0104e-02, -1.4014e-03,  1.6381e-01,  5.8007e-01,\n",
       "          -5.6445e-01, -3.6321e-02, -1.2807e-01, -2.9857e-02, -1.2163e-01,\n",
       "          -1.0122e-01,  1.7037e-01,  1.1601e-01, -8.2954e-02,  2.6946e-03,\n",
       "          -1.1136e-01, -8.7299e-02, -2.0346e-01, -1.9766e-01, -2.6130e-02,\n",
       "          -4.3175e-01,  4.3006e-01, -1.5023e-01,  2.6643e-01,  1.0258e-01,\n",
       "          -5.2727e-01,  5.2501e-01, -2.9109e-01,  1.4633e-02,  5.9105e-02,\n",
       "           1.1609e-01,  2.1430e-02, -1.7106e-01, -1.4557e-01,  5.3713e-02,\n",
       "           2.6238e-01, -4.4832e-02,  2.7818e-01,  3.5873e-01, -1.1511e-01,\n",
       "           3.5144e-01, -4.1552e-01, -6.7309e-02, -2.8997e-01, -2.2453e-01,\n",
       "           3.9258e-02,  1.5291e-01, -2.7494e-02, -1.2718e-01,  1.2670e-01,\n",
       "          -2.5984e-01, -8.0735e-02,  2.9160e-01, -4.9139e-02, -2.7177e-02,\n",
       "          -1.5466e-01,  3.1755e-01, -4.1531e-01,  5.7373e-02,  3.9311e-02,\n",
       "          -3.1150e-02, -8.1021e-02,  1.2103e-01, -3.4716e-01,  3.1155e-01,\n",
       "          -2.5113e-01,  3.3502e-01], device='cuda:0')],\n",
       " [tensor([ 5.1199e-01,  6.7217e-02, -6.2822e-01,  2.0367e-01,  3.5700e-02,\n",
       "           1.2200e-02, -1.7521e-01,  4.4324e-04, -6.8688e-02, -1.1810e-01,\n",
       "          -3.5943e-02, -1.3074e-02,  5.2857e-02, -2.8560e-01,  1.5273e-01,\n",
       "          -2.8706e-01,  8.2016e-02, -1.2833e-01,  4.5680e-01,  3.5889e-01,\n",
       "          -1.1988e-01, -1.2642e-01, -1.9967e-01,  2.5266e-01,  3.5789e-01,\n",
       "          -1.8242e-02,  3.3421e-02, -3.7464e-01, -1.7979e-01,  2.7704e-01,\n",
       "           2.5598e-01,  2.5221e-01,  1.6680e-01,  4.9888e-02, -1.0321e-01,\n",
       "           2.2811e-01,  3.0461e-02,  3.0009e-02, -8.6265e-02,  7.8788e-02,\n",
       "           1.2989e-01, -1.0888e-01, -1.2430e-01, -4.4398e-02,  4.5020e-01,\n",
       "           2.6943e-01, -1.8908e-01,  7.7852e-02, -3.0184e-01, -1.7944e-01,\n",
       "           1.3145e-01,  1.8982e-01, -1.8014e-02,  2.6812e-01,  1.1116e-01,\n",
       "           1.8091e-01, -1.4851e-01,  7.7178e-02,  7.3749e-02,  1.7807e-01,\n",
       "          -2.0870e-01,  2.7113e-03,  3.7070e-01,  2.3924e-01,  2.9215e-01,\n",
       "          -1.9263e-03,  7.5360e-02,  9.1701e-02,  1.1798e-01, -3.8894e-02,\n",
       "           9.3340e-02,  2.8528e-01,  2.3927e-01,  1.1966e-02,  2.1036e-01,\n",
       "           6.3470e-03, -1.1353e-01,  2.0703e-01, -1.0335e-01, -1.5467e-01,\n",
       "           3.3455e-01, -1.4482e-01,  1.2664e-01, -3.2243e-01,  7.7489e-03,\n",
       "           2.2305e-01,  2.3832e-01, -2.3618e-01, -3.2453e-01,  1.3344e-01,\n",
       "           2.4659e-01, -1.7344e-03, -8.2906e-02,  1.6583e-01,  1.8659e-01,\n",
       "           8.1166e-02,  1.0294e-01, -3.1155e-01,  1.7839e+00,  8.6899e-02,\n",
       "           7.2251e-02, -1.7992e-01, -5.0660e-01,  5.6370e-02, -2.4217e-01,\n",
       "           3.4459e-01,  2.0594e-01, -4.3753e-02, -1.0069e-01, -1.7036e-01,\n",
       "          -1.6493e-01,  2.7466e-01,  5.7009e-02,  8.5543e-02, -1.8079e-01,\n",
       "           7.2932e-03,  3.3404e-02, -1.3353e-01,  1.9937e-01,  1.9785e-01,\n",
       "           5.2704e-02, -3.0257e-01, -1.1293e-01,  5.3753e-03, -5.9776e-02,\n",
       "          -2.4693e-01,  2.2928e-01, -3.4245e-01,  2.1027e-01, -3.4180e-01,\n",
       "           2.1754e-01,  2.7348e-02,  1.4005e-03,  2.6408e-01, -2.5132e-01,\n",
       "          -2.2898e-01,  1.4256e-01,  5.8775e-03, -1.6325e-01,  6.9918e-02,\n",
       "          -3.6745e-02, -1.4522e-02,  2.8980e-01, -2.7921e-02,  1.5737e-01,\n",
       "          -1.9639e-01, -1.9266e-01, -2.7696e-01,  1.7241e-01, -4.3368e-01,\n",
       "          -2.2806e-01,  2.2295e-02, -3.3013e-01,  1.2354e-01, -4.2327e-02,\n",
       "          -2.9068e-01,  6.3014e-02,  2.4697e-01,  9.3234e-03, -1.3259e-01,\n",
       "          -3.5415e-02, -2.0506e-03,  2.5672e-01, -1.8524e-01, -1.8568e-03,\n",
       "           1.3712e-01,  7.4625e-02, -2.1933e-02,  2.2335e-01,  1.7142e-01,\n",
       "          -3.5628e-01,  6.8933e-02,  9.1631e-02, -1.9857e-01, -1.9137e-01,\n",
       "           6.3319e-02, -1.7657e-01,  2.6075e-01, -8.5149e-02, -1.0594e-01,\n",
       "           1.1396e-01, -1.8425e-01, -7.1094e-02, -1.1425e-01,  3.4482e-01,\n",
       "           5.8817e-02, -1.0522e-01,  1.0239e-01, -1.4164e-01,  1.1239e-03,\n",
       "          -2.9062e-01,  3.7796e-01, -4.7504e-01, -2.4936e-01, -1.5992e-01,\n",
       "           2.4897e-01, -4.2938e-01, -9.5678e-02, -1.2342e-01, -2.8082e-02,\n",
       "          -2.4457e-01,  1.1679e-03,  1.2541e-01,  1.1309e-01,  1.7459e-01,\n",
       "          -5.4428e-02, -1.8425e-01,  8.9083e-02,  1.7859e-01,  3.5044e-01,\n",
       "          -1.4093e-01, -7.3836e-02, -1.8842e-01,  1.0473e-01,  1.1690e-01,\n",
       "           2.2390e-01,  2.6289e-01, -1.3752e-01, -2.0178e-01, -7.2094e-02,\n",
       "           2.2588e-01, -9.7305e-02,  1.9384e-01,  1.3331e-01,  2.9979e-01,\n",
       "          -2.1996e-01, -1.3837e-01, -8.6046e-02, -3.5572e-02,  8.3026e-02,\n",
       "          -3.7732e-01, -2.6070e-01, -7.1706e-02,  2.5786e-01, -7.4200e-02,\n",
       "          -1.7178e-01,  1.1079e-01,  7.7694e-02, -2.7876e-01, -2.3120e-01,\n",
       "          -2.6540e-02,  2.2528e+00, -5.8911e-02, -4.7647e-02,  1.7766e-02,\n",
       "           5.0872e-02,  1.2243e-01, -2.8358e-01, -5.8081e-02, -5.9568e-02,\n",
       "          -4.9095e-01,  9.5258e-02,  3.9939e-01, -3.3053e-01,  1.1624e-01,\n",
       "           7.4713e-02, -2.2068e-01,  3.9781e-01, -2.2691e-02,  2.0983e-01,\n",
       "          -3.5292e-01,  6.4390e-02,  1.3993e-01,  1.6917e-01, -9.5128e-01,\n",
       "          -8.9082e-02, -8.9976e-02,  1.4344e-01, -1.0179e-01,  1.4457e-01,\n",
       "           1.1434e-01,  3.8826e-01, -5.1193e-03, -7.2353e-02,  3.1514e-01,\n",
       "           6.9131e-01, -1.7789e-01, -1.2838e-01,  2.2159e-01, -4.9201e-02,\n",
       "          -1.7699e-01,  2.2713e-02, -1.9996e-01,  6.2268e-02,  5.7276e-02,\n",
       "           2.8217e-02,  1.2388e-01, -7.9870e-02, -9.3750e-02,  2.2735e-02,\n",
       "           3.2727e-01, -2.5552e-01,  2.5481e-01,  3.0350e-02,  1.0637e-01,\n",
       "           1.1261e-01, -1.7137e-01,  2.5317e-01,  1.7071e-01, -7.8992e-02,\n",
       "           2.0338e-01, -7.4795e-03,  5.6608e-01, -6.3063e-02,  1.1983e-01,\n",
       "          -3.8464e-01,  1.2004e-01, -7.8010e-02,  4.1221e-01, -1.6460e-01,\n",
       "          -3.3751e-01,  4.6604e-02,  3.6122e-01, -1.7714e-02,  6.2197e-02,\n",
       "          -2.3326e-02, -1.8835e-01,  4.0574e-02,  6.3516e-02, -1.2420e-01,\n",
       "          -1.4674e-01,  1.9444e-01, -9.1703e-02, -3.7505e-01,  4.4699e-02,\n",
       "           3.0831e-01, -8.2783e-02, -5.7083e-02,  8.3300e-02, -3.7451e-02,\n",
       "          -5.6810e-01,  3.3395e-01, -1.6767e-01,  6.6744e-01,  3.6663e-01,\n",
       "          -3.7911e-02, -2.4347e-01, -6.3793e-02, -2.1815e-02, -1.7477e-01,\n",
       "           8.1840e-02,  8.6021e-02,  1.7668e-01,  1.9756e-01, -2.5598e-01,\n",
       "          -1.1381e-01, -3.4735e-01,  1.2064e-01, -4.9369e-03,  3.7298e-01,\n",
       "           6.6049e-02,  1.7251e-01, -1.3744e-01,  2.2471e-01,  6.2586e-02,\n",
       "           3.3100e-01,  1.1605e-01, -7.6892e-02, -1.1532e-01,  2.6284e-02,\n",
       "          -1.9663e-01,  1.3052e-01, -3.9807e-02,  2.4890e-01, -1.4221e-01,\n",
       "          -6.9413e-03, -9.4677e-02,  6.7206e-02, -4.4246e-01,  2.0318e-01,\n",
       "          -5.4312e-02,  2.2955e-01, -1.1185e-01,  4.5975e-02, -3.0779e-01,\n",
       "          -2.0885e-01, -5.5737e-01, -2.5313e-01,  1.2569e-01, -2.0181e-01,\n",
       "          -2.5140e-01, -2.1319e-01, -2.3389e-01,  2.5118e-01,  1.0444e-01,\n",
       "          -9.3709e-02,  2.6567e-01, -9.2356e-03,  8.5706e-02,  2.9555e-02,\n",
       "          -1.0901e-03,  1.0854e-01,  1.4911e-01,  1.0911e-01,  4.6317e-03,\n",
       "          -4.5364e-01,  6.4951e-02,  5.1204e-01,  4.5070e-02,  3.5309e-01,\n",
       "          -2.6169e-01,  1.5892e-01, -1.8759e-01,  8.0931e-02, -1.5673e-01,\n",
       "           3.1781e-01,  2.4186e-01,  1.4195e-01, -5.7960e-02, -2.1879e-01,\n",
       "          -3.9357e-02,  2.9624e-01,  6.9045e-02,  4.2252e-02, -1.3590e-01,\n",
       "          -2.0413e-01, -2.9312e-01, -1.0235e-01,  1.8207e-02, -2.2969e-01,\n",
       "           4.6641e-01, -9.0372e-02,  3.6463e-02, -1.0181e-01,  1.4460e-01,\n",
       "          -1.8688e-01,  3.1896e-02, -4.6111e-02, -6.0641e-02,  1.0143e-01,\n",
       "          -9.1146e-02,  9.6946e-02,  2.2511e-01,  6.5458e-02,  3.0277e-02,\n",
       "           2.5575e-01, -1.4143e-03, -6.1967e-02,  1.6394e-01,  2.6592e-01,\n",
       "          -7.1706e-02,  1.2125e-01,  6.1986e-02, -5.4316e-02, -2.9374e-01,\n",
       "          -2.5845e-02,  3.6862e-02,  8.3491e-03,  1.1467e-01,  4.3105e-01,\n",
       "          -2.5261e-01, -8.2594e-02, -4.5061e-02, -3.9239e-02, -6.1770e-02,\n",
       "          -8.9522e-02,  1.6852e-01,  7.5903e-02, -6.2312e-02,  1.7686e-02,\n",
       "          -5.5122e-02, -1.3073e-01, -1.5270e-01, -5.3549e-02,  5.3696e-02,\n",
       "          -1.7208e-01,  4.1693e-01, -1.0629e-01,  2.4776e-01, -8.4640e-02,\n",
       "          -3.8824e-01,  2.6061e-01, -1.5364e-01, -3.6149e-02,  1.2991e-01,\n",
       "           1.6077e-01,  7.8555e-03, -1.7734e-01, -2.5661e-02, -2.8434e-02,\n",
       "           1.4266e-01,  6.3670e-02,  3.5782e-01,  3.5624e-01, -1.4273e-01,\n",
       "           4.0379e-01, -1.6081e-02,  3.6598e-03, -2.1655e-01, -2.2918e-01,\n",
       "           1.0139e-01,  1.5702e-01,  4.5970e-02, -2.2864e-01, -1.5470e-02,\n",
       "          -1.3869e-01,  7.9072e-02, -1.8118e-02, -1.2263e-01, -3.7886e-02,\n",
       "          -2.2483e-01,  3.2867e-01, -3.7717e-01,  3.4737e-02, -5.8808e-02,\n",
       "          -9.2526e-02, -1.8681e-01,  4.8722e-02, -2.5632e-01,  2.3552e-01,\n",
       "          -1.9595e-01,  1.7809e-01], device='cuda:0'),\n",
       "  tensor([ 5.2274e-01,  1.5641e-01, -1.0848e+00,  1.1177e-01,  7.5653e-02,\n",
       "           9.4285e-02, -1.8001e-01,  2.5528e-02, -2.4492e-01, -1.7891e-01,\n",
       "          -4.5921e-02,  2.5578e-02,  1.8064e-01, -4.5970e-01,  6.3076e-02,\n",
       "          -3.7772e-01,  1.5441e-01, -2.4951e-01,  4.7971e-01,  3.5050e-01,\n",
       "          -2.6991e-01, -9.2889e-02, -1.7606e-01,  1.7152e-01,  2.3409e-01,\n",
       "          -7.8755e-02, -4.0918e-02, -3.0925e-01, -1.6300e-01,  3.8787e-01,\n",
       "           2.5585e-01,  3.3782e-01,  1.5435e-01,  7.4610e-02, -2.3172e-02,\n",
       "           1.3508e-01,  2.6527e-02, -4.3903e-02, -6.8146e-02,  2.9305e-01,\n",
       "           2.3502e-02, -1.9274e-01, -7.0365e-02, -3.2197e-01,  5.4049e-01,\n",
       "           3.2196e-01, -2.7936e-01,  7.5554e-02, -1.5153e-01, -2.3615e-01,\n",
       "           2.3531e-01,  1.5285e-01, -2.6181e-01,  1.7080e-01,  2.8013e-01,\n",
       "           1.7029e-01, -1.4272e-02,  1.2108e-02, -1.6482e-01,  8.8154e-02,\n",
       "          -2.0103e-01, -1.1609e-01,  4.7431e-01,  3.1184e-01,  4.3271e-01,\n",
       "           9.0316e-02,  1.8462e-01,  1.6195e-01,  6.8520e-02, -1.0234e-01,\n",
       "          -9.5794e-02,  4.6709e-01,  4.8865e-01, -5.3075e-02,  1.7724e-01,\n",
       "          -1.0268e-01,  2.9243e-02,  3.3458e-01, -1.4299e-01, -1.4205e-01,\n",
       "           5.5196e-01, -1.6875e-01,  1.9743e-01, -2.6752e-01, -1.1521e-02,\n",
       "           1.7574e-01,  6.4551e-02, -5.0148e-01, -3.4393e-01,  2.6587e-01,\n",
       "           4.6449e-01, -5.0084e-04,  8.8365e-03,  2.4609e-01,  2.6972e-01,\n",
       "           8.8671e-05,  1.6340e-01, -3.4826e-01,  2.7051e+00,  2.1946e-01,\n",
       "           1.7937e-01, -2.9697e-01, -4.5259e-01,  2.4084e-01, -2.9354e-01,\n",
       "           4.4238e-01,  3.2129e-01, -7.7020e-02, -3.8553e-02, -1.4583e-01,\n",
       "          -3.7871e-01,  2.1882e-01,  1.4687e-01,  9.9550e-02, -1.0327e-01,\n",
       "           4.0365e-02,  1.8728e-01, -1.0975e-01,  4.5076e-01,  2.5217e-01,\n",
       "           1.8275e-01, -3.9115e-01, -3.5903e-01, -1.4331e-01, -1.1253e-01,\n",
       "          -2.5967e-01,  2.5023e-01, -4.1895e-01,  1.8815e-01, -2.4978e-01,\n",
       "           4.5059e-01,  1.3987e-01, -1.1408e-01,  2.5577e-01, -1.5558e-01,\n",
       "          -2.2749e-01,  2.1994e-01, -2.3140e-01, -4.2276e-01,  1.9363e-01,\n",
       "          -1.8481e-01, -9.6807e-03,  2.1645e-01,  1.5031e-01,  2.4152e-01,\n",
       "          -4.2557e-01, -3.0346e-01, -2.2863e-01,  2.3039e-01, -6.4430e-01,\n",
       "          -2.1962e-01, -7.2523e-02, -3.3888e-01,  4.6646e-02, -3.4023e-02,\n",
       "          -1.6126e-01,  1.8565e-01,  2.4995e-01, -7.5664e-02, -1.1935e-01,\n",
       "          -4.3957e-02,  8.8616e-02,  2.8147e-01, -1.9149e-01,  5.2789e-02,\n",
       "           1.3602e-01, -5.7536e-02, -1.0664e-01,  2.9895e-01,  9.0617e-02,\n",
       "          -2.4501e-01,  6.2611e-02,  9.3418e-02, -3.0951e-01, -1.2341e-01,\n",
       "           3.8067e-02, -1.7876e-01,  4.6486e-01, -1.7777e-01, -7.2212e-02,\n",
       "           3.2726e-01, -3.2004e-01, -2.6162e-02, -1.7617e-01,  4.4617e-01,\n",
       "          -3.3485e-02, -8.0312e-02,  1.2325e-01, -1.3496e-01,  2.0807e-01,\n",
       "          -6.2653e-01,  3.0064e-01, -2.3005e-01, -2.4519e-01, -1.9420e-01,\n",
       "           4.5950e-01, -6.5656e-01, -7.4801e-02, -1.3110e-01, -1.3971e-01,\n",
       "          -3.7797e-01,  5.9950e-02, -1.9938e-02,  1.0263e-01,  2.8390e-01,\n",
       "          -3.7863e-02, -1.5375e-01,  1.1312e-01,  2.1486e-01,  3.8871e-01,\n",
       "          -2.5118e-01,  1.1082e-01, -2.9860e-01, -6.2103e-02,  2.1795e-02,\n",
       "           2.4248e-01,  3.7907e-01, -1.0378e-01, -1.7678e-01, -7.0341e-02,\n",
       "           2.8019e-01, -1.2278e-01,  2.2996e-01,  1.3770e-01,  4.2115e-01,\n",
       "          -3.6226e-01, -6.6510e-01, -1.9985e-02, -1.3854e-01,  1.9808e-01,\n",
       "          -6.1745e-01, -3.9260e-01, -7.5302e-02,  2.5463e-01, -6.3912e-02,\n",
       "          -1.0777e-01,  2.0373e-01,  1.7897e-02, -5.0587e-01, -2.4789e-01,\n",
       "           1.0027e-01,  3.4695e+00, -1.2569e-01, -7.9449e-02,  8.5338e-02,\n",
       "           4.5396e-02,  1.8641e-01, -4.3182e-01, -2.8186e-02, -6.3559e-01,\n",
       "          -6.4887e-01, -6.9457e-02,  4.8287e-01, -5.8451e-01,  2.1489e-01,\n",
       "           1.6621e-01, -6.8065e-02,  4.4866e-01, -3.4670e-02,  3.2352e-01,\n",
       "          -3.7933e-01,  1.3323e-02,  3.2149e-01,  2.9157e-01, -1.3252e+00,\n",
       "          -4.1968e-03, -3.6521e-02,  2.1551e-01, -2.0766e-01,  3.7599e-01,\n",
       "           2.2564e-01,  5.2119e-01, -4.4355e-02, -1.8952e-01,  2.5574e-01,\n",
       "           1.1558e+00, -1.8362e-01, -2.5785e-01,  3.3655e-01, -5.9746e-03,\n",
       "          -2.3840e-01,  4.8814e-02, -3.9995e-01, -5.1703e-02,  9.0431e-02,\n",
       "          -1.4829e-01,  2.6324e-01, -2.1299e-01, -2.0745e-01,  2.1008e-01,\n",
       "           4.3979e-01, -4.1771e-01,  1.6865e-01, -1.4575e-03,  1.6424e-01,\n",
       "          -2.7560e-02, -2.9673e-01,  4.4356e-01,  3.8644e-01, -1.7177e-02,\n",
       "           3.0430e-01, -1.3036e-01,  5.8990e-01, -2.6672e-02,  1.3839e-01,\n",
       "          -2.1776e-01,  2.5886e-01, -1.0365e-01,  5.0708e-01,  7.1468e-02,\n",
       "          -2.8835e-01,  9.0630e-03,  5.4950e-01,  1.3183e-02,  7.8507e-02,\n",
       "          -2.4742e-02, -2.8229e-01,  6.9248e-02,  1.3588e-01, -8.1964e-02,\n",
       "          -1.2618e-01,  1.7700e-01, -1.8295e-01, -3.7349e-01, -3.0257e-02,\n",
       "           4.9901e-01, -4.6310e-02, -1.0667e-01,  2.5187e-01, -2.4974e-01,\n",
       "          -1.0153e+00,  2.5389e-01, -1.6947e-01,  9.5279e-01,  2.2441e-01,\n",
       "           1.8631e-02, -3.5385e-01, -3.7518e-03, -1.3832e-03, -1.4773e-01,\n",
       "           1.1633e-01, -1.4843e-02,  3.2812e-01,  7.6172e-02, -8.9335e-02,\n",
       "          -2.7072e-02, -3.8204e-01,  1.4398e-01, -2.2048e-01,  2.6724e-01,\n",
       "           1.7931e-01,  1.9831e-02, -3.5856e-02,  3.1362e-01, -5.6898e-02,\n",
       "           2.9256e-01,  1.3741e-01,  1.1482e-01, -2.2795e-01,  6.1906e-02,\n",
       "          -3.8382e-01,  9.9245e-02,  2.6809e-02,  1.2444e-01, -1.0488e-01,\n",
       "          -6.3618e-02, -1.7001e-01, -9.3777e-02, -4.4260e-01,  2.7408e-01,\n",
       "          -2.6038e-01,  5.7491e-02,  4.8378e-02,  2.6667e-02, -2.7939e-01,\n",
       "          -2.1347e-01, -5.7146e-01, -4.2780e-01,  3.9927e-01, -2.3433e-01,\n",
       "          -3.4036e-01, -2.6126e-01, -2.2133e-01,  3.1489e-01,  1.0589e-01,\n",
       "          -2.3049e-01,  3.3393e-01,  3.9580e-02,  6.9268e-02,  1.0786e-01,\n",
       "           4.3680e-02, -1.3661e-03,  4.0690e-02,  3.1856e-01,  4.8938e-02,\n",
       "          -4.7885e-01,  1.4441e-01,  9.0219e-01, -6.4605e-02,  3.9035e-01,\n",
       "          -1.3006e-01,  2.2442e-01, -1.6776e-01, -9.6538e-02, -3.2596e-01,\n",
       "           2.4414e-01,  1.7990e-01,  1.3210e-01, -2.7508e-02, -1.6708e-01,\n",
       "          -7.2453e-02,  3.6112e-01,  1.7543e-03,  2.1864e-01,  3.5058e-02,\n",
       "          -1.1467e-01, -2.9627e-01, -9.2460e-03,  1.7361e-02, -1.5156e-01,\n",
       "           4.3672e-01, -1.0810e-01,  2.1949e-01, -1.6031e-01,  7.9630e-02,\n",
       "          -2.6704e-01,  9.6436e-02, -9.1067e-02, -3.4006e-02, -1.0135e-01,\n",
       "          -1.3300e-01, -1.1336e-01,  2.4301e-01, -3.6827e-02, -7.1317e-03,\n",
       "           3.4798e-01,  2.6985e-03,  3.1138e-02,  1.8100e-01,  2.7475e-01,\n",
       "          -6.9054e-02, -6.8759e-02, -7.0484e-02,  6.8572e-02, -3.5036e-01,\n",
       "           2.1605e-02, -2.0104e-02, -1.4014e-03,  1.6381e-01,  5.8007e-01,\n",
       "          -5.6445e-01, -3.6321e-02, -1.2807e-01, -2.9857e-02, -1.2163e-01,\n",
       "          -1.0122e-01,  1.7037e-01,  1.1601e-01, -8.2954e-02,  2.6946e-03,\n",
       "          -1.1136e-01, -8.7299e-02, -2.0346e-01, -1.9766e-01, -2.6130e-02,\n",
       "          -4.3175e-01,  4.3006e-01, -1.5023e-01,  2.6643e-01,  1.0258e-01,\n",
       "          -5.2727e-01,  5.2501e-01, -2.9109e-01,  1.4633e-02,  5.9105e-02,\n",
       "           1.1609e-01,  2.1430e-02, -1.7106e-01, -1.4557e-01,  5.3713e-02,\n",
       "           2.6238e-01, -4.4832e-02,  2.7818e-01,  3.5873e-01, -1.1511e-01,\n",
       "           3.5144e-01, -4.1552e-01, -6.7309e-02, -2.8997e-01, -2.2453e-01,\n",
       "           3.9258e-02,  1.5291e-01, -2.7494e-02, -1.2718e-01,  1.2670e-01,\n",
       "          -2.5984e-01, -8.0735e-02,  2.9160e-01, -4.9139e-02, -2.7177e-02,\n",
       "          -1.5466e-01,  3.1755e-01, -4.1531e-01,  5.7373e-02,  3.9311e-02,\n",
       "          -3.1150e-02, -8.1021e-02,  1.2103e-01, -3.4716e-01,  3.1155e-01,\n",
       "          -2.5113e-01,  3.3502e-01], device='cuda:0')],\n",
       " [tensor([ 5.1199e-01,  6.7217e-02, -6.2822e-01,  2.0367e-01,  3.5700e-02,\n",
       "           1.2200e-02, -1.7521e-01,  4.4324e-04, -6.8688e-02, -1.1810e-01,\n",
       "          -3.5943e-02, -1.3074e-02,  5.2857e-02, -2.8560e-01,  1.5273e-01,\n",
       "          -2.8706e-01,  8.2016e-02, -1.2833e-01,  4.5680e-01,  3.5889e-01,\n",
       "          -1.1988e-01, -1.2642e-01, -1.9967e-01,  2.5266e-01,  3.5789e-01,\n",
       "          -1.8242e-02,  3.3421e-02, -3.7464e-01, -1.7979e-01,  2.7704e-01,\n",
       "           2.5598e-01,  2.5221e-01,  1.6680e-01,  4.9888e-02, -1.0321e-01,\n",
       "           2.2811e-01,  3.0461e-02,  3.0009e-02, -8.6265e-02,  7.8788e-02,\n",
       "           1.2989e-01, -1.0888e-01, -1.2430e-01, -4.4398e-02,  4.5020e-01,\n",
       "           2.6943e-01, -1.8908e-01,  7.7852e-02, -3.0184e-01, -1.7944e-01,\n",
       "           1.3145e-01,  1.8982e-01, -1.8014e-02,  2.6812e-01,  1.1116e-01,\n",
       "           1.8091e-01, -1.4851e-01,  7.7178e-02,  7.3749e-02,  1.7807e-01,\n",
       "          -2.0870e-01,  2.7113e-03,  3.7070e-01,  2.3924e-01,  2.9215e-01,\n",
       "          -1.9263e-03,  7.5360e-02,  9.1701e-02,  1.1798e-01, -3.8894e-02,\n",
       "           9.3340e-02,  2.8528e-01,  2.3927e-01,  1.1966e-02,  2.1036e-01,\n",
       "           6.3470e-03, -1.1353e-01,  2.0703e-01, -1.0335e-01, -1.5467e-01,\n",
       "           3.3455e-01, -1.4482e-01,  1.2664e-01, -3.2243e-01,  7.7489e-03,\n",
       "           2.2305e-01,  2.3832e-01, -2.3618e-01, -3.2453e-01,  1.3344e-01,\n",
       "           2.4659e-01, -1.7344e-03, -8.2906e-02,  1.6583e-01,  1.8659e-01,\n",
       "           8.1166e-02,  1.0294e-01, -3.1155e-01,  1.7839e+00,  8.6899e-02,\n",
       "           7.2251e-02, -1.7992e-01, -5.0660e-01,  5.6370e-02, -2.4217e-01,\n",
       "           3.4459e-01,  2.0594e-01, -4.3753e-02, -1.0069e-01, -1.7036e-01,\n",
       "          -1.6493e-01,  2.7466e-01,  5.7009e-02,  8.5543e-02, -1.8079e-01,\n",
       "           7.2932e-03,  3.3404e-02, -1.3353e-01,  1.9937e-01,  1.9785e-01,\n",
       "           5.2704e-02, -3.0257e-01, -1.1293e-01,  5.3753e-03, -5.9776e-02,\n",
       "          -2.4693e-01,  2.2928e-01, -3.4245e-01,  2.1027e-01, -3.4180e-01,\n",
       "           2.1754e-01,  2.7348e-02,  1.4005e-03,  2.6408e-01, -2.5132e-01,\n",
       "          -2.2898e-01,  1.4256e-01,  5.8775e-03, -1.6325e-01,  6.9918e-02,\n",
       "          -3.6745e-02, -1.4522e-02,  2.8980e-01, -2.7921e-02,  1.5737e-01,\n",
       "          -1.9639e-01, -1.9266e-01, -2.7696e-01,  1.7241e-01, -4.3368e-01,\n",
       "          -2.2806e-01,  2.2295e-02, -3.3013e-01,  1.2354e-01, -4.2327e-02,\n",
       "          -2.9068e-01,  6.3014e-02,  2.4697e-01,  9.3234e-03, -1.3259e-01,\n",
       "          -3.5415e-02, -2.0506e-03,  2.5672e-01, -1.8524e-01, -1.8568e-03,\n",
       "           1.3712e-01,  7.4625e-02, -2.1933e-02,  2.2335e-01,  1.7142e-01,\n",
       "          -3.5628e-01,  6.8933e-02,  9.1631e-02, -1.9857e-01, -1.9137e-01,\n",
       "           6.3319e-02, -1.7657e-01,  2.6075e-01, -8.5149e-02, -1.0594e-01,\n",
       "           1.1396e-01, -1.8425e-01, -7.1094e-02, -1.1425e-01,  3.4482e-01,\n",
       "           5.8817e-02, -1.0522e-01,  1.0239e-01, -1.4164e-01,  1.1239e-03,\n",
       "          -2.9062e-01,  3.7796e-01, -4.7504e-01, -2.4936e-01, -1.5992e-01,\n",
       "           2.4897e-01, -4.2938e-01, -9.5678e-02, -1.2342e-01, -2.8082e-02,\n",
       "          -2.4457e-01,  1.1679e-03,  1.2541e-01,  1.1309e-01,  1.7459e-01,\n",
       "          -5.4428e-02, -1.8425e-01,  8.9083e-02,  1.7859e-01,  3.5044e-01,\n",
       "          -1.4093e-01, -7.3836e-02, -1.8842e-01,  1.0473e-01,  1.1690e-01,\n",
       "           2.2390e-01,  2.6289e-01, -1.3752e-01, -2.0178e-01, -7.2094e-02,\n",
       "           2.2588e-01, -9.7305e-02,  1.9384e-01,  1.3331e-01,  2.9979e-01,\n",
       "          -2.1996e-01, -1.3837e-01, -8.6046e-02, -3.5572e-02,  8.3026e-02,\n",
       "          -3.7732e-01, -2.6070e-01, -7.1706e-02,  2.5786e-01, -7.4200e-02,\n",
       "          -1.7178e-01,  1.1079e-01,  7.7694e-02, -2.7876e-01, -2.3120e-01,\n",
       "          -2.6540e-02,  2.2528e+00, -5.8911e-02, -4.7647e-02,  1.7766e-02,\n",
       "           5.0872e-02,  1.2243e-01, -2.8358e-01, -5.8081e-02, -5.9568e-02,\n",
       "          -4.9095e-01,  9.5258e-02,  3.9939e-01, -3.3053e-01,  1.1624e-01,\n",
       "           7.4713e-02, -2.2068e-01,  3.9781e-01, -2.2691e-02,  2.0983e-01,\n",
       "          -3.5292e-01,  6.4390e-02,  1.3993e-01,  1.6917e-01, -9.5128e-01,\n",
       "          -8.9082e-02, -8.9976e-02,  1.4344e-01, -1.0179e-01,  1.4457e-01,\n",
       "           1.1434e-01,  3.8826e-01, -5.1193e-03, -7.2353e-02,  3.1514e-01,\n",
       "           6.9131e-01, -1.7789e-01, -1.2838e-01,  2.2159e-01, -4.9201e-02,\n",
       "          -1.7699e-01,  2.2713e-02, -1.9996e-01,  6.2268e-02,  5.7276e-02,\n",
       "           2.8217e-02,  1.2388e-01, -7.9870e-02, -9.3750e-02,  2.2735e-02,\n",
       "           3.2727e-01, -2.5552e-01,  2.5481e-01,  3.0350e-02,  1.0637e-01,\n",
       "           1.1261e-01, -1.7137e-01,  2.5317e-01,  1.7071e-01, -7.8992e-02,\n",
       "           2.0338e-01, -7.4795e-03,  5.6608e-01, -6.3063e-02,  1.1983e-01,\n",
       "          -3.8464e-01,  1.2004e-01, -7.8010e-02,  4.1221e-01, -1.6460e-01,\n",
       "          -3.3751e-01,  4.6604e-02,  3.6122e-01, -1.7714e-02,  6.2197e-02,\n",
       "          -2.3326e-02, -1.8835e-01,  4.0574e-02,  6.3516e-02, -1.2420e-01,\n",
       "          -1.4674e-01,  1.9444e-01, -9.1703e-02, -3.7505e-01,  4.4699e-02,\n",
       "           3.0831e-01, -8.2783e-02, -5.7083e-02,  8.3300e-02, -3.7451e-02,\n",
       "          -5.6810e-01,  3.3395e-01, -1.6767e-01,  6.6744e-01,  3.6663e-01,\n",
       "          -3.7911e-02, -2.4347e-01, -6.3793e-02, -2.1815e-02, -1.7477e-01,\n",
       "           8.1840e-02,  8.6021e-02,  1.7668e-01,  1.9756e-01, -2.5598e-01,\n",
       "          -1.1381e-01, -3.4735e-01,  1.2064e-01, -4.9369e-03,  3.7298e-01,\n",
       "           6.6049e-02,  1.7251e-01, -1.3744e-01,  2.2471e-01,  6.2586e-02,\n",
       "           3.3100e-01,  1.1605e-01, -7.6892e-02, -1.1532e-01,  2.6284e-02,\n",
       "          -1.9663e-01,  1.3052e-01, -3.9807e-02,  2.4890e-01, -1.4221e-01,\n",
       "          -6.9413e-03, -9.4677e-02,  6.7206e-02, -4.4246e-01,  2.0318e-01,\n",
       "          -5.4312e-02,  2.2955e-01, -1.1185e-01,  4.5975e-02, -3.0779e-01,\n",
       "          -2.0885e-01, -5.5737e-01, -2.5313e-01,  1.2569e-01, -2.0181e-01,\n",
       "          -2.5140e-01, -2.1319e-01, -2.3389e-01,  2.5118e-01,  1.0444e-01,\n",
       "          -9.3709e-02,  2.6567e-01, -9.2356e-03,  8.5706e-02,  2.9555e-02,\n",
       "          -1.0901e-03,  1.0854e-01,  1.4911e-01,  1.0911e-01,  4.6317e-03,\n",
       "          -4.5364e-01,  6.4951e-02,  5.1204e-01,  4.5070e-02,  3.5309e-01,\n",
       "          -2.6169e-01,  1.5892e-01, -1.8759e-01,  8.0931e-02, -1.5673e-01,\n",
       "           3.1781e-01,  2.4186e-01,  1.4195e-01, -5.7960e-02, -2.1879e-01,\n",
       "          -3.9357e-02,  2.9624e-01,  6.9045e-02,  4.2252e-02, -1.3590e-01,\n",
       "          -2.0413e-01, -2.9312e-01, -1.0235e-01,  1.8207e-02, -2.2969e-01,\n",
       "           4.6641e-01, -9.0372e-02,  3.6463e-02, -1.0181e-01,  1.4460e-01,\n",
       "          -1.8688e-01,  3.1896e-02, -4.6111e-02, -6.0641e-02,  1.0143e-01,\n",
       "          -9.1146e-02,  9.6946e-02,  2.2511e-01,  6.5458e-02,  3.0277e-02,\n",
       "           2.5575e-01, -1.4143e-03, -6.1967e-02,  1.6394e-01,  2.6592e-01,\n",
       "          -7.1706e-02,  1.2125e-01,  6.1986e-02, -5.4316e-02, -2.9374e-01,\n",
       "          -2.5845e-02,  3.6862e-02,  8.3491e-03,  1.1467e-01,  4.3105e-01,\n",
       "          -2.5261e-01, -8.2594e-02, -4.5061e-02, -3.9239e-02, -6.1770e-02,\n",
       "          -8.9522e-02,  1.6852e-01,  7.5903e-02, -6.2312e-02,  1.7686e-02,\n",
       "          -5.5122e-02, -1.3073e-01, -1.5270e-01, -5.3549e-02,  5.3696e-02,\n",
       "          -1.7208e-01,  4.1693e-01, -1.0629e-01,  2.4776e-01, -8.4640e-02,\n",
       "          -3.8824e-01,  2.6061e-01, -1.5364e-01, -3.6149e-02,  1.2991e-01,\n",
       "           1.6077e-01,  7.8555e-03, -1.7734e-01, -2.5661e-02, -2.8434e-02,\n",
       "           1.4266e-01,  6.3670e-02,  3.5782e-01,  3.5624e-01, -1.4273e-01,\n",
       "           4.0379e-01, -1.6081e-02,  3.6598e-03, -2.1655e-01, -2.2918e-01,\n",
       "           1.0139e-01,  1.5702e-01,  4.5970e-02, -2.2864e-01, -1.5470e-02,\n",
       "          -1.3869e-01,  7.9072e-02, -1.8118e-02, -1.2263e-01, -3.7886e-02,\n",
       "          -2.2483e-01,  3.2867e-01, -3.7717e-01,  3.4737e-02, -5.8808e-02,\n",
       "          -9.2526e-02, -1.8681e-01,  4.8722e-02, -2.5632e-01,  2.3552e-01,\n",
       "          -1.9595e-01,  1.7809e-01], device='cuda:0'),\n",
       "  tensor([ 5.2274e-01,  1.5641e-01, -1.0848e+00,  1.1177e-01,  7.5653e-02,\n",
       "           9.4285e-02, -1.8001e-01,  2.5528e-02, -2.4492e-01, -1.7891e-01,\n",
       "          -4.5921e-02,  2.5578e-02,  1.8064e-01, -4.5970e-01,  6.3076e-02,\n",
       "          -3.7772e-01,  1.5441e-01, -2.4951e-01,  4.7971e-01,  3.5050e-01,\n",
       "          -2.6991e-01, -9.2889e-02, -1.7606e-01,  1.7152e-01,  2.3409e-01,\n",
       "          -7.8755e-02, -4.0918e-02, -3.0925e-01, -1.6300e-01,  3.8787e-01,\n",
       "           2.5585e-01,  3.3782e-01,  1.5435e-01,  7.4610e-02, -2.3172e-02,\n",
       "           1.3508e-01,  2.6527e-02, -4.3903e-02, -6.8146e-02,  2.9305e-01,\n",
       "           2.3502e-02, -1.9274e-01, -7.0365e-02, -3.2197e-01,  5.4049e-01,\n",
       "           3.2196e-01, -2.7936e-01,  7.5554e-02, -1.5153e-01, -2.3615e-01,\n",
       "           2.3531e-01,  1.5285e-01, -2.6181e-01,  1.7080e-01,  2.8013e-01,\n",
       "           1.7029e-01, -1.4272e-02,  1.2108e-02, -1.6482e-01,  8.8154e-02,\n",
       "          -2.0103e-01, -1.1609e-01,  4.7431e-01,  3.1184e-01,  4.3271e-01,\n",
       "           9.0316e-02,  1.8462e-01,  1.6195e-01,  6.8520e-02, -1.0234e-01,\n",
       "          -9.5794e-02,  4.6709e-01,  4.8865e-01, -5.3075e-02,  1.7724e-01,\n",
       "          -1.0268e-01,  2.9243e-02,  3.3458e-01, -1.4299e-01, -1.4205e-01,\n",
       "           5.5196e-01, -1.6875e-01,  1.9743e-01, -2.6752e-01, -1.1521e-02,\n",
       "           1.7574e-01,  6.4551e-02, -5.0148e-01, -3.4393e-01,  2.6587e-01,\n",
       "           4.6449e-01, -5.0084e-04,  8.8365e-03,  2.4609e-01,  2.6972e-01,\n",
       "           8.8671e-05,  1.6340e-01, -3.4826e-01,  2.7051e+00,  2.1946e-01,\n",
       "           1.7937e-01, -2.9697e-01, -4.5259e-01,  2.4084e-01, -2.9354e-01,\n",
       "           4.4238e-01,  3.2129e-01, -7.7020e-02, -3.8553e-02, -1.4583e-01,\n",
       "          -3.7871e-01,  2.1882e-01,  1.4687e-01,  9.9550e-02, -1.0327e-01,\n",
       "           4.0365e-02,  1.8728e-01, -1.0975e-01,  4.5076e-01,  2.5217e-01,\n",
       "           1.8275e-01, -3.9115e-01, -3.5903e-01, -1.4331e-01, -1.1253e-01,\n",
       "          -2.5967e-01,  2.5023e-01, -4.1895e-01,  1.8815e-01, -2.4978e-01,\n",
       "           4.5059e-01,  1.3987e-01, -1.1408e-01,  2.5577e-01, -1.5558e-01,\n",
       "          -2.2749e-01,  2.1994e-01, -2.3140e-01, -4.2276e-01,  1.9363e-01,\n",
       "          -1.8481e-01, -9.6807e-03,  2.1645e-01,  1.5031e-01,  2.4152e-01,\n",
       "          -4.2557e-01, -3.0346e-01, -2.2863e-01,  2.3039e-01, -6.4430e-01,\n",
       "          -2.1962e-01, -7.2523e-02, -3.3888e-01,  4.6646e-02, -3.4023e-02,\n",
       "          -1.6126e-01,  1.8565e-01,  2.4995e-01, -7.5664e-02, -1.1935e-01,\n",
       "          -4.3957e-02,  8.8616e-02,  2.8147e-01, -1.9149e-01,  5.2789e-02,\n",
       "           1.3602e-01, -5.7536e-02, -1.0664e-01,  2.9895e-01,  9.0617e-02,\n",
       "          -2.4501e-01,  6.2611e-02,  9.3418e-02, -3.0951e-01, -1.2341e-01,\n",
       "           3.8067e-02, -1.7876e-01,  4.6486e-01, -1.7777e-01, -7.2212e-02,\n",
       "           3.2726e-01, -3.2004e-01, -2.6162e-02, -1.7617e-01,  4.4617e-01,\n",
       "          -3.3485e-02, -8.0312e-02,  1.2325e-01, -1.3496e-01,  2.0807e-01,\n",
       "          -6.2653e-01,  3.0064e-01, -2.3005e-01, -2.4519e-01, -1.9420e-01,\n",
       "           4.5950e-01, -6.5656e-01, -7.4801e-02, -1.3110e-01, -1.3971e-01,\n",
       "          -3.7797e-01,  5.9950e-02, -1.9938e-02,  1.0263e-01,  2.8390e-01,\n",
       "          -3.7863e-02, -1.5375e-01,  1.1312e-01,  2.1486e-01,  3.8871e-01,\n",
       "          -2.5118e-01,  1.1082e-01, -2.9860e-01, -6.2103e-02,  2.1795e-02,\n",
       "           2.4248e-01,  3.7907e-01, -1.0378e-01, -1.7678e-01, -7.0341e-02,\n",
       "           2.8019e-01, -1.2278e-01,  2.2996e-01,  1.3770e-01,  4.2115e-01,\n",
       "          -3.6226e-01, -6.6510e-01, -1.9985e-02, -1.3854e-01,  1.9808e-01,\n",
       "          -6.1745e-01, -3.9260e-01, -7.5302e-02,  2.5463e-01, -6.3912e-02,\n",
       "          -1.0777e-01,  2.0373e-01,  1.7897e-02, -5.0587e-01, -2.4789e-01,\n",
       "           1.0027e-01,  3.4695e+00, -1.2569e-01, -7.9449e-02,  8.5338e-02,\n",
       "           4.5396e-02,  1.8641e-01, -4.3182e-01, -2.8186e-02, -6.3559e-01,\n",
       "          -6.4887e-01, -6.9457e-02,  4.8287e-01, -5.8451e-01,  2.1489e-01,\n",
       "           1.6621e-01, -6.8065e-02,  4.4866e-01, -3.4670e-02,  3.2352e-01,\n",
       "          -3.7933e-01,  1.3323e-02,  3.2149e-01,  2.9157e-01, -1.3252e+00,\n",
       "          -4.1968e-03, -3.6521e-02,  2.1551e-01, -2.0766e-01,  3.7599e-01,\n",
       "           2.2564e-01,  5.2119e-01, -4.4355e-02, -1.8952e-01,  2.5574e-01,\n",
       "           1.1558e+00, -1.8362e-01, -2.5785e-01,  3.3655e-01, -5.9746e-03,\n",
       "          -2.3840e-01,  4.8814e-02, -3.9995e-01, -5.1703e-02,  9.0431e-02,\n",
       "          -1.4829e-01,  2.6324e-01, -2.1299e-01, -2.0745e-01,  2.1008e-01,\n",
       "           4.3979e-01, -4.1771e-01,  1.6865e-01, -1.4575e-03,  1.6424e-01,\n",
       "          -2.7560e-02, -2.9673e-01,  4.4356e-01,  3.8644e-01, -1.7177e-02,\n",
       "           3.0430e-01, -1.3036e-01,  5.8990e-01, -2.6672e-02,  1.3839e-01,\n",
       "          -2.1776e-01,  2.5886e-01, -1.0365e-01,  5.0708e-01,  7.1468e-02,\n",
       "          -2.8835e-01,  9.0630e-03,  5.4950e-01,  1.3183e-02,  7.8507e-02,\n",
       "          -2.4742e-02, -2.8229e-01,  6.9248e-02,  1.3588e-01, -8.1964e-02,\n",
       "          -1.2618e-01,  1.7700e-01, -1.8295e-01, -3.7349e-01, -3.0257e-02,\n",
       "           4.9901e-01, -4.6310e-02, -1.0667e-01,  2.5187e-01, -2.4974e-01,\n",
       "          -1.0153e+00,  2.5389e-01, -1.6947e-01,  9.5279e-01,  2.2441e-01,\n",
       "           1.8631e-02, -3.5385e-01, -3.7518e-03, -1.3832e-03, -1.4773e-01,\n",
       "           1.1633e-01, -1.4843e-02,  3.2812e-01,  7.6172e-02, -8.9335e-02,\n",
       "          -2.7072e-02, -3.8204e-01,  1.4398e-01, -2.2048e-01,  2.6724e-01,\n",
       "           1.7931e-01,  1.9831e-02, -3.5856e-02,  3.1362e-01, -5.6898e-02,\n",
       "           2.9256e-01,  1.3741e-01,  1.1482e-01, -2.2795e-01,  6.1906e-02,\n",
       "          -3.8382e-01,  9.9245e-02,  2.6809e-02,  1.2444e-01, -1.0488e-01,\n",
       "          -6.3618e-02, -1.7001e-01, -9.3777e-02, -4.4260e-01,  2.7408e-01,\n",
       "          -2.6038e-01,  5.7491e-02,  4.8378e-02,  2.6667e-02, -2.7939e-01,\n",
       "          -2.1347e-01, -5.7146e-01, -4.2780e-01,  3.9927e-01, -2.3433e-01,\n",
       "          -3.4036e-01, -2.6126e-01, -2.2133e-01,  3.1489e-01,  1.0589e-01,\n",
       "          -2.3049e-01,  3.3393e-01,  3.9580e-02,  6.9268e-02,  1.0786e-01,\n",
       "           4.3680e-02, -1.3661e-03,  4.0690e-02,  3.1856e-01,  4.8938e-02,\n",
       "          -4.7885e-01,  1.4441e-01,  9.0219e-01, -6.4605e-02,  3.9035e-01,\n",
       "          -1.3006e-01,  2.2442e-01, -1.6776e-01, -9.6538e-02, -3.2596e-01,\n",
       "           2.4414e-01,  1.7990e-01,  1.3210e-01, -2.7508e-02, -1.6708e-01,\n",
       "          -7.2453e-02,  3.6112e-01,  1.7543e-03,  2.1864e-01,  3.5058e-02,\n",
       "          -1.1467e-01, -2.9627e-01, -9.2460e-03,  1.7361e-02, -1.5156e-01,\n",
       "           4.3672e-01, -1.0810e-01,  2.1949e-01, -1.6031e-01,  7.9630e-02,\n",
       "          -2.6704e-01,  9.6436e-02, -9.1067e-02, -3.4006e-02, -1.0135e-01,\n",
       "          -1.3300e-01, -1.1336e-01,  2.4301e-01, -3.6827e-02, -7.1317e-03,\n",
       "           3.4798e-01,  2.6985e-03,  3.1138e-02,  1.8100e-01,  2.7475e-01,\n",
       "          -6.9054e-02, -6.8759e-02, -7.0484e-02,  6.8572e-02, -3.5036e-01,\n",
       "           2.1605e-02, -2.0104e-02, -1.4014e-03,  1.6381e-01,  5.8007e-01,\n",
       "          -5.6445e-01, -3.6321e-02, -1.2807e-01, -2.9857e-02, -1.2163e-01,\n",
       "          -1.0122e-01,  1.7037e-01,  1.1601e-01, -8.2954e-02,  2.6946e-03,\n",
       "          -1.1136e-01, -8.7299e-02, -2.0346e-01, -1.9766e-01, -2.6130e-02,\n",
       "          -4.3175e-01,  4.3006e-01, -1.5023e-01,  2.6643e-01,  1.0258e-01,\n",
       "          -5.2727e-01,  5.2501e-01, -2.9109e-01,  1.4633e-02,  5.9105e-02,\n",
       "           1.1609e-01,  2.1430e-02, -1.7106e-01, -1.4557e-01,  5.3713e-02,\n",
       "           2.6238e-01, -4.4832e-02,  2.7818e-01,  3.5873e-01, -1.1511e-01,\n",
       "           3.5144e-01, -4.1552e-01, -6.7309e-02, -2.8997e-01, -2.2453e-01,\n",
       "           3.9258e-02,  1.5291e-01, -2.7494e-02, -1.2718e-01,  1.2670e-01,\n",
       "          -2.5984e-01, -8.0735e-02,  2.9160e-01, -4.9139e-02, -2.7177e-02,\n",
       "          -1.5466e-01,  3.1755e-01, -4.1531e-01,  5.7373e-02,  3.9311e-02,\n",
       "          -3.1150e-02, -8.1021e-02,  1.2103e-01, -3.4716e-01,  3.1155e-01,\n",
       "          -2.5113e-01,  3.3502e-01], device='cuda:0')],\n",
       " [tensor([ 5.1199e-01,  6.7217e-02, -6.2822e-01,  2.0367e-01,  3.5700e-02,\n",
       "           1.2200e-02, -1.7521e-01,  4.4324e-04, -6.8688e-02, -1.1810e-01,\n",
       "          -3.5943e-02, -1.3074e-02,  5.2857e-02, -2.8560e-01,  1.5273e-01,\n",
       "          -2.8706e-01,  8.2016e-02, -1.2833e-01,  4.5680e-01,  3.5889e-01,\n",
       "          -1.1988e-01, -1.2642e-01, -1.9967e-01,  2.5266e-01,  3.5789e-01,\n",
       "          -1.8242e-02,  3.3421e-02, -3.7464e-01, -1.7979e-01,  2.7704e-01,\n",
       "           2.5598e-01,  2.5221e-01,  1.6680e-01,  4.9888e-02, -1.0321e-01,\n",
       "           2.2811e-01,  3.0461e-02,  3.0009e-02, -8.6265e-02,  7.8788e-02,\n",
       "           1.2989e-01, -1.0888e-01, -1.2430e-01, -4.4398e-02,  4.5020e-01,\n",
       "           2.6943e-01, -1.8908e-01,  7.7852e-02, -3.0184e-01, -1.7944e-01,\n",
       "           1.3145e-01,  1.8982e-01, -1.8014e-02,  2.6812e-01,  1.1116e-01,\n",
       "           1.8091e-01, -1.4851e-01,  7.7178e-02,  7.3749e-02,  1.7807e-01,\n",
       "          -2.0870e-01,  2.7113e-03,  3.7070e-01,  2.3924e-01,  2.9215e-01,\n",
       "          -1.9263e-03,  7.5360e-02,  9.1701e-02,  1.1798e-01, -3.8894e-02,\n",
       "           9.3340e-02,  2.8528e-01,  2.3927e-01,  1.1966e-02,  2.1036e-01,\n",
       "           6.3470e-03, -1.1353e-01,  2.0703e-01, -1.0335e-01, -1.5467e-01,\n",
       "           3.3455e-01, -1.4482e-01,  1.2664e-01, -3.2243e-01,  7.7489e-03,\n",
       "           2.2305e-01,  2.3832e-01, -2.3618e-01, -3.2453e-01,  1.3344e-01,\n",
       "           2.4659e-01, -1.7344e-03, -8.2906e-02,  1.6583e-01,  1.8659e-01,\n",
       "           8.1166e-02,  1.0294e-01, -3.1155e-01,  1.7839e+00,  8.6899e-02,\n",
       "           7.2251e-02, -1.7992e-01, -5.0660e-01,  5.6370e-02, -2.4217e-01,\n",
       "           3.4459e-01,  2.0594e-01, -4.3753e-02, -1.0069e-01, -1.7036e-01,\n",
       "          -1.6493e-01,  2.7466e-01,  5.7009e-02,  8.5543e-02, -1.8079e-01,\n",
       "           7.2932e-03,  3.3404e-02, -1.3353e-01,  1.9937e-01,  1.9785e-01,\n",
       "           5.2704e-02, -3.0257e-01, -1.1293e-01,  5.3753e-03, -5.9776e-02,\n",
       "          -2.4693e-01,  2.2928e-01, -3.4245e-01,  2.1027e-01, -3.4180e-01,\n",
       "           2.1754e-01,  2.7348e-02,  1.4005e-03,  2.6408e-01, -2.5132e-01,\n",
       "          -2.2898e-01,  1.4256e-01,  5.8775e-03, -1.6325e-01,  6.9918e-02,\n",
       "          -3.6745e-02, -1.4522e-02,  2.8980e-01, -2.7921e-02,  1.5737e-01,\n",
       "          -1.9639e-01, -1.9266e-01, -2.7696e-01,  1.7241e-01, -4.3368e-01,\n",
       "          -2.2806e-01,  2.2295e-02, -3.3013e-01,  1.2354e-01, -4.2327e-02,\n",
       "          -2.9068e-01,  6.3014e-02,  2.4697e-01,  9.3234e-03, -1.3259e-01,\n",
       "          -3.5415e-02, -2.0506e-03,  2.5672e-01, -1.8524e-01, -1.8568e-03,\n",
       "           1.3712e-01,  7.4625e-02, -2.1933e-02,  2.2335e-01,  1.7142e-01,\n",
       "          -3.5628e-01,  6.8933e-02,  9.1631e-02, -1.9857e-01, -1.9137e-01,\n",
       "           6.3319e-02, -1.7657e-01,  2.6075e-01, -8.5149e-02, -1.0594e-01,\n",
       "           1.1396e-01, -1.8425e-01, -7.1094e-02, -1.1425e-01,  3.4482e-01,\n",
       "           5.8817e-02, -1.0522e-01,  1.0239e-01, -1.4164e-01,  1.1239e-03,\n",
       "          -2.9062e-01,  3.7796e-01, -4.7504e-01, -2.4936e-01, -1.5992e-01,\n",
       "           2.4897e-01, -4.2938e-01, -9.5678e-02, -1.2342e-01, -2.8082e-02,\n",
       "          -2.4457e-01,  1.1679e-03,  1.2541e-01,  1.1309e-01,  1.7459e-01,\n",
       "          -5.4428e-02, -1.8425e-01,  8.9083e-02,  1.7859e-01,  3.5044e-01,\n",
       "          -1.4093e-01, -7.3836e-02, -1.8842e-01,  1.0473e-01,  1.1690e-01,\n",
       "           2.2390e-01,  2.6289e-01, -1.3752e-01, -2.0178e-01, -7.2094e-02,\n",
       "           2.2588e-01, -9.7305e-02,  1.9384e-01,  1.3331e-01,  2.9979e-01,\n",
       "          -2.1996e-01, -1.3837e-01, -8.6046e-02, -3.5572e-02,  8.3026e-02,\n",
       "          -3.7732e-01, -2.6070e-01, -7.1706e-02,  2.5786e-01, -7.4200e-02,\n",
       "          -1.7178e-01,  1.1079e-01,  7.7694e-02, -2.7876e-01, -2.3120e-01,\n",
       "          -2.6540e-02,  2.2528e+00, -5.8911e-02, -4.7647e-02,  1.7766e-02,\n",
       "           5.0872e-02,  1.2243e-01, -2.8358e-01, -5.8081e-02, -5.9568e-02,\n",
       "          -4.9095e-01,  9.5258e-02,  3.9939e-01, -3.3053e-01,  1.1624e-01,\n",
       "           7.4713e-02, -2.2068e-01,  3.9781e-01, -2.2691e-02,  2.0983e-01,\n",
       "          -3.5292e-01,  6.4390e-02,  1.3993e-01,  1.6917e-01, -9.5128e-01,\n",
       "          -8.9082e-02, -8.9976e-02,  1.4344e-01, -1.0179e-01,  1.4457e-01,\n",
       "           1.1434e-01,  3.8826e-01, -5.1193e-03, -7.2353e-02,  3.1514e-01,\n",
       "           6.9131e-01, -1.7789e-01, -1.2838e-01,  2.2159e-01, -4.9201e-02,\n",
       "          -1.7699e-01,  2.2713e-02, -1.9996e-01,  6.2268e-02,  5.7276e-02,\n",
       "           2.8217e-02,  1.2388e-01, -7.9870e-02, -9.3750e-02,  2.2735e-02,\n",
       "           3.2727e-01, -2.5552e-01,  2.5481e-01,  3.0350e-02,  1.0637e-01,\n",
       "           1.1261e-01, -1.7137e-01,  2.5317e-01,  1.7071e-01, -7.8992e-02,\n",
       "           2.0338e-01, -7.4795e-03,  5.6608e-01, -6.3063e-02,  1.1983e-01,\n",
       "          -3.8464e-01,  1.2004e-01, -7.8010e-02,  4.1221e-01, -1.6460e-01,\n",
       "          -3.3751e-01,  4.6604e-02,  3.6122e-01, -1.7714e-02,  6.2197e-02,\n",
       "          -2.3326e-02, -1.8835e-01,  4.0574e-02,  6.3516e-02, -1.2420e-01,\n",
       "          -1.4674e-01,  1.9444e-01, -9.1703e-02, -3.7505e-01,  4.4699e-02,\n",
       "           3.0831e-01, -8.2783e-02, -5.7083e-02,  8.3300e-02, -3.7451e-02,\n",
       "          -5.6810e-01,  3.3395e-01, -1.6767e-01,  6.6744e-01,  3.6663e-01,\n",
       "          -3.7911e-02, -2.4347e-01, -6.3793e-02, -2.1815e-02, -1.7477e-01,\n",
       "           8.1840e-02,  8.6021e-02,  1.7668e-01,  1.9756e-01, -2.5598e-01,\n",
       "          -1.1381e-01, -3.4735e-01,  1.2064e-01, -4.9369e-03,  3.7298e-01,\n",
       "           6.6049e-02,  1.7251e-01, -1.3744e-01,  2.2471e-01,  6.2586e-02,\n",
       "           3.3100e-01,  1.1605e-01, -7.6892e-02, -1.1532e-01,  2.6284e-02,\n",
       "          -1.9663e-01,  1.3052e-01, -3.9807e-02,  2.4890e-01, -1.4221e-01,\n",
       "          -6.9413e-03, -9.4677e-02,  6.7206e-02, -4.4246e-01,  2.0318e-01,\n",
       "          -5.4312e-02,  2.2955e-01, -1.1185e-01,  4.5975e-02, -3.0779e-01,\n",
       "          -2.0885e-01, -5.5737e-01, -2.5313e-01,  1.2569e-01, -2.0181e-01,\n",
       "          -2.5140e-01, -2.1319e-01, -2.3389e-01,  2.5118e-01,  1.0444e-01,\n",
       "          -9.3709e-02,  2.6567e-01, -9.2356e-03,  8.5706e-02,  2.9555e-02,\n",
       "          -1.0901e-03,  1.0854e-01,  1.4911e-01,  1.0911e-01,  4.6317e-03,\n",
       "          -4.5364e-01,  6.4951e-02,  5.1204e-01,  4.5070e-02,  3.5309e-01,\n",
       "          -2.6169e-01,  1.5892e-01, -1.8759e-01,  8.0931e-02, -1.5673e-01,\n",
       "           3.1781e-01,  2.4186e-01,  1.4195e-01, -5.7960e-02, -2.1879e-01,\n",
       "          -3.9357e-02,  2.9624e-01,  6.9045e-02,  4.2252e-02, -1.3590e-01,\n",
       "          -2.0413e-01, -2.9312e-01, -1.0235e-01,  1.8207e-02, -2.2969e-01,\n",
       "           4.6641e-01, -9.0372e-02,  3.6463e-02, -1.0181e-01,  1.4460e-01,\n",
       "          -1.8688e-01,  3.1896e-02, -4.6111e-02, -6.0641e-02,  1.0143e-01,\n",
       "          -9.1146e-02,  9.6946e-02,  2.2511e-01,  6.5458e-02,  3.0277e-02,\n",
       "           2.5575e-01, -1.4143e-03, -6.1967e-02,  1.6394e-01,  2.6592e-01,\n",
       "          -7.1706e-02,  1.2125e-01,  6.1986e-02, -5.4316e-02, -2.9374e-01,\n",
       "          -2.5845e-02,  3.6862e-02,  8.3491e-03,  1.1467e-01,  4.3105e-01,\n",
       "          -2.5261e-01, -8.2594e-02, -4.5061e-02, -3.9239e-02, -6.1770e-02,\n",
       "          -8.9522e-02,  1.6852e-01,  7.5903e-02, -6.2312e-02,  1.7686e-02,\n",
       "          -5.5122e-02, -1.3073e-01, -1.5270e-01, -5.3549e-02,  5.3696e-02,\n",
       "          -1.7208e-01,  4.1693e-01, -1.0629e-01,  2.4776e-01, -8.4640e-02,\n",
       "          -3.8824e-01,  2.6061e-01, -1.5364e-01, -3.6149e-02,  1.2991e-01,\n",
       "           1.6077e-01,  7.8555e-03, -1.7734e-01, -2.5661e-02, -2.8434e-02,\n",
       "           1.4266e-01,  6.3670e-02,  3.5782e-01,  3.5624e-01, -1.4273e-01,\n",
       "           4.0379e-01, -1.6081e-02,  3.6598e-03, -2.1655e-01, -2.2918e-01,\n",
       "           1.0139e-01,  1.5702e-01,  4.5970e-02, -2.2864e-01, -1.5470e-02,\n",
       "          -1.3869e-01,  7.9072e-02, -1.8118e-02, -1.2263e-01, -3.7886e-02,\n",
       "          -2.2483e-01,  3.2867e-01, -3.7717e-01,  3.4737e-02, -5.8808e-02,\n",
       "          -9.2526e-02, -1.8681e-01,  4.8722e-02, -2.5632e-01,  2.3552e-01,\n",
       "          -1.9595e-01,  1.7809e-01], device='cuda:0'),\n",
       "  tensor([ 5.2274e-01,  1.5641e-01, -1.0848e+00,  1.1177e-01,  7.5653e-02,\n",
       "           9.4285e-02, -1.8001e-01,  2.5528e-02, -2.4492e-01, -1.7891e-01,\n",
       "          -4.5921e-02,  2.5578e-02,  1.8064e-01, -4.5970e-01,  6.3076e-02,\n",
       "          -3.7772e-01,  1.5441e-01, -2.4951e-01,  4.7971e-01,  3.5050e-01,\n",
       "          -2.6991e-01, -9.2889e-02, -1.7606e-01,  1.7152e-01,  2.3409e-01,\n",
       "          -7.8755e-02, -4.0918e-02, -3.0925e-01, -1.6300e-01,  3.8787e-01,\n",
       "           2.5585e-01,  3.3782e-01,  1.5435e-01,  7.4610e-02, -2.3172e-02,\n",
       "           1.3508e-01,  2.6527e-02, -4.3903e-02, -6.8146e-02,  2.9305e-01,\n",
       "           2.3502e-02, -1.9274e-01, -7.0365e-02, -3.2197e-01,  5.4049e-01,\n",
       "           3.2196e-01, -2.7936e-01,  7.5554e-02, -1.5153e-01, -2.3615e-01,\n",
       "           2.3531e-01,  1.5285e-01, -2.6181e-01,  1.7080e-01,  2.8013e-01,\n",
       "           1.7029e-01, -1.4272e-02,  1.2108e-02, -1.6482e-01,  8.8154e-02,\n",
       "          -2.0103e-01, -1.1609e-01,  4.7431e-01,  3.1184e-01,  4.3271e-01,\n",
       "           9.0316e-02,  1.8462e-01,  1.6195e-01,  6.8520e-02, -1.0234e-01,\n",
       "          -9.5794e-02,  4.6709e-01,  4.8865e-01, -5.3075e-02,  1.7724e-01,\n",
       "          -1.0268e-01,  2.9243e-02,  3.3458e-01, -1.4299e-01, -1.4205e-01,\n",
       "           5.5196e-01, -1.6875e-01,  1.9743e-01, -2.6752e-01, -1.1521e-02,\n",
       "           1.7574e-01,  6.4551e-02, -5.0148e-01, -3.4393e-01,  2.6587e-01,\n",
       "           4.6449e-01, -5.0084e-04,  8.8365e-03,  2.4609e-01,  2.6972e-01,\n",
       "           8.8671e-05,  1.6340e-01, -3.4826e-01,  2.7051e+00,  2.1946e-01,\n",
       "           1.7937e-01, -2.9697e-01, -4.5259e-01,  2.4084e-01, -2.9354e-01,\n",
       "           4.4238e-01,  3.2129e-01, -7.7020e-02, -3.8553e-02, -1.4583e-01,\n",
       "          -3.7871e-01,  2.1882e-01,  1.4687e-01,  9.9550e-02, -1.0327e-01,\n",
       "           4.0365e-02,  1.8728e-01, -1.0975e-01,  4.5076e-01,  2.5217e-01,\n",
       "           1.8275e-01, -3.9115e-01, -3.5903e-01, -1.4331e-01, -1.1253e-01,\n",
       "          -2.5967e-01,  2.5023e-01, -4.1895e-01,  1.8815e-01, -2.4978e-01,\n",
       "           4.5059e-01,  1.3987e-01, -1.1408e-01,  2.5577e-01, -1.5558e-01,\n",
       "          -2.2749e-01,  2.1994e-01, -2.3140e-01, -4.2276e-01,  1.9363e-01,\n",
       "          -1.8481e-01, -9.6807e-03,  2.1645e-01,  1.5031e-01,  2.4152e-01,\n",
       "          -4.2557e-01, -3.0346e-01, -2.2863e-01,  2.3039e-01, -6.4430e-01,\n",
       "          -2.1962e-01, -7.2523e-02, -3.3888e-01,  4.6646e-02, -3.4023e-02,\n",
       "          -1.6126e-01,  1.8565e-01,  2.4995e-01, -7.5664e-02, -1.1935e-01,\n",
       "          -4.3957e-02,  8.8616e-02,  2.8147e-01, -1.9149e-01,  5.2789e-02,\n",
       "           1.3602e-01, -5.7536e-02, -1.0664e-01,  2.9895e-01,  9.0617e-02,\n",
       "          -2.4501e-01,  6.2611e-02,  9.3418e-02, -3.0951e-01, -1.2341e-01,\n",
       "           3.8067e-02, -1.7876e-01,  4.6486e-01, -1.7777e-01, -7.2212e-02,\n",
       "           3.2726e-01, -3.2004e-01, -2.6162e-02, -1.7617e-01,  4.4617e-01,\n",
       "          -3.3485e-02, -8.0312e-02,  1.2325e-01, -1.3496e-01,  2.0807e-01,\n",
       "          -6.2653e-01,  3.0064e-01, -2.3005e-01, -2.4519e-01, -1.9420e-01,\n",
       "           4.5950e-01, -6.5656e-01, -7.4801e-02, -1.3110e-01, -1.3971e-01,\n",
       "          -3.7797e-01,  5.9950e-02, -1.9938e-02,  1.0263e-01,  2.8390e-01,\n",
       "          -3.7863e-02, -1.5375e-01,  1.1312e-01,  2.1486e-01,  3.8871e-01,\n",
       "          -2.5118e-01,  1.1082e-01, -2.9860e-01, -6.2103e-02,  2.1795e-02,\n",
       "           2.4248e-01,  3.7907e-01, -1.0378e-01, -1.7678e-01, -7.0341e-02,\n",
       "           2.8019e-01, -1.2278e-01,  2.2996e-01,  1.3770e-01,  4.2115e-01,\n",
       "          -3.6226e-01, -6.6510e-01, -1.9985e-02, -1.3854e-01,  1.9808e-01,\n",
       "          -6.1745e-01, -3.9260e-01, -7.5302e-02,  2.5463e-01, -6.3912e-02,\n",
       "          -1.0777e-01,  2.0373e-01,  1.7897e-02, -5.0587e-01, -2.4789e-01,\n",
       "           1.0027e-01,  3.4695e+00, -1.2569e-01, -7.9449e-02,  8.5338e-02,\n",
       "           4.5396e-02,  1.8641e-01, -4.3182e-01, -2.8186e-02, -6.3559e-01,\n",
       "          -6.4887e-01, -6.9457e-02,  4.8287e-01, -5.8451e-01,  2.1489e-01,\n",
       "           1.6621e-01, -6.8065e-02,  4.4866e-01, -3.4670e-02,  3.2352e-01,\n",
       "          -3.7933e-01,  1.3323e-02,  3.2149e-01,  2.9157e-01, -1.3252e+00,\n",
       "          -4.1968e-03, -3.6521e-02,  2.1551e-01, -2.0766e-01,  3.7599e-01,\n",
       "           2.2564e-01,  5.2119e-01, -4.4355e-02, -1.8952e-01,  2.5574e-01,\n",
       "           1.1558e+00, -1.8362e-01, -2.5785e-01,  3.3655e-01, -5.9746e-03,\n",
       "          -2.3840e-01,  4.8814e-02, -3.9995e-01, -5.1703e-02,  9.0431e-02,\n",
       "          -1.4829e-01,  2.6324e-01, -2.1299e-01, -2.0745e-01,  2.1008e-01,\n",
       "           4.3979e-01, -4.1771e-01,  1.6865e-01, -1.4575e-03,  1.6424e-01,\n",
       "          -2.7560e-02, -2.9673e-01,  4.4356e-01,  3.8644e-01, -1.7177e-02,\n",
       "           3.0430e-01, -1.3036e-01,  5.8990e-01, -2.6672e-02,  1.3839e-01,\n",
       "          -2.1776e-01,  2.5886e-01, -1.0365e-01,  5.0708e-01,  7.1468e-02,\n",
       "          -2.8835e-01,  9.0630e-03,  5.4950e-01,  1.3183e-02,  7.8507e-02,\n",
       "          -2.4742e-02, -2.8229e-01,  6.9248e-02,  1.3588e-01, -8.1964e-02,\n",
       "          -1.2618e-01,  1.7700e-01, -1.8295e-01, -3.7349e-01, -3.0257e-02,\n",
       "           4.9901e-01, -4.6310e-02, -1.0667e-01,  2.5187e-01, -2.4974e-01,\n",
       "          -1.0153e+00,  2.5389e-01, -1.6947e-01,  9.5279e-01,  2.2441e-01,\n",
       "           1.8631e-02, -3.5385e-01, -3.7518e-03, -1.3832e-03, -1.4773e-01,\n",
       "           1.1633e-01, -1.4843e-02,  3.2812e-01,  7.6172e-02, -8.9335e-02,\n",
       "          -2.7072e-02, -3.8204e-01,  1.4398e-01, -2.2048e-01,  2.6724e-01,\n",
       "           1.7931e-01,  1.9831e-02, -3.5856e-02,  3.1362e-01, -5.6898e-02,\n",
       "           2.9256e-01,  1.3741e-01,  1.1482e-01, -2.2795e-01,  6.1906e-02,\n",
       "          -3.8382e-01,  9.9245e-02,  2.6809e-02,  1.2444e-01, -1.0488e-01,\n",
       "          -6.3618e-02, -1.7001e-01, -9.3777e-02, -4.4260e-01,  2.7408e-01,\n",
       "          -2.6038e-01,  5.7491e-02,  4.8378e-02,  2.6667e-02, -2.7939e-01,\n",
       "          -2.1347e-01, -5.7146e-01, -4.2780e-01,  3.9927e-01, -2.3433e-01,\n",
       "          -3.4036e-01, -2.6126e-01, -2.2133e-01,  3.1489e-01,  1.0589e-01,\n",
       "          -2.3049e-01,  3.3393e-01,  3.9580e-02,  6.9268e-02,  1.0786e-01,\n",
       "           4.3680e-02, -1.3661e-03,  4.0690e-02,  3.1856e-01,  4.8938e-02,\n",
       "          -4.7885e-01,  1.4441e-01,  9.0219e-01, -6.4605e-02,  3.9035e-01,\n",
       "          -1.3006e-01,  2.2442e-01, -1.6776e-01, -9.6538e-02, -3.2596e-01,\n",
       "           2.4414e-01,  1.7990e-01,  1.3210e-01, -2.7508e-02, -1.6708e-01,\n",
       "          -7.2453e-02,  3.6112e-01,  1.7543e-03,  2.1864e-01,  3.5058e-02,\n",
       "          -1.1467e-01, -2.9627e-01, -9.2460e-03,  1.7361e-02, -1.5156e-01,\n",
       "           4.3672e-01, -1.0810e-01,  2.1949e-01, -1.6031e-01,  7.9630e-02,\n",
       "          -2.6704e-01,  9.6436e-02, -9.1067e-02, -3.4006e-02, -1.0135e-01,\n",
       "          -1.3300e-01, -1.1336e-01,  2.4301e-01, -3.6827e-02, -7.1317e-03,\n",
       "           3.4798e-01,  2.6985e-03,  3.1138e-02,  1.8100e-01,  2.7475e-01,\n",
       "          -6.9054e-02, -6.8759e-02, -7.0484e-02,  6.8572e-02, -3.5036e-01,\n",
       "           2.1605e-02, -2.0104e-02, -1.4014e-03,  1.6381e-01,  5.8007e-01,\n",
       "          -5.6445e-01, -3.6321e-02, -1.2807e-01, -2.9857e-02, -1.2163e-01,\n",
       "          -1.0122e-01,  1.7037e-01,  1.1601e-01, -8.2954e-02,  2.6946e-03,\n",
       "          -1.1136e-01, -8.7299e-02, -2.0346e-01, -1.9766e-01, -2.6130e-02,\n",
       "          -4.3175e-01,  4.3006e-01, -1.5023e-01,  2.6643e-01,  1.0258e-01,\n",
       "          -5.2727e-01,  5.2501e-01, -2.9109e-01,  1.4633e-02,  5.9105e-02,\n",
       "           1.1609e-01,  2.1430e-02, -1.7106e-01, -1.4557e-01,  5.3713e-02,\n",
       "           2.6238e-01, -4.4832e-02,  2.7818e-01,  3.5873e-01, -1.1511e-01,\n",
       "           3.5144e-01, -4.1552e-01, -6.7309e-02, -2.8997e-01, -2.2453e-01,\n",
       "           3.9258e-02,  1.5291e-01, -2.7494e-02, -1.2718e-01,  1.2670e-01,\n",
       "          -2.5984e-01, -8.0735e-02,  2.9160e-01, -4.9139e-02, -2.7177e-02,\n",
       "          -1.5466e-01,  3.1755e-01, -4.1531e-01,  5.7373e-02,  3.9311e-02,\n",
       "          -3.1150e-02, -8.1021e-02,  1.2103e-01, -3.4716e-01,  3.1155e-01,\n",
       "          -2.5113e-01,  3.3502e-01], device='cuda:0')],\n",
       " [tensor([ 5.1199e-01,  6.7217e-02, -6.2822e-01,  2.0367e-01,  3.5700e-02,\n",
       "           1.2200e-02, -1.7521e-01,  4.4324e-04, -6.8688e-02, -1.1810e-01,\n",
       "          -3.5943e-02, -1.3074e-02,  5.2857e-02, -2.8560e-01,  1.5273e-01,\n",
       "          -2.8706e-01,  8.2016e-02, -1.2833e-01,  4.5680e-01,  3.5889e-01,\n",
       "          -1.1988e-01, -1.2642e-01, -1.9967e-01,  2.5266e-01,  3.5789e-01,\n",
       "          -1.8242e-02,  3.3421e-02, -3.7464e-01, -1.7979e-01,  2.7704e-01,\n",
       "           2.5598e-01,  2.5221e-01,  1.6680e-01,  4.9888e-02, -1.0321e-01,\n",
       "           2.2811e-01,  3.0461e-02,  3.0009e-02, -8.6265e-02,  7.8788e-02,\n",
       "           1.2989e-01, -1.0888e-01, -1.2430e-01, -4.4398e-02,  4.5020e-01,\n",
       "           2.6943e-01, -1.8908e-01,  7.7852e-02, -3.0184e-01, -1.7944e-01,\n",
       "           1.3145e-01,  1.8982e-01, -1.8014e-02,  2.6812e-01,  1.1116e-01,\n",
       "           1.8091e-01, -1.4851e-01,  7.7178e-02,  7.3749e-02,  1.7807e-01,\n",
       "          -2.0870e-01,  2.7113e-03,  3.7070e-01,  2.3924e-01,  2.9215e-01,\n",
       "          -1.9263e-03,  7.5360e-02,  9.1701e-02,  1.1798e-01, -3.8894e-02,\n",
       "           9.3340e-02,  2.8528e-01,  2.3927e-01,  1.1966e-02,  2.1036e-01,\n",
       "           6.3470e-03, -1.1353e-01,  2.0703e-01, -1.0335e-01, -1.5467e-01,\n",
       "           3.3455e-01, -1.4482e-01,  1.2664e-01, -3.2243e-01,  7.7489e-03,\n",
       "           2.2305e-01,  2.3832e-01, -2.3618e-01, -3.2453e-01,  1.3344e-01,\n",
       "           2.4659e-01, -1.7344e-03, -8.2906e-02,  1.6583e-01,  1.8659e-01,\n",
       "           8.1166e-02,  1.0294e-01, -3.1155e-01,  1.7839e+00,  8.6899e-02,\n",
       "           7.2251e-02, -1.7992e-01, -5.0660e-01,  5.6370e-02, -2.4217e-01,\n",
       "           3.4459e-01,  2.0594e-01, -4.3753e-02, -1.0069e-01, -1.7036e-01,\n",
       "          -1.6493e-01,  2.7466e-01,  5.7009e-02,  8.5543e-02, -1.8079e-01,\n",
       "           7.2932e-03,  3.3404e-02, -1.3353e-01,  1.9937e-01,  1.9785e-01,\n",
       "           5.2704e-02, -3.0257e-01, -1.1293e-01,  5.3753e-03, -5.9776e-02,\n",
       "          -2.4693e-01,  2.2928e-01, -3.4245e-01,  2.1027e-01, -3.4180e-01,\n",
       "           2.1754e-01,  2.7348e-02,  1.4005e-03,  2.6408e-01, -2.5132e-01,\n",
       "          -2.2898e-01,  1.4256e-01,  5.8775e-03, -1.6325e-01,  6.9918e-02,\n",
       "          -3.6745e-02, -1.4522e-02,  2.8980e-01, -2.7921e-02,  1.5737e-01,\n",
       "          -1.9639e-01, -1.9266e-01, -2.7696e-01,  1.7241e-01, -4.3368e-01,\n",
       "          -2.2806e-01,  2.2295e-02, -3.3013e-01,  1.2354e-01, -4.2327e-02,\n",
       "          -2.9068e-01,  6.3014e-02,  2.4697e-01,  9.3234e-03, -1.3259e-01,\n",
       "          -3.5415e-02, -2.0506e-03,  2.5672e-01, -1.8524e-01, -1.8568e-03,\n",
       "           1.3712e-01,  7.4625e-02, -2.1933e-02,  2.2335e-01,  1.7142e-01,\n",
       "          -3.5628e-01,  6.8933e-02,  9.1631e-02, -1.9857e-01, -1.9137e-01,\n",
       "           6.3319e-02, -1.7657e-01,  2.6075e-01, -8.5149e-02, -1.0594e-01,\n",
       "           1.1396e-01, -1.8425e-01, -7.1094e-02, -1.1425e-01,  3.4482e-01,\n",
       "           5.8817e-02, -1.0522e-01,  1.0239e-01, -1.4164e-01,  1.1239e-03,\n",
       "          -2.9062e-01,  3.7796e-01, -4.7504e-01, -2.4936e-01, -1.5992e-01,\n",
       "           2.4897e-01, -4.2938e-01, -9.5678e-02, -1.2342e-01, -2.8082e-02,\n",
       "          -2.4457e-01,  1.1679e-03,  1.2541e-01,  1.1309e-01,  1.7459e-01,\n",
       "          -5.4428e-02, -1.8425e-01,  8.9083e-02,  1.7859e-01,  3.5044e-01,\n",
       "          -1.4093e-01, -7.3836e-02, -1.8842e-01,  1.0473e-01,  1.1690e-01,\n",
       "           2.2390e-01,  2.6289e-01, -1.3752e-01, -2.0178e-01, -7.2094e-02,\n",
       "           2.2588e-01, -9.7305e-02,  1.9384e-01,  1.3331e-01,  2.9979e-01,\n",
       "          -2.1996e-01, -1.3837e-01, -8.6046e-02, -3.5572e-02,  8.3026e-02,\n",
       "          -3.7732e-01, -2.6070e-01, -7.1706e-02,  2.5786e-01, -7.4200e-02,\n",
       "          -1.7178e-01,  1.1079e-01,  7.7694e-02, -2.7876e-01, -2.3120e-01,\n",
       "          -2.6540e-02,  2.2528e+00, -5.8911e-02, -4.7647e-02,  1.7766e-02,\n",
       "           5.0872e-02,  1.2243e-01, -2.8358e-01, -5.8081e-02, -5.9568e-02,\n",
       "          -4.9095e-01,  9.5258e-02,  3.9939e-01, -3.3053e-01,  1.1624e-01,\n",
       "           7.4713e-02, -2.2068e-01,  3.9781e-01, -2.2691e-02,  2.0983e-01,\n",
       "          -3.5292e-01,  6.4390e-02,  1.3993e-01,  1.6917e-01, -9.5128e-01,\n",
       "          -8.9082e-02, -8.9976e-02,  1.4344e-01, -1.0179e-01,  1.4457e-01,\n",
       "           1.1434e-01,  3.8826e-01, -5.1193e-03, -7.2353e-02,  3.1514e-01,\n",
       "           6.9131e-01, -1.7789e-01, -1.2838e-01,  2.2159e-01, -4.9201e-02,\n",
       "          -1.7699e-01,  2.2713e-02, -1.9996e-01,  6.2268e-02,  5.7276e-02,\n",
       "           2.8217e-02,  1.2388e-01, -7.9870e-02, -9.3750e-02,  2.2735e-02,\n",
       "           3.2727e-01, -2.5552e-01,  2.5481e-01,  3.0350e-02,  1.0637e-01,\n",
       "           1.1261e-01, -1.7137e-01,  2.5317e-01,  1.7071e-01, -7.8992e-02,\n",
       "           2.0338e-01, -7.4795e-03,  5.6608e-01, -6.3063e-02,  1.1983e-01,\n",
       "          -3.8464e-01,  1.2004e-01, -7.8010e-02,  4.1221e-01, -1.6460e-01,\n",
       "          -3.3751e-01,  4.6604e-02,  3.6122e-01, -1.7714e-02,  6.2197e-02,\n",
       "          -2.3326e-02, -1.8835e-01,  4.0574e-02,  6.3516e-02, -1.2420e-01,\n",
       "          -1.4674e-01,  1.9444e-01, -9.1703e-02, -3.7505e-01,  4.4699e-02,\n",
       "           3.0831e-01, -8.2783e-02, -5.7083e-02,  8.3300e-02, -3.7451e-02,\n",
       "          -5.6810e-01,  3.3395e-01, -1.6767e-01,  6.6744e-01,  3.6663e-01,\n",
       "          -3.7911e-02, -2.4347e-01, -6.3793e-02, -2.1815e-02, -1.7477e-01,\n",
       "           8.1840e-02,  8.6021e-02,  1.7668e-01,  1.9756e-01, -2.5598e-01,\n",
       "          -1.1381e-01, -3.4735e-01,  1.2064e-01, -4.9369e-03,  3.7298e-01,\n",
       "           6.6049e-02,  1.7251e-01, -1.3744e-01,  2.2471e-01,  6.2586e-02,\n",
       "           3.3100e-01,  1.1605e-01, -7.6892e-02, -1.1532e-01,  2.6284e-02,\n",
       "          -1.9663e-01,  1.3052e-01, -3.9807e-02,  2.4890e-01, -1.4221e-01,\n",
       "          -6.9413e-03, -9.4677e-02,  6.7206e-02, -4.4246e-01,  2.0318e-01,\n",
       "          -5.4312e-02,  2.2955e-01, -1.1185e-01,  4.5975e-02, -3.0779e-01,\n",
       "          -2.0885e-01, -5.5737e-01, -2.5313e-01,  1.2569e-01, -2.0181e-01,\n",
       "          -2.5140e-01, -2.1319e-01, -2.3389e-01,  2.5118e-01,  1.0444e-01,\n",
       "          -9.3709e-02,  2.6567e-01, -9.2356e-03,  8.5706e-02,  2.9555e-02,\n",
       "          -1.0901e-03,  1.0854e-01,  1.4911e-01,  1.0911e-01,  4.6317e-03,\n",
       "          -4.5364e-01,  6.4951e-02,  5.1204e-01,  4.5070e-02,  3.5309e-01,\n",
       "          -2.6169e-01,  1.5892e-01, -1.8759e-01,  8.0931e-02, -1.5673e-01,\n",
       "           3.1781e-01,  2.4186e-01,  1.4195e-01, -5.7960e-02, -2.1879e-01,\n",
       "          -3.9357e-02,  2.9624e-01,  6.9045e-02,  4.2252e-02, -1.3590e-01,\n",
       "          -2.0413e-01, -2.9312e-01, -1.0235e-01,  1.8207e-02, -2.2969e-01,\n",
       "           4.6641e-01, -9.0372e-02,  3.6463e-02, -1.0181e-01,  1.4460e-01,\n",
       "          -1.8688e-01,  3.1896e-02, -4.6111e-02, -6.0641e-02,  1.0143e-01,\n",
       "          -9.1146e-02,  9.6946e-02,  2.2511e-01,  6.5458e-02,  3.0277e-02,\n",
       "           2.5575e-01, -1.4143e-03, -6.1967e-02,  1.6394e-01,  2.6592e-01,\n",
       "          -7.1706e-02,  1.2125e-01,  6.1986e-02, -5.4316e-02, -2.9374e-01,\n",
       "          -2.5845e-02,  3.6862e-02,  8.3491e-03,  1.1467e-01,  4.3105e-01,\n",
       "          -2.5261e-01, -8.2594e-02, -4.5061e-02, -3.9239e-02, -6.1770e-02,\n",
       "          -8.9522e-02,  1.6852e-01,  7.5903e-02, -6.2312e-02,  1.7686e-02,\n",
       "          -5.5122e-02, -1.3073e-01, -1.5270e-01, -5.3549e-02,  5.3696e-02,\n",
       "          -1.7208e-01,  4.1693e-01, -1.0629e-01,  2.4776e-01, -8.4640e-02,\n",
       "          -3.8824e-01,  2.6061e-01, -1.5364e-01, -3.6149e-02,  1.2991e-01,\n",
       "           1.6077e-01,  7.8555e-03, -1.7734e-01, -2.5661e-02, -2.8434e-02,\n",
       "           1.4266e-01,  6.3670e-02,  3.5782e-01,  3.5624e-01, -1.4273e-01,\n",
       "           4.0379e-01, -1.6081e-02,  3.6598e-03, -2.1655e-01, -2.2918e-01,\n",
       "           1.0139e-01,  1.5702e-01,  4.5970e-02, -2.2864e-01, -1.5470e-02,\n",
       "          -1.3869e-01,  7.9072e-02, -1.8118e-02, -1.2263e-01, -3.7886e-02,\n",
       "          -2.2483e-01,  3.2867e-01, -3.7717e-01,  3.4737e-02, -5.8808e-02,\n",
       "          -9.2526e-02, -1.8681e-01,  4.8722e-02, -2.5632e-01,  2.3552e-01,\n",
       "          -1.9595e-01,  1.7809e-01], device='cuda:0'),\n",
       "  tensor([ 5.2274e-01,  1.5641e-01, -1.0848e+00,  1.1177e-01,  7.5653e-02,\n",
       "           9.4285e-02, -1.8001e-01,  2.5528e-02, -2.4492e-01, -1.7891e-01,\n",
       "          -4.5921e-02,  2.5578e-02,  1.8064e-01, -4.5970e-01,  6.3076e-02,\n",
       "          -3.7772e-01,  1.5441e-01, -2.4951e-01,  4.7971e-01,  3.5050e-01,\n",
       "          -2.6991e-01, -9.2889e-02, -1.7606e-01,  1.7152e-01,  2.3409e-01,\n",
       "          -7.8755e-02, -4.0918e-02, -3.0925e-01, -1.6300e-01,  3.8787e-01,\n",
       "           2.5585e-01,  3.3782e-01,  1.5435e-01,  7.4610e-02, -2.3172e-02,\n",
       "           1.3508e-01,  2.6527e-02, -4.3903e-02, -6.8146e-02,  2.9305e-01,\n",
       "           2.3502e-02, -1.9274e-01, -7.0365e-02, -3.2197e-01,  5.4049e-01,\n",
       "           3.2196e-01, -2.7936e-01,  7.5554e-02, -1.5153e-01, -2.3615e-01,\n",
       "           2.3531e-01,  1.5285e-01, -2.6181e-01,  1.7080e-01,  2.8013e-01,\n",
       "           1.7029e-01, -1.4272e-02,  1.2108e-02, -1.6482e-01,  8.8154e-02,\n",
       "          -2.0103e-01, -1.1609e-01,  4.7431e-01,  3.1184e-01,  4.3271e-01,\n",
       "           9.0316e-02,  1.8462e-01,  1.6195e-01,  6.8520e-02, -1.0234e-01,\n",
       "          -9.5794e-02,  4.6709e-01,  4.8865e-01, -5.3075e-02,  1.7724e-01,\n",
       "          -1.0268e-01,  2.9243e-02,  3.3458e-01, -1.4299e-01, -1.4205e-01,\n",
       "           5.5196e-01, -1.6875e-01,  1.9743e-01, -2.6752e-01, -1.1521e-02,\n",
       "           1.7574e-01,  6.4551e-02, -5.0148e-01, -3.4393e-01,  2.6587e-01,\n",
       "           4.6449e-01, -5.0084e-04,  8.8365e-03,  2.4609e-01,  2.6972e-01,\n",
       "           8.8671e-05,  1.6340e-01, -3.4826e-01,  2.7051e+00,  2.1946e-01,\n",
       "           1.7937e-01, -2.9697e-01, -4.5259e-01,  2.4084e-01, -2.9354e-01,\n",
       "           4.4238e-01,  3.2129e-01, -7.7020e-02, -3.8553e-02, -1.4583e-01,\n",
       "          -3.7871e-01,  2.1882e-01,  1.4687e-01,  9.9550e-02, -1.0327e-01,\n",
       "           4.0365e-02,  1.8728e-01, -1.0975e-01,  4.5076e-01,  2.5217e-01,\n",
       "           1.8275e-01, -3.9115e-01, -3.5903e-01, -1.4331e-01, -1.1253e-01,\n",
       "          -2.5967e-01,  2.5023e-01, -4.1895e-01,  1.8815e-01, -2.4978e-01,\n",
       "           4.5059e-01,  1.3987e-01, -1.1408e-01,  2.5577e-01, -1.5558e-01,\n",
       "          -2.2749e-01,  2.1994e-01, -2.3140e-01, -4.2276e-01,  1.9363e-01,\n",
       "          -1.8481e-01, -9.6807e-03,  2.1645e-01,  1.5031e-01,  2.4152e-01,\n",
       "          -4.2557e-01, -3.0346e-01, -2.2863e-01,  2.3039e-01, -6.4430e-01,\n",
       "          -2.1962e-01, -7.2523e-02, -3.3888e-01,  4.6646e-02, -3.4023e-02,\n",
       "          -1.6126e-01,  1.8565e-01,  2.4995e-01, -7.5664e-02, -1.1935e-01,\n",
       "          -4.3957e-02,  8.8616e-02,  2.8147e-01, -1.9149e-01,  5.2789e-02,\n",
       "           1.3602e-01, -5.7536e-02, -1.0664e-01,  2.9895e-01,  9.0617e-02,\n",
       "          -2.4501e-01,  6.2611e-02,  9.3418e-02, -3.0951e-01, -1.2341e-01,\n",
       "           3.8067e-02, -1.7876e-01,  4.6486e-01, -1.7777e-01, -7.2212e-02,\n",
       "           3.2726e-01, -3.2004e-01, -2.6162e-02, -1.7617e-01,  4.4617e-01,\n",
       "          -3.3485e-02, -8.0312e-02,  1.2325e-01, -1.3496e-01,  2.0807e-01,\n",
       "          -6.2653e-01,  3.0064e-01, -2.3005e-01, -2.4519e-01, -1.9420e-01,\n",
       "           4.5950e-01, -6.5656e-01, -7.4801e-02, -1.3110e-01, -1.3971e-01,\n",
       "          -3.7797e-01,  5.9950e-02, -1.9938e-02,  1.0263e-01,  2.8390e-01,\n",
       "          -3.7863e-02, -1.5375e-01,  1.1312e-01,  2.1486e-01,  3.8871e-01,\n",
       "          -2.5118e-01,  1.1082e-01, -2.9860e-01, -6.2103e-02,  2.1795e-02,\n",
       "           2.4248e-01,  3.7907e-01, -1.0378e-01, -1.7678e-01, -7.0341e-02,\n",
       "           2.8019e-01, -1.2278e-01,  2.2996e-01,  1.3770e-01,  4.2115e-01,\n",
       "          -3.6226e-01, -6.6510e-01, -1.9985e-02, -1.3854e-01,  1.9808e-01,\n",
       "          -6.1745e-01, -3.9260e-01, -7.5302e-02,  2.5463e-01, -6.3912e-02,\n",
       "          -1.0777e-01,  2.0373e-01,  1.7897e-02, -5.0587e-01, -2.4789e-01,\n",
       "           1.0027e-01,  3.4695e+00, -1.2569e-01, -7.9449e-02,  8.5338e-02,\n",
       "           4.5396e-02,  1.8641e-01, -4.3182e-01, -2.8186e-02, -6.3559e-01,\n",
       "          -6.4887e-01, -6.9457e-02,  4.8287e-01, -5.8451e-01,  2.1489e-01,\n",
       "           1.6621e-01, -6.8065e-02,  4.4866e-01, -3.4670e-02,  3.2352e-01,\n",
       "          -3.7933e-01,  1.3323e-02,  3.2149e-01,  2.9157e-01, -1.3252e+00,\n",
       "          -4.1968e-03, -3.6521e-02,  2.1551e-01, -2.0766e-01,  3.7599e-01,\n",
       "           2.2564e-01,  5.2119e-01, -4.4355e-02, -1.8952e-01,  2.5574e-01,\n",
       "           1.1558e+00, -1.8362e-01, -2.5785e-01,  3.3655e-01, -5.9746e-03,\n",
       "          -2.3840e-01,  4.8814e-02, -3.9995e-01, -5.1703e-02,  9.0431e-02,\n",
       "          -1.4829e-01,  2.6324e-01, -2.1299e-01, -2.0745e-01,  2.1008e-01,\n",
       "           4.3979e-01, -4.1771e-01,  1.6865e-01, -1.4575e-03,  1.6424e-01,\n",
       "          -2.7560e-02, -2.9673e-01,  4.4356e-01,  3.8644e-01, -1.7177e-02,\n",
       "           3.0430e-01, -1.3036e-01,  5.8990e-01, -2.6672e-02,  1.3839e-01,\n",
       "          -2.1776e-01,  2.5886e-01, -1.0365e-01,  5.0708e-01,  7.1468e-02,\n",
       "          -2.8835e-01,  9.0630e-03,  5.4950e-01,  1.3183e-02,  7.8507e-02,\n",
       "          -2.4742e-02, -2.8229e-01,  6.9248e-02,  1.3588e-01, -8.1964e-02,\n",
       "          -1.2618e-01,  1.7700e-01, -1.8295e-01, -3.7349e-01, -3.0257e-02,\n",
       "           4.9901e-01, -4.6310e-02, -1.0667e-01,  2.5187e-01, -2.4974e-01,\n",
       "          -1.0153e+00,  2.5389e-01, -1.6947e-01,  9.5279e-01,  2.2441e-01,\n",
       "           1.8631e-02, -3.5385e-01, -3.7518e-03, -1.3832e-03, -1.4773e-01,\n",
       "           1.1633e-01, -1.4843e-02,  3.2812e-01,  7.6172e-02, -8.9335e-02,\n",
       "          -2.7072e-02, -3.8204e-01,  1.4398e-01, -2.2048e-01,  2.6724e-01,\n",
       "           1.7931e-01,  1.9831e-02, -3.5856e-02,  3.1362e-01, -5.6898e-02,\n",
       "           2.9256e-01,  1.3741e-01,  1.1482e-01, -2.2795e-01,  6.1906e-02,\n",
       "          -3.8382e-01,  9.9245e-02,  2.6809e-02,  1.2444e-01, -1.0488e-01,\n",
       "          -6.3618e-02, -1.7001e-01, -9.3777e-02, -4.4260e-01,  2.7408e-01,\n",
       "          -2.6038e-01,  5.7491e-02,  4.8378e-02,  2.6667e-02, -2.7939e-01,\n",
       "          -2.1347e-01, -5.7146e-01, -4.2780e-01,  3.9927e-01, -2.3433e-01,\n",
       "          -3.4036e-01, -2.6126e-01, -2.2133e-01,  3.1489e-01,  1.0589e-01,\n",
       "          -2.3049e-01,  3.3393e-01,  3.9580e-02,  6.9268e-02,  1.0786e-01,\n",
       "           4.3680e-02, -1.3661e-03,  4.0690e-02,  3.1856e-01,  4.8938e-02,\n",
       "          -4.7885e-01,  1.4441e-01,  9.0219e-01, -6.4605e-02,  3.9035e-01,\n",
       "          -1.3006e-01,  2.2442e-01, -1.6776e-01, -9.6538e-02, -3.2596e-01,\n",
       "           2.4414e-01,  1.7990e-01,  1.3210e-01, -2.7508e-02, -1.6708e-01,\n",
       "          -7.2453e-02,  3.6112e-01,  1.7543e-03,  2.1864e-01,  3.5058e-02,\n",
       "          -1.1467e-01, -2.9627e-01, -9.2460e-03,  1.7361e-02, -1.5156e-01,\n",
       "           4.3672e-01, -1.0810e-01,  2.1949e-01, -1.6031e-01,  7.9630e-02,\n",
       "          -2.6704e-01,  9.6436e-02, -9.1067e-02, -3.4006e-02, -1.0135e-01,\n",
       "          -1.3300e-01, -1.1336e-01,  2.4301e-01, -3.6827e-02, -7.1317e-03,\n",
       "           3.4798e-01,  2.6985e-03,  3.1138e-02,  1.8100e-01,  2.7475e-01,\n",
       "          -6.9054e-02, -6.8759e-02, -7.0484e-02,  6.8572e-02, -3.5036e-01,\n",
       "           2.1605e-02, -2.0104e-02, -1.4014e-03,  1.6381e-01,  5.8007e-01,\n",
       "          -5.6445e-01, -3.6321e-02, -1.2807e-01, -2.9857e-02, -1.2163e-01,\n",
       "          -1.0122e-01,  1.7037e-01,  1.1601e-01, -8.2954e-02,  2.6946e-03,\n",
       "          -1.1136e-01, -8.7299e-02, -2.0346e-01, -1.9766e-01, -2.6130e-02,\n",
       "          -4.3175e-01,  4.3006e-01, -1.5023e-01,  2.6643e-01,  1.0258e-01,\n",
       "          -5.2727e-01,  5.2501e-01, -2.9109e-01,  1.4633e-02,  5.9105e-02,\n",
       "           1.1609e-01,  2.1430e-02, -1.7106e-01, -1.4557e-01,  5.3713e-02,\n",
       "           2.6238e-01, -4.4832e-02,  2.7818e-01,  3.5873e-01, -1.1511e-01,\n",
       "           3.5144e-01, -4.1552e-01, -6.7309e-02, -2.8997e-01, -2.2453e-01,\n",
       "           3.9258e-02,  1.5291e-01, -2.7494e-02, -1.2718e-01,  1.2670e-01,\n",
       "          -2.5984e-01, -8.0735e-02,  2.9160e-01, -4.9139e-02, -2.7177e-02,\n",
       "          -1.5466e-01,  3.1755e-01, -4.1531e-01,  5.7373e-02,  3.9311e-02,\n",
       "          -3.1150e-02, -8.1021e-02,  1.2103e-01, -3.4716e-01,  3.1155e-01,\n",
       "          -2.5113e-01,  3.3502e-01], device='cuda:0')]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.clipmodel.text.transformer.pooler\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"CLIP/ckpt/BiomedNLP-BiomedBERT-base-uncased-abstract\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.35.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clipmodel.text.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclipmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/MVFA/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "model.clipmodel.text.proj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# text prompt\n",
    "with torch.cuda.amp.autocast(), torch.no_grad():\n",
    "    text_features = encode_text_with_prompt_ensemble(biomedclip_model, tokenizer, REAL_NAME[args.obj], device)\n",
    "\n",
    "print(text_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(512).expand((1, -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer([\"a photo of\",\"a apple of\"])\n",
    "prompt.shape\n",
    "# o = model.clipmodel.text.transformer.embeddings.word_embeddings(prompt.cuda())\n",
    "# model.clipmodel.encode_text(prompt.unsqueeze(0).cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.prompt_learner.ctx.shape\n",
    "# tokenizer(\"a plt a\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = \"brain\"\n",
    "ctx_init = \"a photo of a\"\n",
    "prompt_normal = ['{}', 'flawless {}', 'perfect {}', 'unblemished {}', '{} without flaw', '{} without defect', '{} without damage']\n",
    "prompt_abnormal = ['damaged {}', 'broken {}', '{} with flaw', '{} with defect', '{} with damage']\n",
    "prompt_state = [prompt_normal, prompt_abnormal]\n",
    "text_features = []\n",
    "for i in range(len(prompt_state)):\n",
    "    prompted_state = [state.format(obj) for state in prompt_state[i]]\n",
    "    prompted_sentence = []\n",
    "    for s in prompted_state:\n",
    "        prompted_sentence.append(ctx_init + \" \" + s)\n",
    "    prompted_sentence = tokenizer(prompted_sentence).to(device)\n",
    "    print(len(prompted_sentence))\n",
    "    prompted_sentence = torch.tensor(prompted_sentence).cuda().float()\n",
    "    prompted_sentence = prompted_sentence.mean(dim=0)\n",
    "    text_features.append(prompted_sentence)\n",
    "tokenized_prompts = torch.stack(text_features, dim=0).cuda().float()\n",
    "tokenized_prompts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.prompt_learner.token_prefix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_normal = ['{}', 'flawless {}', 'perfect {}', 'unblemished {}', '{} without flaw', '{} without defect', '{} without damage']\n",
    "prompt_abnormal = ['damaged {}', 'broken {}', '{} with flaw', '{} with defect', '{} with damage']\n",
    "prompt_state = [prompt_normal, prompt_abnormal]\n",
    "prompt_templates = ['a bad photo of a {}.', 'a low resolution photo of the {}.', 'a bad photo of the {}.', 'a cropped photo of the {}.', 'a bright photo of a {}.', 'a dark photo of the {}.', 'a photo of my {}.', 'a photo of the cool {}.', 'a close-up photo of a {}.', 'a black and white photo of the {}.', 'a bright photo of the {}.', 'a cropped photo of a {}.', 'a jpeg corrupted photo of a {}.', 'a blurry photo of the {}.', 'a photo of the {}.', 'a good photo of the {}.', 'a photo of one {}.', 'a close-up photo of the {}.', 'a photo of a {}.', 'a low resolution photo of a {}.', 'a photo of a large {}.', 'a blurry photo of a {}.', 'a jpeg corrupted photo of the {}.', 'a good photo of a {}.', 'a photo of the small {}.', 'a photo of the large {}.', 'a black and white photo of a {}.', 'a dark photo of a {}.', 'a photo of a cool {}.', 'a photo of a small {}.', 'there is a {} in the scene.', 'there is the {} in the scene.', 'this is a {} in the scene.', 'this is the {} in the scene.', 'this is one {} in the scene.']\n",
    "\n",
    "text_features = []\n",
    "for i in range(len(prompt_state)):\n",
    "    prompted_state = [state.format(obj) for state in prompt_state[i]]\n",
    "    prompted_sentence = []\n",
    "    for s in prompted_state:\n",
    "        for template in prompt_templates:\n",
    "            prompted_sentence.append(template.format(s))\n",
    "    # prompted_sentence = tokenize(prompted_sentence).to(device)\n",
    "\n",
    "    prompted_sentence = tokenizer(prompted_sentence)\n",
    "    prompted_sentence = torch.tensor(prompted_sentence).to(device)\n",
    "\n",
    "    class_embeddings = model.clipmodel.encode_text(prompted_sentence)\n",
    "    class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "    class_embedding = class_embeddings.mean(dim=0)\n",
    "    class_embedding /= class_embedding.norm()\n",
    "    text_features.append(class_embedding)\n",
    "text_features = torch.stack(text_features, dim=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([175, 256])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompted_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 703/968 [00:02<00:01, 197.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 728/968 [00:03<00:02, 83.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 746/968 [00:04<00:03, 62.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 760/968 [00:04<00:04, 51.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 771/968 [00:05<00:04, 45.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 773/968 [00:05<00:01, 149.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 245\u001b[0m\n\u001b[1;32m    241\u001b[0m seg_mem_features \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mcat([seg_features[j][i] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seg_features))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seg_features[\u001b[38;5;241m0\u001b[39m]))]\n\u001b[1;32m    242\u001b[0m det_mem_features \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mcat([det_features[j][i] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(det_features))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(det_features[\u001b[38;5;241m0\u001b[39m]))]\n\u001b[0;32m--> 245\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_mem_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdet_mem_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m>\u001b[39m best_result:\n\u001b[1;32m    247\u001b[0m     best_result \u001b[38;5;241m=\u001b[39m result\n",
      "Cell \u001b[0;32mIn[31], line 74\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(args, model, test_loader, text_features, seg_mem_features, det_mem_features)\u001b[0m\n\u001b[1;32m     72\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos_sim(det_mem_features[idx], p)\n\u001b[1;32m     73\u001b[0m height \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39msqrt(cos\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m---> 74\u001b[0m anomaly_map_few_shot \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, height, height)\n\u001b[1;32m     75\u001b[0m anomaly_map_few_shot \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(torch\u001b[38;5;241m.\u001b[39mtensor(anomaly_map_few_shot),\n\u001b[1;32m     76\u001b[0m                                         size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mimg_size, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     77\u001b[0m anomaly_maps_few_shot\u001b[38;5;241m.\u001b[39mappend(anomaly_map_few_shot[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 测试原本\n",
    "args.batch_size = 1\n",
    "args.shot = 4\n",
    "def test(args, model, test_loader, text_features, seg_mem_features, det_mem_features):\n",
    "    gt_list = []\n",
    "    gt_mask_list = []\n",
    "\n",
    "    det_image_scores_zero = []\n",
    "    det_image_scores_few = []\n",
    "    \n",
    "    seg_score_map_zero = []\n",
    "    seg_score_map_few= []\n",
    "\n",
    "    step = 0\n",
    "    for (image, y, mask) in tqdm(test_loader):\n",
    "        step += 1\n",
    "        if step < 700:\n",
    "            continue\n",
    "        \n",
    "        image = image.to(device)\n",
    "        mask[mask > 0.5], mask[mask <= 0.5] = 1, 0\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            _, seg_patch_tokens, det_patch_tokens = model(image)\n",
    "            seg_patch_tokens = [p[0, 1:, :] for p in seg_patch_tokens]\n",
    "            det_patch_tokens = [p[0, 1:, :] for p in det_patch_tokens]\n",
    "\n",
    "            if CLASS_INDEX[args.obj] > 0:\n",
    "\n",
    "                # few-shot, seg head\n",
    "                anomaly_maps_few_shot = []\n",
    "                for idx, p in enumerate(seg_patch_tokens):\n",
    "                    \n",
    "                    cos = cos_sim(seg_mem_features[idx], p)\n",
    "                    height = int(np.sqrt(cos.shape[1]))\n",
    "                    anomaly_map_few_shot = torch.min((1 - cos), 0)[0].reshape(1, 1, height, height)\n",
    "\n",
    "                    anomaly_map_few_shot = F.interpolate(torch.tensor(anomaly_map_few_shot),\n",
    "                                                            size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                    anomaly_maps_few_shot.append(anomaly_map_few_shot[0].cpu().numpy())\n",
    " \n",
    "                score_map_few = np.sum(anomaly_maps_few_shot, axis=0)\n",
    "                seg_score_map_few.append(score_map_few)\n",
    "\n",
    "                # zero-shot, seg head\n",
    "                anomaly_maps = []\n",
    "                for layer in range(len(seg_patch_tokens)):\n",
    "                    \n",
    "                    seg_patch_tokens[layer] /= seg_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "                    anomaly_map = (100.0 * seg_patch_tokens[layer] @ text_features).unsqueeze(0)\n",
    "        \n",
    "                    B, L, C = anomaly_map.shape\n",
    "                    H = int(np.sqrt(L))\n",
    "                    anomaly_map = F.interpolate(anomaly_map.permute(0, 2, 1).view(B, 2, H, H),\n",
    "                                                size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                    # print(\"augment_normal_imgs.shape:\", anomaly_map.shape)\n",
    "                    anomaly_map = torch.softmax(anomaly_map, dim=1)[:, 1, :, :]\n",
    "                    print(\"anomaly_map.shape:\", anomaly_map.shape)\n",
    "                    anomaly_maps.append(anomaly_map.cpu().numpy())\n",
    "                    print(len(anomaly_maps))\n",
    "                    print(\"anomaly_maps.shape:\", anomaly_maps[0].shape)\n",
    "                score_map_zero = np.sum(anomaly_maps, axis=0)\n",
    "                print(\"score_map_zero.shape:\", score_map_zero.shape)\n",
    "                seg_score_map_zero.append(score_map_zero)\n",
    "                \n",
    "\n",
    "\n",
    "            else:\n",
    "                # few-shot, det head\n",
    "                anomaly_maps_few_shot = []\n",
    "                for idx, p in enumerate(det_patch_tokens):\n",
    "                    cos = cos_sim(det_mem_features[idx], p)\n",
    "                    height = int(np.sqrt(cos.shape[1]))\n",
    "                    anomaly_map_few_shot = torch.min((1 - cos), 0)[0].reshape(1, 1, height, height)\n",
    "                    anomaly_map_few_shot = F.interpolate(torch.tensor(anomaly_map_few_shot),\n",
    "                                                            size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                    anomaly_maps_few_shot.append(anomaly_map_few_shot[0].cpu().numpy())\n",
    "                print(\"shape anomaly\",len(anomaly_maps_few_shot))\n",
    "                print(\"shapt\", anomaly_map_few_shot[0].shape)\n",
    "                \n",
    "                anomaly_map_few_shot = np.sum(anomaly_maps_few_shot, axis=0)\n",
    "\n",
    "                print(\"shape anomaly\",len(anomaly_map_few_shot))\n",
    "                print(\"shapt\", anomaly_map_few_shot[0].shape)\n",
    "                \n",
    "                # print(\"sahtp\")\n",
    "                # print(\"augment_normal_img len(anomaly_map_few_shot):\", len(anomaly_map_few_shot))\n",
    "                # print(\"anomaly_map_few_shot.shape:\", anomaly_map_few_shot.shape)\n",
    "                score_few_det = anomaly_map_few_shot.mean()\n",
    "                # print(\"ss\",score_few_det)\n",
    "                # print(\"score_few_det.shape:\", score_few_det.shape)\n",
    "                det_image_scores_few.append(score_few_det)\n",
    "\n",
    "                # zero-shot, det head\n",
    "                anomaly_score = 0\n",
    "                for layer in range(len(det_patch_tokens)):\n",
    "                    det_patch_tokens[layer] /= det_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "                    anomaly_map = (100.0 * det_patch_tokens[layer] @ text_features).unsqueeze(0)\n",
    "                    anomaly_map = torch.softmax(anomaly_map, dim=-1)[:, :, 1]\n",
    "                    anomaly_score += anomaly_map.mean()\n",
    "                det_image_scores_zero.append(anomaly_score.cpu().numpy())\n",
    "\n",
    "            \n",
    "            gt_mask_list.append(mask.squeeze().cpu().detach().numpy())\n",
    "            gt_list.extend(y.cpu().detach().numpy())\n",
    "            \n",
    "\n",
    "    gt_list = np.array(gt_list)\n",
    "    gt_mask_list = np.asarray(gt_mask_list)\n",
    "    gt_mask_list = (gt_mask_list>0).astype(np.int_)\n",
    "\n",
    "\n",
    "    if CLASS_INDEX[args.obj] > 0:\n",
    "\n",
    "        seg_score_map_zero = np.array(seg_score_map_zero)\n",
    "        seg_score_map_few = np.array(seg_score_map_few)\n",
    "\n",
    "        seg_score_map_zero = (seg_score_map_zero - seg_score_map_zero.min()) / (seg_score_map_zero.max() - seg_score_map_zero.min())\n",
    "        seg_score_map_few = (seg_score_map_few - seg_score_map_few.min()) / (seg_score_map_few.max() - seg_score_map_few.min())\n",
    "\n",
    "        print(\"seg_score_map_zero.shape:\", seg_score_map_zero.shape)\n",
    "        print('seg_score_map_few.shape:', seg_score_map_few.shape)\n",
    "        segment_scores = 0.5 * seg_score_map_zero + 0.5 * seg_score_map_few\n",
    "        seg_roc_auc = roc_auc_score(gt_mask_list.flatten(), segment_scores.flatten())\n",
    "        print(f'{args.obj} pAUC : {round(seg_roc_auc,4)}')\n",
    "\n",
    "        segment_scores_flatten = segment_scores.reshape(segment_scores.shape[0], -1)\n",
    "        roc_auc_im = roc_auc_score(gt_list, np.max(segment_scores_flatten, axis=1))\n",
    "        print(f'{args.obj} AUC : {round(roc_auc_im, 4)}')\n",
    "\n",
    "        return seg_roc_auc + roc_auc_im\n",
    "\n",
    "    else:\n",
    "\n",
    "        det_image_scores_zero = np.array(det_image_scores_zero)\n",
    "        det_image_scores_few = np.array(det_image_scores_few)\n",
    "\n",
    "        det_image_scores_zero = (det_image_scores_zero - det_image_scores_zero.min()) / (det_image_scores_zero.max() - det_image_scores_zero.min())\n",
    "        det_image_scores_few = (det_image_scores_few - det_image_scores_few.min()) / (det_image_scores_few.max() - det_image_scores_few.min())\n",
    "    \n",
    "        image_scores = 0.5 * det_image_scores_zero + 0.5 * det_image_scores_few\n",
    "        img_roc_auc_det = roc_auc_score(gt_list, image_scores)\n",
    "        print(f'{args.obj} AUC : {round(img_roc_auc_det,4)}')\n",
    "\n",
    "        return img_roc_auc_det\n",
    "    \n",
    "    # load test dataset\n",
    "kwargs = {'num_workers': 12, 'pin_memory': True} if use_cuda else {}\n",
    "test_dataset = MedDataset(args.data_path, args.obj, args.img_size, args.shot, args.iterate)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "# few-shot image augmentation\n",
    "augment_abnorm_img, augment_abnorm_mask = augment(test_dataset.fewshot_abnorm_img, test_dataset.fewshot_abnorm_mask)\n",
    "augment_normal_img, augment_normal_mask = augment(test_dataset.fewshot_norm_img)\n",
    "\n",
    "augment_fewshot_img = torch.cat([augment_abnorm_img, augment_normal_img], dim=0)\n",
    "augment_fewshot_mask = torch.cat([augment_abnorm_mask, augment_normal_mask], dim=0)\n",
    "\n",
    "augment_fewshot_label = torch.cat([torch.Tensor([1] * len(augment_abnorm_img)), torch.Tensor([0] * len(augment_normal_img))], dim=0)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(augment_fewshot_img, augment_fewshot_mask, augment_fewshot_label)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "# memory bank construction\n",
    "support_dataset = torch.utils.data.TensorDataset(augment_normal_img)\n",
    "support_loader = torch.utils.data.DataLoader(support_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "best_result = 0\n",
    "\n",
    "for epoch in range(args.epoch):\n",
    "        print('epoch ', epoch, ':')\n",
    "\n",
    "        # loss_list = []\n",
    "        # for (image, gt, label) in train_loader:\n",
    "        #     image = image.to(device)\n",
    "        #     with torch.cuda.amp.autocast():\n",
    "        #         _, seg_patch_tokens, det_patch_tokens = model(image)\n",
    "        #         # seg_patch_tokens size { [batch_size,196,512] * 4} \n",
    "        #         seg_patch_tokens = [p[0, 1:, :] for p in seg_patch_tokens]\n",
    "        #         det_patch_tokens = [p[0, 1:, :] for p in det_patch_tokens]\n",
    "\n",
    "        #         # det loss\n",
    "        #         det_loss = 0\n",
    "        #         image_label = label.to(device)\n",
    "        #         for layer in range(len(det_patch_tokens)):\n",
    "        #             det_patch_tokens[layer] = det_patch_tokens[layer] / det_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "        #             anomaly_map = (100.0 * det_patch_tokens[layer] @ text_features).unsqueeze(0)    \n",
    "        #             anomaly_map = torch.softmax(anomaly_map, dim=-1)[:, :, 1]\n",
    "        #             anomaly_score = torch.mean(anomaly_map, dim=-1)\n",
    "        #             det_loss += loss_bce(anomaly_score, image_label)\n",
    "\n",
    "        #         if CLASS_INDEX[args.obj] > 0:\n",
    "        #             # pixel level\n",
    "        #             seg_loss = 0\n",
    "        #             mask = gt.squeeze(0).to(device)\n",
    "        #             mask[mask > 0.5], mask[mask <= 0.5] = 1, 0\n",
    "        #             for layer in range(len(seg_patch_tokens)):\n",
    "        #                 seg_patch_tokens[layer] = seg_patch_tokens[layer] / seg_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "        #                 anomaly_map = (100.0 * seg_patch_tokens[layer] @ text_features).unsqueeze(0)\n",
    "        #                 B, L, C = anomaly_map.shape\n",
    "        #                 H = int(np.sqrt(L))\n",
    "        #                 anomaly_map = F.interpolate(anomaly_map.permute(0, 2, 1).view(B, 2, H, H),\n",
    "        #                                             size=args.img_size, mode='bilinear', align_corners=True)\n",
    "        #                 anomaly_map = torch.softmax(anomaly_map, dim=1)\n",
    "        #                 seg_loss += loss_focal(anomaly_map, mask)\n",
    "        #                 seg_loss += loss_dice(anomaly_map[:, 1, :, :], mask)\n",
    "                    \n",
    "        #             loss = seg_loss + det_loss\n",
    "        #             loss.requires_grad_(True)\n",
    "        #             seg_optimizer.zero_grad()\n",
    "        #             det_optimizer.zero_grad()\n",
    "        #             loss.backward()\n",
    "        #             seg_optimizer.step()\n",
    "        #             det_optimizer.step()\n",
    "\n",
    "        #         else:\n",
    "        #             loss = det_loss\n",
    "        #             loss.requires_grad_(True)\n",
    "        #             det_optimizer.zero_grad()\n",
    "        #             loss.backward()\n",
    "        #             det_optimizer.step()\n",
    "\n",
    "        #         loss_list.append(loss.item())\n",
    "\n",
    "        # print(\"Loss: \", np.mean(loss_list))\n",
    "\n",
    "\n",
    "        seg_features = []\n",
    "        det_features = []\n",
    "        for image in support_loader:\n",
    "            image = image[0].to(device)\n",
    "            with torch.no_grad():\n",
    "                _, seg_patch_tokens, det_patch_tokens = model(image)\n",
    "\n",
    "                seg_patch_tokens = [p[0].contiguous() for p in seg_patch_tokens]\n",
    "                det_patch_tokens = [p[0].contiguous() for p in det_patch_tokens]\n",
    "                seg_features.append(seg_patch_tokens)\n",
    "                det_features.append(det_patch_tokens)\n",
    "        seg_mem_features = [torch.cat([seg_features[j][i] for j in range(len(seg_features))], dim=0) for i in range(len(seg_features[0]))]\n",
    "        det_mem_features = [torch.cat([det_features[j][i] for j in range(len(det_features))], dim=0) for i in range(len(det_features[0]))]\n",
    "        \n",
    "\n",
    "        result = test(args, model, test_loader, text_features, seg_mem_features, det_mem_features)\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            print(\"Best result\\n\")\n",
    "            if args.save_model == 1:\n",
    "                ckp_path = os.path.join(args.save_path, f'{args.obj}.pth')\n",
    "                torch.save({'seg_adapters': model.seg_adapters.state_dict(),\n",
    "                            'det_adapters': model.det_adapters.state_dict()}, \n",
    "                            ckp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数组形状: (4, 4, 1, 2)\n",
      "原始数组内容:\n",
      " [[[[ 0  1]]\n",
      "\n",
      "  [[ 2  3]]\n",
      "\n",
      "  [[ 4  5]]\n",
      "\n",
      "  [[ 6  7]]]\n",
      "\n",
      "\n",
      " [[[ 8  9]]\n",
      "\n",
      "  [[10 11]]\n",
      "\n",
      "  [[12 13]]\n",
      "\n",
      "  [[14 15]]]\n",
      "\n",
      "\n",
      " [[[16 17]]\n",
      "\n",
      "  [[18 19]]\n",
      "\n",
      "  [[20 21]]\n",
      "\n",
      "  [[22 23]]]\n",
      "\n",
      "\n",
      " [[[24 25]]\n",
      "\n",
      "  [[26 27]]\n",
      "\n",
      "  [[28 29]]\n",
      "\n",
      "  [[30 31]]]]\n",
      "\n",
      "沿 axis=0 求和后形状: (4, 1, 2)\n",
      "结果:\n",
      " [[[48 52]]\n",
      "\n",
      " [[56 60]]\n",
      "\n",
      " [[64 68]]\n",
      "\n",
      " [[72 76]]]\n",
      "\n",
      "沿 axis=1 求和后形状: (4, 1, 2)\n",
      "结果:\n",
      " [[[ 12  16]]\n",
      "\n",
      " [[ 44  48]]\n",
      "\n",
      " [[ 76  80]]\n",
      "\n",
      " [[108 112]]]\n",
      "\n",
      "沿 axis=2 求和后形状: (4, 4, 2)\n",
      "结果:\n",
      " [[[ 0  1]\n",
      "  [ 2  3]\n",
      "  [ 4  5]\n",
      "  [ 6  7]]\n",
      "\n",
      " [[ 8  9]\n",
      "  [10 11]\n",
      "  [12 13]\n",
      "  [14 15]]\n",
      "\n",
      " [[16 17]\n",
      "  [18 19]\n",
      "  [20 21]\n",
      "  [22 23]]\n",
      "\n",
      " [[24 25]\n",
      "  [26 27]\n",
      "  [28 29]\n",
      "  [30 31]]]\n",
      "\n",
      "沿 axis=3 求和后形状: (4, 4, 1)\n",
      "结果:\n",
      " [[[ 1]\n",
      "  [ 5]\n",
      "  [ 9]\n",
      "  [13]]\n",
      "\n",
      " [[17]\n",
      "  [21]\n",
      "  [25]\n",
      "  [29]]\n",
      "\n",
      " [[33]\n",
      "  [37]\n",
      "  [41]\n",
      "  [45]]\n",
      "\n",
      " [[49]\n",
      "  [53]\n",
      "  [57]\n",
      "  [61]]]\n"
     ]
    }
   ],
   "source": [
    "# 生成一段高维数组（4，4，1，2）的对于各维度的求和示例代码\n",
    "import numpy as np\n",
    "\n",
    "# 生成一个高维数组 (4,4,1,2)，填充有序数字便于观察\n",
    "arr = np.arange(4*4*1*2).reshape(4,4,1,2)\n",
    "print(\"原始数组形状:\", arr.shape)\n",
    "print(\"原始数组内容:\\n\", arr)\n",
    "\n",
    "# ----------------------------\n",
    "# 不同维度的求和示例\n",
    "# ----------------------------\n",
    "\n",
    "# 1. 沿 axis=0 求和（合并第1个维度）\n",
    "sum_axis0 = np.sum(arr, axis=0)\n",
    "print(\"\\n沿 axis=0 求和后形状:\", sum_axis0.shape)\n",
    "print(\"结果:\\n\", sum_axis0)\n",
    "\n",
    "# 2. 沿 axis=1 求和（合并第2个维度）\n",
    "sum_axis1 = np.sum(arr, axis=1)\n",
    "print(\"\\n沿 axis=1 求和后形状:\", sum_axis1.shape)\n",
    "print(\"结果:\\n\", sum_axis1)\n",
    "\n",
    "# 3. 沿 axis=2 求和（合并第3个维度）\n",
    "sum_axis2 = np.sum(arr, axis=2)\n",
    "print(\"\\n沿 axis=2 求和后形状:\", sum_axis2.shape)\n",
    "print(\"结果:\\n\", sum_axis2)\n",
    "\n",
    "# 4. 沿 axis=3 求和（合并第4个维度）\n",
    "sum_axis3 = np.sum(arr, axis=3)\n",
    "print(\"\\n沿 axis=3 求和后形状:\", sum_axis3.shape)\n",
    "print(\"结果:\\n\", sum_axis3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Context vectors shape:  torch.Size([4, 768])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"biomedclip_local\"\n",
    "from CLIP.clip import get_model_config, list_models, list_pretrained_tags_by_model,load_checkpoint\n",
    "from CLIP.model import CLIP,get_cast_dtype\n",
    "import logging\n",
    "import torch\n",
    "from open_clip import CustomTextCLIP\n",
    " \n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "model_name = model_name.replace('/', '-')  # for callers using old naming with / in ViT names\n",
    "checkpoint_path = None\n",
    "model_cfg = None\n",
    "\n",
    "if isinstance(device, str):\n",
    "    device = torch.device(device)\n",
    "\n",
    "model_cfg = model_cfg or get_model_config(model_name)\n",
    "if model_cfg is not None:\n",
    "    logging.info(f'Loaded {model_name} model config.')\n",
    "else:\n",
    "    logging.error(f'Model config for {model_name} not found; available models {list_models()}.')\n",
    "    raise RuntimeError(f'Model config for {model_name} not found.')\n",
    "\n",
    "pretrained = \"CLIP/ckpt/model.pth.tar-100\"\n",
    "\n",
    "# cast_dtype set for fp16 and bf16 (manual mixed-precision), not set for 'amp' or 'pure' modes\n",
    "cast_dtype = get_cast_dtype(\"fp32\")\n",
    "is_hf_model = 'hf_model_name' in model_cfg.get('text_cfg', {})\n",
    "if is_hf_model:\n",
    "    # load pretrained weights for HF text model IFF no CLIP weights being loaded\n",
    "    model_cfg['text_cfg']['hf_model_pretrained'] = True and not pretrained\n",
    "custom_text = model_cfg.pop('custom_text', False) or False or is_hf_model\n",
    "\n",
    "if custom_text:\n",
    "    model = CustomTextCLIP(**model_cfg, cast_dtype=cast_dtype)\n",
    "else:\n",
    "    model = CLIP(**model_cfg, cast_dtype=cast_dtype)\n",
    "\n",
    "from CLIP.biomedcoop_biomedclip  import CustomCLIP\n",
    "model = CustomCLIP( model_cfg,[ \"normal brain\",\"glioma tumor\",\"meningioma tumor\",''\"pituitary tumor\"] ,model.eval())\n",
    "\n",
    "\n",
    "\n",
    "# 加载预训练模型权重\n",
    "logging.info(f'Loading pretrained {model_name} weight.')\n",
    "pretrained_loaded = False\n",
    "if pretrained:\n",
    "    checkpoint_path = pretrained\n",
    "    \n",
    "    if checkpoint_path:\n",
    "        logging.info(f'Loading pretrained {model_name} weights ({pretrained}).')\n",
    "        load_checkpoint(model, checkpoint_path)\n",
    "    else:\n",
    "        error_str = (\n",
    "            f'Pretrained weights ({pretrained}) not found for model {model_name}.'\n",
    "            f'Available pretrained tags ({list_pretrained_tags_by_model(model_name)}.')\n",
    "        logging.warning(error_str)\n",
    "        raise RuntimeError(error_str)\n",
    "    pretrained_loaded = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'ctx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m c \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mprompt_learner\u001b[38;5;241m.\u001b[39mnamed_parameters()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'ctx'"
     ]
    }
   ],
   "source": [
    "c = model.prompt_learner.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.5, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0005\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.optim.Adam([model.prompt_learner.ctx],lr=0.0005,betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seg_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mseg_features\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seg_features' is not defined"
     ]
    }
   ],
   "source": [
    "seg_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = model.prompt_learner.ctx\n",
    "emb = model.text_encoder.model.text.transformer.embeddings.word_embeddings\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.weight.data\n",
    "tokenizer = model.prompt_learner.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最接近的词汇： ['a', 'an', 'the', 'of', ',']\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def vector_to_token(vector, embedding_matrix, tokenizer):\n",
    "    similarities = []\n",
    "    for idx in range(embedding_matrix.shape[0]):\n",
    "        sim = 1 - cosine(vector.cpu().numpy(), embedding_matrix[idx].cpu().numpy())\n",
    "        similarities.append((sim, idx))\n",
    "    # 取相似度最高的前k个词\n",
    "    top_k = sorted(similarities, reverse=True)[:5]\n",
    "    tokens = [tokenizer.decode([idx]) for (sim, idx) in top_k]\n",
    "    return tokens\n",
    "\n",
    "# 示例：还原第一个类别（n_cls=0）的第一个上下文向量（n_ctx=0）\n",
    "ctx_vector = ctx.data[3, :]  # 形状 (ctx_dim,)\n",
    "closest_tokens = vector_to_token(ctx_vector, emb.weight.data, tokenizer.tokenizer)\n",
    "print(\"最接近的词汇：\", closest_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ! BTMRI 数据集对应训练好的模型 的 ctx\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# mri  curcumin  of  a \u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name,param \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# print(name)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(param\u001b[38;5;241m.\u001b[39mrequires_grad)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# ! BTMRI 数据集对应训练好的模型 的 ctx\n",
    "# mri  curcumin  of  a \n",
    "\n",
    "for name,param in model.named_parameters():\n",
    "    # print(name)\n",
    "    print(param.requires_grad)\n",
    "\n",
    "# a photo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000e+00, 4.2000e+01, 7.7450e+03, 1.6850e+03, 4.2000e+01, 7.9049e+03,\n",
       "         3.7863e+03, 8.0160e+03, 6.2929e+02, 2.3971e+02, 4.0371e+02, 4.2857e-01,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [2.0000e+00, 4.2000e+01, 7.7450e+03, 1.6850e+03, 4.2000e+01, 7.8268e+03,\n",
       "         2.1594e+03, 7.7480e+03, 1.8000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = \"brain\"\n",
    "ctx_init = \"a photo of a\"\n",
    "prompt_normal = ['{}', 'flawless {}', 'perfect {}', 'unblemished {}', '{} without flaw', '{} without defect', '{} without damage']\n",
    "prompt_abnormal = ['damaged {}', 'broken {}', '{} with flaw', '{} with defect', '{} with damage']\n",
    "prompt_state = [prompt_normal, prompt_abnormal]\n",
    "text_features = []\n",
    "for i in range(len(prompt_state)):\n",
    "    prompted_state = [state.format(obj) for state in prompt_state[i]]\n",
    "    prompted_sentence = []\n",
    "    for s in prompted_state:\n",
    "        prompted_sentence.append(ctx_init + \" \" + s)\n",
    "    prompted_sentence = tokenizer(prompted_sentence).to(device)\n",
    "    print(len(prompted_sentence))\n",
    "    prompted_sentence = torch.tensor(prompted_sentence).cuda().float()\n",
    "    prompted_sentence = prompted_sentence.mean(dim=0)\n",
    "    text_features.append(prompted_sentence)\n",
    "tokenized_prompts = torch.stack(text_features, dim=0).cuda().float()\n",
    "tokenized_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2233, -0.1067, -0.2095,  ..., -0.0742,  0.1278,  0.2407],\n",
       "        [ 0.2651, -0.0482, -0.1715,  ..., -0.0790,  0.1668,  0.1746],\n",
       "        [ 0.3134, -0.1118, -0.1505,  ...,  0.0369,  0.0741,  0.3053],\n",
       "        [ 0.1828, -0.0915, -0.1521,  ..., -0.0306,  0.1244,  0.2279],\n",
       "        [ 0.1878, -0.0841, -0.1896,  ..., -0.0871,  0.1765,  0.2513]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clipmodel.encode_text(prompted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CustomTextCLIP.encode_text of CustomTextCLIP(\n",
       "  (visual): TimmModel(\n",
       "    (trunk): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=512, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (text): HFTextEncoder(\n",
       "    (transformer): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): ClsLastHiddenStatePooler()\n",
       "    (proj): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=640, bias=False)\n",
       "      (1): GELU()\n",
       "      (2): Linear(in_features=640, out_features=512, bias=False)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m biomedclip_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 模型添加适配器\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCLIP_Inplanted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbiomedclip_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_list\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'tokenizer'"
     ]
    }
   ],
   "source": [
    "# coop\n",
    "parser = argparse.ArgumentParser(description='Testing')\n",
    "parser.add_argument('--model_name', type=str, default='biomedclip_local',)\n",
    "parser.add_argument('--pretrain', type=str, default='CLIP/ckpt/open_clip_pytorch_model.bin')\n",
    "parser.add_argument('--obj', type=str, default='Liver')\n",
    "parser.add_argument('--data_path', type=str, default='/root/data/')\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "parser.add_argument('--save_model', type=int, default=1)\n",
    "parser.add_argument('--save_path', type=str, default='./ckpt/few-shot/')\n",
    "parser.add_argument('--img_size', type=int, default=224)\n",
    "parser.add_argument(\"--epoch\", type=int, default=50, help=\"epochs\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.001, help=\"learning rate\")\n",
    "parser.add_argument(\"--features_list\", type=int, nargs=\"+\", default=[3,6,9,12], help=\"features used\")\n",
    "parser.add_argument('--seed', type=int, default=111)\n",
    "parser.add_argument('--shot', type=int, default=4)\n",
    "parser.add_argument('--iterate', type=int, default=0)\n",
    "args = parser.parse_args(args=['--obj', 'Liver',  '--shot', '4', '--batch_size', '1','--data_path','../MVFA-AD/data/'])\n",
    "\n",
    "setup_seed(args.seed)\n",
    "\n",
    "# fixed feature extractor\n",
    "biomedclip_model,tokenizer = create_model(model_name=args.model_name, \n",
    "                            force_image_size=args.img_size, \n",
    "                            device=device, \n",
    "                            pretrained=args.pretrain, \n",
    "                            require_pretrained=True)\n",
    "\n",
    "biomedclip_model.eval()\n",
    "\n",
    "# 模型添加适配器\n",
    "model = CLIP_Inplanted(clip_model=biomedclip_model,tokenizer=tokenizer, features=args.features_list).to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'seg_adapters' in name or 'det_adapters' in name:\n",
    "        param.requires_grad = True\n",
    "    if 'prompt_learner.ctx' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "# optimizer for only adapters\n",
    "seg_optimizer = torch.optim.Adam(list(model.seg_adapters.parameters()), lr=args.learning_rate, betas=(0.5, 0.999))\n",
    "det_optimizer = torch.optim.Adam(list(model.det_adapters.parameters()), lr=args.learning_rate, betas=(0.5, 0.999))\n",
    "ctx_optimizer = torch.optim.Adam([model.prompt_learner.ctx], lr=args.learning_rate, betas=(-1.5, 0.999))\n",
    " \n",
    "# losses\n",
    "loss_focal = FocalLoss()\n",
    "loss_dice = BinaryDiceLoss()\n",
    "loss_bce = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_optimizer = torch.optim.Adam([model.prompt_learner.ctx], lr=args.learning_rate, betas=(0.5, 0.999))\n",
    " \n",
    "# losses\n",
    "loss_focal = FocalLoss()\n",
    "loss_dice = BinaryDiceLoss()\n",
    "loss_bce = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MVFA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
