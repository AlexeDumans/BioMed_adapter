{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qin/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from CLIP.adapter import CLIP_Inplanted\n",
    "from CLIP.clip import create_model\n",
    "\n",
    "from loss import FocalLoss, BinaryDiceLoss\n",
    "\n",
    "from dataset.medical_zero import MedTestDataset, MedTrainDataset\n",
    "from dataset.medical_few import MedDataset\n",
    "\n",
    "from utils import augment, cos_sim, encode_text_with_prompt_ensemble\n",
    "from prompt import REAL_NAME\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "CLASS_INDEX = {'Brain':3, 'Liver':2, 'Retina_RESC':1, 'Retina_OCT2017':-1, 'Chest':-2, 'Histopathology':-3}\n",
    "CLASS_INDEX_INV = {3:'Brain', 2:'Liver', 1:'Retina_RESC', -1:'Retina_OCT2017', -2:'Chest', -3:'Histopathology'}\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, test_loader, text_features, seg_mem_features, det_mem_features):\n",
    "    gt_list = []\n",
    "    gt_mask_list = []\n",
    "\n",
    "    det_image_scores_zero = []\n",
    "    det_image_scores_few = []\n",
    "    \n",
    "    seg_score_map_zero = []\n",
    "    seg_score_map_few= []\n",
    "    \n",
    "    step = 0\n",
    "    for (image, y, mask) in tqdm(test_loader):\n",
    "        # step+=1\n",
    "        # if step < 80:\n",
    "        #     continue\n",
    "        image = image.to(device)\n",
    "        mask[mask > 0.5], mask[mask <= 0.5] = 1, 0\n",
    "        # print(\"mask.shape:\", mask.shape)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            _, seg_patch_tokens, det_patch_tokens = model(image)\n",
    "            # 去掉cls token ，由于 biomedclip cls token 位置不同，此处需要对应改变\n",
    "            seg_patch_tokens = [p[:, 1:, :] for p in seg_patch_tokens]\n",
    "            det_patch_tokens = [p[:, 1:, :] for p in det_patch_tokens]\n",
    "   \n",
    "            if CLASS_INDEX[args.obj] > 0:\n",
    "\n",
    "                # few-shot, seg head\n",
    "                anomaly_maps_few_shot = []\n",
    "                for idx, p in enumerate(seg_patch_tokens):\n",
    "                    batch_cos_sim = []\n",
    "                    for b in range(p.shape[0]):\n",
    "                        cos = cos_sim(seg_mem_features[idx][b], p[b])\n",
    "                        height = int(np.sqrt(cos.shape[1]))\n",
    "                        # * 去掉cls_token\n",
    "                        anomaly_map_few_shot = torch.min((1 - cos), 0)[0].reshape(1, 1, height, height)\n",
    "                        anomaly_map_few_shot = F.interpolate(torch.tensor(anomaly_map_few_shot),\n",
    "                                                            size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                        # * 去掉\n",
    "                        batch_cos_sim.append(anomaly_map_few_shot[0].cpu().numpy())\n",
    "                        # print('batch_cos_sim.shape:', batch_cos_sim[0].shape)\n",
    "                    anomaly_maps_few_shot.append(np.stack(batch_cos_sim, axis=0))\n",
    "                    # print('anomaly_maps_few_shot.shape:', anomaly_maps_few_shot[0].shape)\n",
    "                score_map_few = np.sum(anomaly_maps_few_shot, axis=0)\n",
    "                seg_score_map_few.append(score_map_few)\n",
    "                # print('seg_score_map_few.shape:', seg_score_map_few[0].shape)\n",
    "                \n",
    "                # zero-shot, seg head\n",
    "                anomaly_maps = []\n",
    "                for layer in range(len(seg_patch_tokens)):\n",
    "                    seg_patch_tokens[layer] /= seg_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "                    anomaly_map = (100.0 * seg_patch_tokens[layer] @ text_features)\n",
    "                    B, L, C = anomaly_map.shape\n",
    "                    H = int(np.sqrt(L))\n",
    "                    anomaly_map = F.interpolate(anomaly_map.permute(0, 2, 1).view(B, 2, H, H),\n",
    "                                                size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                    # print('anomaly_map.shape:', anomaly_map.shape)\n",
    "                    # 4 2 224 224\n",
    "                    anomaly_map = torch.softmax(anomaly_map, dim=1)[:, 1:2, :, :]\n",
    "                    # 4 224 224 \n",
    "                    # print('anomaly_map.shape:', anomaly_map.shape)\n",
    "                    anomaly_maps.append(anomaly_map.cpu().numpy())\n",
    "                    # print('anomaly_map.shape:', anomaly_map[0].shape)\n",
    "                    # print(len(anomaly_maps))\n",
    "                    # print('anomaly_map.shape:', anomaly_maps[0].shape)\n",
    "                # print('anomaly_maps.shape:', len(anomaly_maps))\n",
    "                \n",
    "                # print('anomaly_maps[0].shape:', anomaly_maps[0].shape)\n",
    "                score_map_zero = np.sum(anomaly_maps, axis=0)\n",
    "                # print('score_map_zero.shape:', score_map_zero.shape)\n",
    "                # print('score_map_zero.shape:', score_map_zero.shape)\n",
    "                seg_score_map_zero.append(score_map_zero)\n",
    "                # print(len(seg_score_map_zero))\n",
    "                # print('seg_score_map_zero.shape:', seg_score_map_zero[0].shape)\n",
    "                # \n",
    "\n",
    "\n",
    "            else:\n",
    "                # few-shot, det head\n",
    "                anomaly_maps_few_shot = []\n",
    "                for idx, p in enumerate(seg_patch_tokens):\n",
    "                    batch_cos_sim = []\n",
    "                    for b in range(p.shape[0]):\n",
    "                        cos = cos_sim(seg_mem_features[idx][b], p[b])\n",
    "                        # print(\"cos_shape\",cos.shape)\n",
    "                        height = int(np.sqrt(cos.shape[1]))\n",
    "                        # * 提取cls——token\n",
    "                        anomaly_map_few_shot = torch.min((1 - cos), 0)[0].reshape(1, 1, height, height)\n",
    "                        # print(\"anoma\",anomaly_map_few_shot.shape)\n",
    "                        anomaly_map_few_shot = F.interpolate(torch.tensor(anomaly_map_few_shot),\n",
    "                                                            size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                        \n",
    "                        # print(\"anoma\",anomaly_map_few_shot[0].shape)\n",
    "                        #* 去除多余维度\n",
    "                        batch_cos_sim.append(anomaly_map_few_shot[0].cpu().numpy())\n",
    "                    #     print('batch_cos_sim.shape:', batch_cos_sim[0].shape)\n",
    "                    # print(\"len\",len(batch_cos_sim))\n",
    "                    # print(\"shape_batch\",batch_cos_sim[0].shape)\n",
    "                    anomaly_maps_few_shot.append(np.stack(batch_cos_sim, axis=0))\n",
    "                #     print('anomaly_maps_few_shot.shape:', len(anomaly_maps_few_shot))\n",
    "                # print(\"shape anomaly\",len(anomaly_maps_few_shot))\n",
    "                # print(\"shapt\", anomaly_maps_few_shot[0].shape)\n",
    "                \n",
    "                # anomaly_map_few_shot 4,4,1,244,244 各特征层求和\n",
    "                anomaly_map_few_shot = np.sum(anomaly_maps_few_shot, axis=0)\n",
    "\n",
    "                # anomaly_map_few_shot 4,1,244,244\n",
    "                # print(\"shape anomaly\",len(anomaly_map_few_shot))\n",
    "                # print(\"shapt\", anomaly_map_few_shot.shape)\n",
    "                \n",
    "                # \n",
    "                score_few_det = anomaly_map_few_shot.mean(axis=(1, 2,3))\n",
    "                # print('score_few_det.shape:', score_few_det.shape)\n",
    "                det_image_scores_few.append(score_few_det)\n",
    "                # print(len(det_image_scores_few))\n",
    "\n",
    "                # zero-shot, det head\n",
    "                anomaly_score = 0\n",
    "                for layer in range(len(det_patch_tokens)):\n",
    "                    det_patch_tokens[layer] /= det_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "                    anomaly_map = (100.0 * det_patch_tokens[layer] @ text_features)\n",
    "                    anomaly_map = torch.softmax(anomaly_map, dim=-1)[:, :, 1]\n",
    "                    # print(\"shap\",anomaly_map.shape)\n",
    "                    anomaly_score += anomaly_map.mean(dim=1)\n",
    "                    # print(\"shapt\",anomaly_map.mean(dim=1))\n",
    "                    \n",
    "                det_image_scores_zero.append(anomaly_score.cpu().numpy())\n",
    "\n",
    "            # 使用tensor将mask添加到gt_mask_list中\n",
    "            gt_mask_list.append(mask.cpu().detach().numpy())\n",
    "            gt_list.extend(y.cpu().detach().numpy())\n",
    "\n",
    "            \n",
    "    # 问题不在于gt_mask_list的维度，在于seg_score_map_zero的维度被缩减了\n",
    "    gt_list = np.array(gt_list)\n",
    "    # gt-list (932,)\n",
    "    \n",
    "    # print(\"len(gt_mask_list):\", len(gt_mask_list))\n",
    "    # print(\"shape(gt_mask_list):\", gt_mask_list[-1].shape)\n",
    "\n",
    "    # print(\"shape(gt_mask_list[-1]):\", gt_mask_list[0])\n",
    "    # print(\"shape(gt_mask_list[-1]):\", gt_mask_list[-1])\n",
    "    # print(gt_mask_list[-2])\n",
    "    # print(gt_mask_list[-1])\n",
    "    #! 最后只有一个时，维度会被压缩\n",
    "    # asarray batch_size设置使得最后的对象与前面的大小不统一会报错\n",
    "    gt_mask_list = [\n",
    "        gt_mask_list[j][i] if len(gt_mask_list[j].shape) > 2 else gt_mask_list[j]\n",
    "            for j in range(len(gt_mask_list))        # 先遍历元素索引 j\n",
    "            for i in range(gt_mask_list[j].shape[0])  # 再遍历元素内部的样本索引 i\n",
    "            ]\n",
    "    # print('gt_mask_list shape:', len(gt_mask_list))\n",
    "    # print('gt_mask_list[0].shape:', gt_mask_list[-2].shape)\n",
    "    # print('gt_mask_list[-1].shape:', gt_mask_list[-1].shape)\n",
    "\n",
    "    gt_mask_list = np.asarray(gt_mask_list)\n",
    "    gt_mask_list = (gt_mask_list>0).astype(np.int_)\n",
    "    # print('gt_mask_list shape:', gt_mask_list.shape)\n",
    "    \n",
    "    # gt_mask_list = gt_mask_list[:len_gt_mask_list]\n",
    "    # gt_mask_list.shape image_nums,batch_size,224,224\n",
    "\n",
    "    if CLASS_INDEX[args.obj] > 0:\n",
    "        print(\"seg_score_map_zero shape:\", len(seg_score_map_zero))\n",
    "        print('seg_score_map_zero[0].shape:', seg_score_map_zero[0].shape)\n",
    "        seg_score_map_zero = [seg_score_map_zero[j][i] if len(seg_score_map_zero[j].shape) > 2 else seg_score_map_zero[j]\n",
    "            for j in range(len(seg_score_map_zero))        # 先遍历元素索引 j\n",
    "            for i in range(seg_score_map_zero[j].shape[0])  # 再遍历元素内部的样本索引 i\n",
    "            ]\n",
    "        seg_score_map_zero = np.array(seg_score_map_zero)\n",
    "        print('seg_score_map_zero shape:', seg_score_map_zero.shape)\n",
    "        \n",
    "        seg_score_map_few = [seg_score_map_few[j][i] if len(seg_score_map_few[j].shape) > 2 else seg_score_map_few[j]\n",
    "            for j in range(len(seg_score_map_few))        # 先遍历元素索引 j\n",
    "            for i in range(seg_score_map_few[j].shape[0])  # 再遍历元素内部的样本索引 i\n",
    "            ]\n",
    "        seg_score_map_few = np.array(seg_score_map_few)\n",
    "\n",
    "        seg_score_map_zero = (seg_score_map_zero - seg_score_map_zero.min()) / (seg_score_map_zero.max() - seg_score_map_zero.min())\n",
    "        seg_score_map_few = (seg_score_map_few - seg_score_map_few.min()) / (seg_score_map_few.max() - seg_score_map_few.min())\n",
    "        segment_scores = 0.5 * seg_score_map_zero + 0.5 * seg_score_map_few\n",
    "\n",
    "        seg_roc_auc = roc_auc_score(gt_mask_list.flatten(), segment_scores.flatten())\n",
    "        print(f'{args.obj} pAUC : {round(seg_roc_auc,4)}')\n",
    "\n",
    "        # segment_scores size (238, 4, 1, 224, 224)\n",
    "        segment_scores_flatten = segment_scores.reshape(segment_scores.shape[0] * segment_scores.shape[1], -1)\n",
    "        # return segment_scores_flatten,gt_list\n",
    "    \n",
    "        roc_auc_im = roc_auc_score(gt_list, np.max(segment_scores_flatten, axis=1))\n",
    "        print(f'{args.obj} AUC : {round(roc_auc_im, 4)}')\n",
    "\n",
    "        return seg_roc_auc + roc_auc_im\n",
    "\n",
    "    else:\n",
    "        # * 多batch展平\n",
    "        print(len(gt_list))\n",
    "        print('det_image_scores_zero shape:', len(det_image_scores_zero))\n",
    "        print(\"shape(det_image_scores_zero):\", det_image_scores_zero[0].shape)\n",
    "        print(\"shape(det_image_scores_zero):\", det_image_scores_zero[-1].shape)\n",
    "        print('det_image_scores_few shape:', len(det_image_scores_few))\n",
    "        print(\"shape(det_image_scores_few):\", det_image_scores_few[0].shape)\n",
    "        print(\"shape(det_image_scores_few):\", det_image_scores_few[-1].shape)\n",
    "        # det_image_scores_zero = [det_image_scores_zero[j][i] if len(det_image_scores_zero[j].shape) > 2 else det_image_scores_zero[j]\n",
    "        #     for j in range(len(det_image_scores_zero))        # 先遍历元素索引 j\n",
    "        #     for i in range(det_image_scores_zero[j].shape[0])  # 再遍历元素内部的样本索引 i\n",
    "        #     ]\n",
    "        # det_image_scores_few = [det_image_scores_few[j][i] if len(det_image_scores_few[j].shape) > 2 else det_image_scores_few[j]\n",
    "        #     for j in range(len(det_image_scores_few))        # 先遍历元素索引 j\n",
    "        #     for i in range(det_image_scores_few[j].shape[0])  # 再遍历元素内部的样本索引 i\n",
    "        #     ]\n",
    "        det_image_scores_zero = np.concatenate(det_image_scores_zero)\n",
    "        det_image_scores_few = np.concatenate(det_image_scores_few)\n",
    "\n",
    "        det_image_scores_zero = np.array(det_image_scores_zero)\n",
    "        det_image_scores_few = np.array(det_image_scores_few)\n",
    "        print(det_image_scores_few.shape)\n",
    "        print(det_image_scores_zero.shape)\n",
    "        \n",
    "        det_image_scores_zero = (det_image_scores_zero - det_image_scores_zero.min()) / (det_image_scores_zero.max() - det_image_scores_zero.min())\n",
    "        det_image_scores_few = (det_image_scores_few - det_image_scores_few.min()) / (det_image_scores_few.max() - det_image_scores_few.min())\n",
    "    \n",
    "        image_scores = 0.5 * det_image_scores_zero + 0.5 * det_image_scores_few\n",
    "        print(gt_list)\n",
    "        print(\">>\",image_scores)\n",
    "        img_roc_auc_det = roc_auc_score(gt_list, image_scores)\n",
    "        print(f'{args.obj} AUC : {round(img_roc_auc_det,4)}')\n",
    "\n",
    "        return img_roc_auc_det\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Testing')\n",
    "parser.add_argument('--model_name', type=str, default='biomedclip_local',)\n",
    "parser.add_argument('--pretrain', type=str, default='CLIP/ckpt/open_clip_pytorch_model.bin')\n",
    "parser.add_argument('--obj', type=str, default='Liver')\n",
    "parser.add_argument('--data_path', type=str, default='/root/data/')\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "parser.add_argument('--save_model', type=int, default=1)\n",
    "parser.add_argument('--save_path', type=str, default='./ckpt/few-shot/')\n",
    "parser.add_argument('--img_size', type=int, default=224)\n",
    "parser.add_argument(\"--epoch\", type=int, default=50, help=\"epochs\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.001, help=\"learning rate\")\n",
    "parser.add_argument(\"--features_list\", type=int, nargs=\"+\", default=[3,6,9,12], help=\"features used\")\n",
    "parser.add_argument('--seed', type=int, default=111)\n",
    "parser.add_argument('--shot', type=int, default=4)\n",
    "parser.add_argument('--iterate', type=int, default=0)\n",
    "args = parser.parse_args(args=['--obj', 'Liver',  '--shot', '4', '--batch_size', '1','--data_path','../MVFA-AD/data/'])\n",
    "\n",
    "setup_seed(args.seed)\n",
    "\n",
    "# fixed feature extractor\n",
    "biomedclip_model,tokenizer = create_model(model_name=args.model_name, \n",
    "                            force_image_size=args.img_size, \n",
    "                            device=device, \n",
    "                            pretrained=args.pretrain, \n",
    "                            require_pretrained=True)\n",
    "\n",
    "biomedclip_model.eval()\n",
    "\n",
    "# 模型添加适配器\n",
    "model = CLIP_Inplanted(clip_model=biomedclip_model, features=args.features_list).to(device)\n",
    "model.eval()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# optimizer for only adapters\n",
    "seg_optimizer = torch.optim.Adam(list(model.seg_adapters.parameters()), lr=args.learning_rate, betas=(0.5, 0.999))\n",
    "det_optimizer = torch.optim.Adam(list(model.det_adapters.parameters()), lr=args.learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# losses\n",
    "loss_focal = FocalLoss()\n",
    "loss_dice = BinaryDiceLoss()\n",
    "loss_bce = torch.nn.BCEWithLogitsLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 4\n",
    "args.shot = 4\n",
    "args.obj = 'Retina_OCT2017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text prompt\n",
    "with torch.cuda.amp.autocast(), torch.no_grad():\n",
    "    text_features = encode_text_with_prompt_ensemble(biomedclip_model, tokenizer, REAL_NAME[args.obj], device)\n",
    "\n",
    "\n",
    "best_result = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "kwargs = {'num_workers': 12, 'pin_memory': True} if use_cuda else {}\n",
    "test_dataset = MedDataset(args.data_path, args.obj, args.img_size, args.shot, args.iterate)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "# few-shot image augmentation\n",
    "augment_abnorm_img, augment_abnorm_mask = augment(test_dataset.fewshot_abnorm_img, test_dataset.fewshot_abnorm_mask)\n",
    "augment_normal_img, augment_normal_mask = augment(test_dataset.fewshot_norm_img)\n",
    "\n",
    "augment_fewshot_img = torch.cat([augment_abnorm_img, augment_normal_img], dim=0)\n",
    "augment_fewshot_mask = torch.cat([augment_abnorm_mask, augment_normal_mask], dim=0)\n",
    "\n",
    "augment_fewshot_label = torch.cat([torch.Tensor([1] * len(augment_abnorm_img)), torch.Tensor([0] * len(augment_normal_img))], dim=0)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(augment_fewshot_img, augment_fewshot_mask, augment_fewshot_label)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "# memory bank construction\n",
    "support_dataset = torch.utils.data.TensorDataset(augment_normal_img)\n",
    "support_loader = torch.utils.data.DataLoader(support_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 242/242 [02:52<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "968\n",
      "det_image_scores_zero shape: 242\n",
      "shape(det_image_scores_zero): (4,)\n",
      "shape(det_image_scores_zero): (4,)\n",
      "det_image_scores_few shape: 242\n",
      "shape(det_image_scores_few): (4,)\n",
      "shape(det_image_scores_few): (4,)\n",
      "(968,)\n",
      "(968,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1]\n",
      ">> [0.11812162 0.20753269 0.4633957  0.15807214 0.10103099 0.21533456\n",
      " 0.04447785 0.21097502 0.1155255  0.19870594 0.01131221 0.10703978\n",
      " 0.11208647 0.22538716 0.00514285 0.09547429 0.42302603 0.26580003\n",
      " 0.10783558 0.16693676 0.08908322 0.20055026 0.12594792 0.2934038\n",
      " 0.09313764 0.21527162 0.09704702 0.0978728  0.09657474 0.22530876\n",
      " 0.10548061 0.11571145 0.08811717 0.2161437  0.05874146 0.05963425\n",
      " 0.30149224 0.20087822 0.02952678 0.10690951 0.23001115 0.2129067\n",
      " 0.1762291  0.09868243 0.07367589 0.22398977 0.08242343 0.10572522\n",
      " 0.10250825 0.19049805 0.07111359 0.1956257  0.13307942 0.20173556\n",
      " 0.09552021 0.1346532  0.08614913 0.21806185 0.08844145 0.0842436\n",
      " 0.15278712 0.2356072  0.07740536 0.19959301 0.09041931 0.195612\n",
      " 0.1726959  0.08935077 0.10691924 0.20889199 0.12142581 0.11707713\n",
      " 0.27672094 0.24138704 0.07377253 0.49063256 0.10139539 0.21525934\n",
      " 0.04020066 0.11392611 0.23503584 0.19360538 0.05493561 0.16328488\n",
      " 0.11338758 0.21335754 0.06744348 0.11949906 0.12051944 0.20177893\n",
      " 0.03117555 0.11944573 0.08499958 0.20794307 0.13199747 0.14339615\n",
      " 0.1568657  0.21207288 0.02458913 0.11288668 0.12227467 0.22530591\n",
      " 0.08681106 0.2875528  0.07792127 0.21253014 0.09503427 0.1751245\n",
      " 0.17203508 0.25568783 0.10540822 0.07610609 0.10053193 0.24458742\n",
      " 0.05356817 0.14544955 0.14168704 0.25404117 0.05862626 0.10059088\n",
      " 0.10740723 0.24203517 0.16811909 0.08356808 0.15004946 0.24569413\n",
      " 0.07151236 0.09212027 0.12316392 0.24557696 0.10518024 0.12343954\n",
      " 0.40031004 0.18595627 0.12244585 0.08990341 0.09082326 0.20201829\n",
      " 0.09808799 0.07929925 0.10795224 0.24717088 0.04661001 0.11145399\n",
      " 0.1198241  0.22299913 0.05199606 0.115035   0.23954149 0.19655278\n",
      " 0.0726622  0.20542294 0.10918459 0.20173596 0.08312424 0.13439427\n",
      " 0.10209305 0.21769217 0.05566783 0.11260692 0.08834577 0.21988055\n",
      " 0.10313042 0.10469259 0.05979893 0.23931593 0.07896436 0.12465542\n",
      " 0.08830742 0.19263177 0.03422546 0.18599363 0.07221969 0.21073058\n",
      " 0.07954531 0.12388629 0.35688812 0.21166648 0.02339081 0.12527941\n",
      " 0.08355609 0.23764022 0.12449992 0.09322833 0.08166476 0.21039997\n",
      " 0.04236685 0.09985049 0.09480316 0.24818973 0.10234255 0.11381617\n",
      " 0.10620251 0.20417674 0.10223098 0.11547852 0.07539728 0.20996626\n",
      " 0.12227239 0.2870201  0.08471624 0.21894088 0.06743987 0.16032262\n",
      " 0.08855667 0.23734875 0.04097974 0.09587191 0.07399769 0.2073753\n",
      " 0.0697244  0.06486823 0.37399825 0.20415398 0.05264411 0.14666906\n",
      " 0.11472034 0.2887212  0.07666766 0.15583792 0.16022103 0.19426024\n",
      " 0.10203661 0.07215109 0.15042976 0.1964534  0.05418316 0.22207482\n",
      " 0.08273633 0.1967503  0.04104912 0.09012714 0.25034317 0.24754259\n",
      " 0.11633132 0.14622903 0.08754981 0.19385976 0.05659636 0.06872592\n",
      " 0.11686814 0.27107033 0.8778535  0.9321453  0.9390837  0.7620867\n",
      " 0.82266504 0.8329158  0.81165504 0.7874423  0.84808326 0.9018457\n",
      " 0.83542395 0.7869688  0.92742336 0.90016747 0.87943804 0.8245697\n",
      " 0.84540534 0.8776702  0.84204566 0.8047724  0.87591314 0.9274713\n",
      " 0.7956109  0.8332464  0.93514633 0.9136869  0.93645173 0.8233802\n",
      " 0.83821213 0.9001527  0.88471794 0.71721864 0.9386698  0.8850126\n",
      " 0.8691292  0.8423171  0.90980333 0.81069744 0.810532   0.83532345\n",
      " 0.91722393 0.890545   0.8446104  0.83362    0.8746121  0.8937273\n",
      " 0.87669104 0.9111328  0.89840674 0.8749112  0.8939009  0.8034649\n",
      " 0.79030037 0.89224803 0.8012418  0.78765893 0.88118213 0.8729502\n",
      " 0.83531034 0.81148887 0.84513396 0.89025027 0.85893273 0.7740197\n",
      " 0.87575895 0.92813045 0.8805512  0.8133282  0.8468919  0.8707522\n",
      " 0.8475614  0.8002875  0.8569683  0.89563036 0.8653177  0.8448669\n",
      " 0.8837502  0.94790494 0.788995   0.8270807  0.9269719  0.83213615\n",
      " 0.8871906  0.78446686 0.89221025 0.86407423 0.7942192  0.79317117\n",
      " 0.8339138  0.9192107  0.8381487  0.79521364 0.8540194  0.842384\n",
      " 0.83891356 0.7777053  0.8820096  0.89200115 0.8653188  0.8098024\n",
      " 0.9060246  0.88225603 0.84327114 0.85087883 0.8809123  0.89248955\n",
      " 0.8718747  0.7785207  0.8624797  0.88062674 0.8406184  0.8112842\n",
      " 0.867266   0.8916663  0.82214147 0.7244148  0.9490752  0.9016966\n",
      " 0.84104276 0.79754925 0.94268    0.9044237  0.85980755 0.8737088\n",
      " 0.8700392  0.8205787  0.85455024 0.759198   0.8234745  0.8770822\n",
      " 0.88363135 0.7827105  0.9116007  0.8879974  0.8410816  0.85392624\n",
      " 0.9547706  0.9280492  0.9241108  0.7961272  0.8466393  0.84755933\n",
      " 0.8786075  0.76523256 0.85396045 0.941206   0.862689   0.82595384\n",
      " 0.91502464 0.85903716 0.8405826  0.7640884  0.8890827  0.8805307\n",
      " 0.912958   0.80398935 0.8429679  0.82474136 0.8720393  0.7563103\n",
      " 0.8292589  0.9244631  0.86168283 0.8642909  0.9433745  0.8757775\n",
      " 0.86842585 0.76996076 0.8191452  0.8929787  0.87055695 0.8104874\n",
      " 0.9098841  0.89018804 0.8443868  0.83245146 0.8367284  0.8733324\n",
      " 0.9032811  0.8055899  0.9366199  0.95761263 0.9592765  0.8401451\n",
      " 0.94223773 0.9428648  0.8726145  0.80507535 0.8958851  0.88140285\n",
      " 0.9297489  0.8185251  0.8734674  0.88677835 0.8525543  0.8358344\n",
      " 0.88083005 0.96247846 0.827421   0.749176   0.9321343  0.8771425\n",
      " 0.8204764  0.83753544 0.85846245 0.9613661  0.93601906 0.85491204\n",
      " 0.89978695 0.7415853  0.82067674 0.7958808  0.8695587  0.8992693\n",
      " 0.93747485 0.83045363 0.8182154  0.8731712  0.8239554  0.77898085\n",
      " 0.8662716  0.89516866 0.7953406  0.68242824 0.8460356  0.87681985\n",
      " 0.8986825  0.828809   0.8177387  0.90008855 0.87694037 0.80760616\n",
      " 0.8862424  0.93716174 0.85997695 0.78440785 0.8919315  0.90871954\n",
      " 0.8535522  0.88646424 0.93542916 0.8708736  0.8444487  0.8258001\n",
      " 0.89425945 0.8905964  0.8763226  0.64420986 0.726212   0.8635709\n",
      " 0.8117342  0.8032682  0.7593472  0.86809504 0.8416039  0.8164847\n",
      " 0.69105875 0.83484364 0.20132652 0.84875655 0.7613379  0.87365615\n",
      " 0.8484029  0.8157229  0.8692852  0.87708616 0.8742118  0.76773655\n",
      " 0.8704233  0.8284456  0.84605324 0.7421934  0.8627598  0.9138422\n",
      " 0.7892415  0.7823396  0.7845626  0.97803783 0.76678413 0.61512864\n",
      " 0.804168   0.8363303  0.8433852  0.8037292  0.8535277  0.88847244\n",
      " 0.7611489  0.7800203  0.68314993 0.8872627  0.8693255  0.802103\n",
      " 0.8535495  0.83214366 0.8219197  0.7329898  0.72134715 0.19890065\n",
      " 0.8514577  0.81408346 0.8617629  0.81947994 0.8470398  0.7743802\n",
      " 0.8242985  0.84544456 0.7614185  0.81688553 0.9096372  0.9066244\n",
      " 0.8810745  0.7084663  0.8306208  0.83173573 0.74145514 0.785162\n",
      " 0.83641374 0.85865265 0.8919986  0.81199    0.7462095  0.9548673\n",
      " 0.8701221  0.84181005 0.8310696  0.8762485  0.8565815  0.49266422\n",
      " 0.86534154 0.93874145 0.7945682  0.8555006  0.82897246 0.8086922\n",
      " 0.910917   0.70277    0.6945629  0.8058984  0.81829834 0.25435132\n",
      " 0.8497424  0.86748266 0.8718075  0.89251935 0.35738796 0.874698\n",
      " 0.81399107 0.7440077  0.82906115 0.8228587  0.8119813  0.7858819\n",
      " 0.8354076  0.79945403 0.87450176 0.8305212  0.3842966  0.8211266\n",
      " 0.7379279  0.77589333 0.8670619  0.88629377 0.7494055  0.74405384\n",
      " 0.86815083 0.8889654  0.88365054 0.6608108  0.7644625  0.9117098\n",
      " 0.8683293  0.7788893  0.72021437 0.9125974  0.8598168  0.8267609\n",
      " 0.8635306  0.92849666 0.777012   0.7779162  0.8743462  0.6822653\n",
      " 0.7855004  0.81777465 0.8550533  0.88622874 0.82434964 0.7811619\n",
      " 0.82497597 0.736078   0.76949894 0.7890812  0.93666613 0.96271306\n",
      " 0.8776269  0.76152176 0.77478397 0.61276245 0.8570723  0.70043224\n",
      " 0.5361105  0.65005744 0.83250654 0.696825   0.8505473  0.82775915\n",
      " 0.80192924 0.6775415  0.8802587  0.8160428  0.8300796  0.7620971\n",
      " 0.77816486 0.81710696 0.7287648  0.7469901  0.7572257  0.79319525\n",
      " 0.7632253  0.79780614 0.87510777 0.6155497  0.6684912  0.85075045\n",
      " 0.77909327 0.12385825 0.8467099  0.8468394  0.82649505 0.7278814\n",
      " 0.8255495  0.7172423  0.64743596 0.80140257 0.7653938  0.725481\n",
      " 0.7972013  0.7029203  0.68978035 0.54970276 0.80923307 0.87904227\n",
      " 0.5204875  0.79065377 0.8699688  0.9033612  0.8521956  0.80671537\n",
      " 0.80863    0.8257679  0.7980463  0.6954725  0.73753273 0.7622191\n",
      " 0.7812273  0.76449454 0.7646272  0.8708139  0.845883   0.7768359\n",
      " 0.8376772  0.82965285 0.8283985  0.73362166 0.80508184 0.8401542\n",
      " 0.7622998  0.7277192  0.8282376  0.8803654  0.90077007 0.7244493\n",
      " 0.8802419  0.94466233 0.9355693  0.7974769  0.7156066  0.7813561\n",
      " 0.8505641  0.76046735 0.5971324  0.85518456 0.816967   0.8719208\n",
      " 0.87272    0.9255545  0.8627902  0.7813691  0.8276544  0.766527\n",
      " 0.78402376 0.7751825  0.72329974 0.86491466 0.8602582  0.75500214\n",
      " 0.79243004 0.85214525 0.7932085  0.77825147 0.8128985  0.86200196\n",
      " 0.8575156  0.78793055 0.8112663  0.84336805 0.8332852  0.7285535\n",
      " 0.80186    0.8378777  0.76826656 0.7532916  0.8348074  0.8557888\n",
      " 0.84706956 0.7462621  0.73136955 0.8770424  0.77742314 0.8092882\n",
      " 0.8469783  0.9090949  0.7909236  0.7288425  0.8503248  0.8937417\n",
      " 0.801175   0.7404064  0.7928399  0.8325784  0.8650504  0.8113719\n",
      " 0.78611004 0.81943345 0.77076834 0.7586604  0.87642217 0.9048766\n",
      " 0.826765   0.76041776 0.8636328  0.84513295 0.7915534  0.7414409\n",
      " 0.82924974 0.8487352  0.81525624 0.7819159  0.8720804  0.84207374\n",
      " 0.8321779  0.8380101  0.909893   0.81034327 0.66775626 0.730596\n",
      " 0.8821726  0.87983584 0.85098076 0.82359797 0.8377411  0.8490765\n",
      " 0.79706454 0.77417815 0.84884644 0.85449773 0.7974603  0.7590859\n",
      " 0.8524531  0.84979093 0.79100555 0.79946524 0.85680366 0.86029357\n",
      " 0.8461672  0.79848087 0.85672045 0.936743   0.861241   0.81729233\n",
      " 0.8527736  0.8384509  0.8446145  0.7986963  0.8994404  0.90986055\n",
      " 0.89296484 0.8413732  0.9028945  0.87593675 0.8320751  0.7638403\n",
      " 0.8328355  0.94460297 0.73468876 0.76231694 0.7842243  0.8877642\n",
      " 0.8616945  0.8564788  0.93256533 0.85131073 0.8355249  0.77316177\n",
      " 0.7986411  0.89065534 0.89903283 0.76541066 0.8339132  0.8845395\n",
      " 0.7982267  0.78464377 0.7972857  0.8574581  0.84982485 0.81678665\n",
      " 0.8092278  0.89745533 0.8231559  0.7447475  0.81211144 0.817893\n",
      " 0.8210453  0.73776126 0.79562354 0.8535951  0.8002501  0.742537\n",
      " 0.87203497 0.8584063  0.80970526 0.75506735 0.7789844  0.87117565\n",
      " 0.84313834 0.74441355 0.76349896 0.91975784 0.82590646 0.7952117\n",
      " 0.93110406 0.91725135 0.8121648  0.7879168  0.83168834 0.83085907\n",
      " 0.8426489  0.74200594 0.8420731  0.8140626  0.8620715  0.72128886\n",
      " 0.87881064 0.9081177  0.7602089  0.7581116  0.7975297  0.8989958\n",
      " 0.8293664  0.7488623  0.7157304  0.442384   0.76914895 0.75417554\n",
      " 0.6554431  0.8021767  0.7911465  0.7334206  0.8403009  0.9429145\n",
      " 0.29151753 0.58249426 0.8030652  0.8021203  0.80398047 0.7739202\n",
      " 0.77968615 0.8629335  0.8124973  0.7494395  0.809136   0.90261614\n",
      " 0.7739037  0.74053097 0.837169   0.8646952  0.8469217  0.7565552\n",
      " 0.8028953  0.8648497  0.7006984  0.7681592  0.8141134  0.88150704\n",
      " 0.7955802  0.7577789  0.90792036 0.8939065  0.80333483 0.79162115\n",
      " 0.8122217  0.8334757  0.8928273  0.76366025 0.7933893  0.86450875\n",
      " 0.8259834  0.76411057 0.7751435  0.83272505 0.75639975 0.66363025\n",
      " 0.8385718  0.8367195  0.8088976  0.77885103 0.84525275 0.44727963\n",
      " 0.7882807  0.7446601  0.78758657 0.91133124 0.8405449  0.8285942\n",
      " 0.78368115 0.8586719 ]\n",
      "Retina_OCT2017 AUC : 0.9984\n",
      "Best result\n",
      "\n",
      "epoch  1 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 66/242 [00:41<01:51,  1.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m det_mem_features \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mcat([det_features[j][i]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,det_features[j][i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m],det_features[j][i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(det_features))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(det_features[\u001b[38;5;241m0\u001b[39m]))]\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# seg_mem_features size =>  4, (image_nums * 197, embed_size) \u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_mem_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdet_mem_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m>\u001b[39m best_result:\n\u001b[1;32m     81\u001b[0m     best_result \u001b[38;5;241m=\u001b[39m result\n",
      "Cell \u001b[0;32mIn[83], line 94\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(args, model, test_loader, text_features, seg_mem_features, det_mem_features)\u001b[0m\n\u001b[1;32m     89\u001b[0m     anomaly_map_few_shot \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(torch\u001b[38;5;241m.\u001b[39mtensor(anomaly_map_few_shot),\n\u001b[1;32m     90\u001b[0m                                         size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mimg_size, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# print(\"anoma\",anomaly_map_few_shot[0].shape)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m#* 去除多余维度\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     batch_cos_sim\u001b[38;5;241m.\u001b[39mappend(\u001b[43manomaly_map_few_shot\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m#     print('batch_cos_sim.shape:', batch_cos_sim[0].shape)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# print(\"len\",len(batch_cos_sim))\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# print(\"shape_batch\",batch_cos_sim[0].shape)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m anomaly_maps_few_shot\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mstack(batch_cos_sim, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epoch):\n",
    "    print('epoch ', epoch, ':')\n",
    "\n",
    "    # loss_list = []\n",
    "    # for (image, gt, label) in train_loader:\n",
    "    #     image = image.to(device)\n",
    "    #     with torch.cuda.amp.autocast():\n",
    "    #         _, seg_patch_tokens, det_patch_tokens = model(image)\n",
    "    #         # seg_patch_tokens size { [batch_size,196,512] * 4} \n",
    "    #         seg_patch_tokens = [p[:, 1:, :] for p in seg_patch_tokens]\n",
    "    #         det_patch_tokens = [p[:, 1:, :] for p in det_patch_tokens]\n",
    "\n",
    "    #         # det loss\n",
    "    #         det_loss = 0\n",
    "    #         image_label = label.to(device)\n",
    "    #         for layer in range(len(det_patch_tokens)):\n",
    "    #             det_patch_tokens[layer] = det_patch_tokens[layer] / det_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "    #             anomaly_map = (100.0 * det_patch_tokens[layer] @ text_features)   \n",
    "    #             anomaly_map = torch.softmax(anomaly_map, dim=-1)[:, :, 1]\n",
    "    #             anomaly_score = torch.mean(anomaly_map, dim=-1)\n",
    "    #             det_loss += loss_bce(anomaly_score, image_label)\n",
    "\n",
    "    #         if CLASS_INDEX[args.obj] > 0:\n",
    "    #             # pixel level\n",
    "    #             seg_loss = 0\n",
    "    #             mask = gt.squeeze(0).to(device)\n",
    "    #             mask[mask > 0.5], mask[mask <= 0.5] = 1, 0\n",
    "    #             for layer in range(len(seg_patch_tokens)):\n",
    "    #                 seg_patch_tokens[layer] = seg_patch_tokens[layer] / seg_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "    #                 anomaly_map = (100.0 * seg_patch_tokens[layer] @ text_features)\n",
    "    #                 B, L, C = anomaly_map.shape\n",
    "    #                 H = int(np.sqrt(L))\n",
    "    #                 anomaly_map = F.interpolate(anomaly_map.permute(0, 2, 1).view(B, 2, H, H),\n",
    "    #                                             size=args.img_size, mode='bilinear', align_corners=True)\n",
    "    #                 anomaly_map = torch.softmax(anomaly_map, dim=1)\n",
    "    #                 seg_loss += loss_focal(anomaly_map, mask)\n",
    "    #                 seg_loss += loss_dice(anomaly_map[:, 1, :, :], mask)\n",
    "                \n",
    "    #             loss = seg_loss + det_loss\n",
    "    #             loss.requires_grad_(True)\n",
    "    #             seg_optimizer.zero_grad()\n",
    "    #             det_optimizer.zero_grad()\n",
    "    #             loss.backward()\n",
    "    #             seg_optimizer.step()\n",
    "    #             det_optimizer.step()\n",
    "\n",
    "    #         else:\n",
    "    #             loss = det_loss\n",
    "    #             loss.requires_grad_(True)\n",
    "    #             det_optimizer.zero_grad()\n",
    "    #             loss.backward()\n",
    "    #             det_optimizer.step()\n",
    "\n",
    "    #         loss_list.append(loss.item())\n",
    "\n",
    "    # print(\"Loss: \", np.mean(loss_list))\n",
    "\n",
    "\n",
    "    seg_features = []\n",
    "    det_features = []\n",
    "    for image in support_loader:\n",
    "        image = image[0].to(device)\n",
    "        with torch.no_grad():\n",
    "            _, seg_patch_tokens, det_patch_tokens = model(image)\n",
    "            #? seg_patch_tokens size { [batch_size,197,512] * 4}\n",
    "            \n",
    "            #! 0 -> : , 仅改变batch_size维度， 不会改变其他维度\n",
    "            seg_patch_tokens = [p.contiguous() for p in seg_patch_tokens]\n",
    "            det_patch_tokens = [p.contiguous() for p in det_patch_tokens]\n",
    "            seg_features.append(seg_patch_tokens)\n",
    "            det_features.append(det_patch_tokens)\n",
    "    # batch_size = 1时， seg_features  image_nums， 4 ， [197,embed_size]\n",
    "    # batch_size = 2时， seg_features  {image_nums * { 4 * [2 ,197,embed_size] }  }\n",
    "    #! batch_size > 1 时， seg_features 维度会缩减！\n",
    "    seg_mem_features = [torch.cat([seg_features[j][i].view(-1,seg_features[j][i].shape[-2],seg_features[j][i].shape[-1]) for j in range(len(seg_features))], dim=0) for i in range(len(seg_features[0]))]\n",
    "    det_mem_features = [torch.cat([det_features[j][i].view(-1,det_features[j][i].shape[-2],det_features[j][i].shape[-1]) for j in range(len(det_features))], dim=0) for i in range(len(det_features[0]))]\n",
    "    # seg_mem_features size =>  4, (image_nums * 197, embed_size) \n",
    "    \n",
    "    result = test(args, model, test_loader, text_features, seg_mem_features, det_mem_features)\n",
    "    if result > best_result:\n",
    "        best_result = result\n",
    "        print(\"Best result\\n\")\n",
    "        if args.save_model == 1:\n",
    "            ckp_path = os.path.join(args.save_path, f'{args.obj}.pth')\n",
    "            torch.save({'seg_adapters': model.seg_adapters.state_dict(),\n",
    "                        'det_adapters': model.det_adapters.state_dict()}, \n",
    "                        ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 703/968 [00:02<00:01, 197.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 728/968 [00:03<00:02, 83.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 746/968 [00:04<00:03, 62.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 760/968 [00:04<00:04, 51.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 771/968 [00:05<00:04, 45.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 773/968 [00:05<00:01, 149.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n",
      "shape anomaly 4\n",
      "shapt torch.Size([1, 224, 224])\n",
      "shape anomaly 1\n",
      "shapt (224, 224)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 245\u001b[0m\n\u001b[1;32m    241\u001b[0m seg_mem_features \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mcat([seg_features[j][i] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seg_features))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seg_features[\u001b[38;5;241m0\u001b[39m]))]\n\u001b[1;32m    242\u001b[0m det_mem_features \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mcat([det_features[j][i] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(det_features))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(det_features[\u001b[38;5;241m0\u001b[39m]))]\n\u001b[0;32m--> 245\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_mem_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdet_mem_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m>\u001b[39m best_result:\n\u001b[1;32m    247\u001b[0m     best_result \u001b[38;5;241m=\u001b[39m result\n",
      "Cell \u001b[0;32mIn[31], line 74\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(args, model, test_loader, text_features, seg_mem_features, det_mem_features)\u001b[0m\n\u001b[1;32m     72\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos_sim(det_mem_features[idx], p)\n\u001b[1;32m     73\u001b[0m height \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39msqrt(cos\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m---> 74\u001b[0m anomaly_map_few_shot \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, height, height)\n\u001b[1;32m     75\u001b[0m anomaly_map_few_shot \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(torch\u001b[38;5;241m.\u001b[39mtensor(anomaly_map_few_shot),\n\u001b[1;32m     76\u001b[0m                                         size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mimg_size, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     77\u001b[0m anomaly_maps_few_shot\u001b[38;5;241m.\u001b[39mappend(anomaly_map_few_shot[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 测试原本\n",
    "args.batch_size = 1\n",
    "args.shot = 4\n",
    "def test(args, model, test_loader, text_features, seg_mem_features, det_mem_features):\n",
    "    gt_list = []\n",
    "    gt_mask_list = []\n",
    "\n",
    "    det_image_scores_zero = []\n",
    "    det_image_scores_few = []\n",
    "    \n",
    "    seg_score_map_zero = []\n",
    "    seg_score_map_few= []\n",
    "\n",
    "    step = 0\n",
    "    for (image, y, mask) in tqdm(test_loader):\n",
    "        step += 1\n",
    "        if step < 700:\n",
    "            continue\n",
    "        \n",
    "        image = image.to(device)\n",
    "        mask[mask > 0.5], mask[mask <= 0.5] = 1, 0\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            _, seg_patch_tokens, det_patch_tokens = model(image)\n",
    "            seg_patch_tokens = [p[0, 1:, :] for p in seg_patch_tokens]\n",
    "            det_patch_tokens = [p[0, 1:, :] for p in det_patch_tokens]\n",
    "\n",
    "            if CLASS_INDEX[args.obj] > 0:\n",
    "\n",
    "                # few-shot, seg head\n",
    "                anomaly_maps_few_shot = []\n",
    "                for idx, p in enumerate(seg_patch_tokens):\n",
    "                    \n",
    "                    cos = cos_sim(seg_mem_features[idx], p)\n",
    "                    height = int(np.sqrt(cos.shape[1]))\n",
    "                    anomaly_map_few_shot = torch.min((1 - cos), 0)[0].reshape(1, 1, height, height)\n",
    "\n",
    "                    anomaly_map_few_shot = F.interpolate(torch.tensor(anomaly_map_few_shot),\n",
    "                                                            size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                    anomaly_maps_few_shot.append(anomaly_map_few_shot[0].cpu().numpy())\n",
    " \n",
    "                score_map_few = np.sum(anomaly_maps_few_shot, axis=0)\n",
    "                seg_score_map_few.append(score_map_few)\n",
    "\n",
    "                # zero-shot, seg head\n",
    "                anomaly_maps = []\n",
    "                for layer in range(len(seg_patch_tokens)):\n",
    "                    \n",
    "                    seg_patch_tokens[layer] /= seg_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "                    anomaly_map = (100.0 * seg_patch_tokens[layer] @ text_features).unsqueeze(0)\n",
    "        \n",
    "                    B, L, C = anomaly_map.shape\n",
    "                    H = int(np.sqrt(L))\n",
    "                    anomaly_map = F.interpolate(anomaly_map.permute(0, 2, 1).view(B, 2, H, H),\n",
    "                                                size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                    # print(\"augment_normal_imgs.shape:\", anomaly_map.shape)\n",
    "                    anomaly_map = torch.softmax(anomaly_map, dim=1)[:, 1, :, :]\n",
    "                    print(\"anomaly_map.shape:\", anomaly_map.shape)\n",
    "                    anomaly_maps.append(anomaly_map.cpu().numpy())\n",
    "                    print(len(anomaly_maps))\n",
    "                    print(\"anomaly_maps.shape:\", anomaly_maps[0].shape)\n",
    "                score_map_zero = np.sum(anomaly_maps, axis=0)\n",
    "                print(\"score_map_zero.shape:\", score_map_zero.shape)\n",
    "                seg_score_map_zero.append(score_map_zero)\n",
    "                \n",
    "\n",
    "\n",
    "            else:\n",
    "                # few-shot, det head\n",
    "                anomaly_maps_few_shot = []\n",
    "                for idx, p in enumerate(det_patch_tokens):\n",
    "                    cos = cos_sim(det_mem_features[idx], p)\n",
    "                    height = int(np.sqrt(cos.shape[1]))\n",
    "                    anomaly_map_few_shot = torch.min((1 - cos), 0)[0].reshape(1, 1, height, height)\n",
    "                    anomaly_map_few_shot = F.interpolate(torch.tensor(anomaly_map_few_shot),\n",
    "                                                            size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                    anomaly_maps_few_shot.append(anomaly_map_few_shot[0].cpu().numpy())\n",
    "                print(\"shape anomaly\",len(anomaly_maps_few_shot))\n",
    "                print(\"shapt\", anomaly_map_few_shot[0].shape)\n",
    "                \n",
    "                anomaly_map_few_shot = np.sum(anomaly_maps_few_shot, axis=0)\n",
    "\n",
    "                print(\"shape anomaly\",len(anomaly_map_few_shot))\n",
    "                print(\"shapt\", anomaly_map_few_shot[0].shape)\n",
    "                \n",
    "                # print(\"sahtp\")\n",
    "                # print(\"augment_normal_img len(anomaly_map_few_shot):\", len(anomaly_map_few_shot))\n",
    "                # print(\"anomaly_map_few_shot.shape:\", anomaly_map_few_shot.shape)\n",
    "                score_few_det = anomaly_map_few_shot.mean()\n",
    "                # print(\"ss\",score_few_det)\n",
    "                # print(\"score_few_det.shape:\", score_few_det.shape)\n",
    "                det_image_scores_few.append(score_few_det)\n",
    "\n",
    "                # zero-shot, det head\n",
    "                anomaly_score = 0\n",
    "                for layer in range(len(det_patch_tokens)):\n",
    "                    det_patch_tokens[layer] /= det_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "                    anomaly_map = (100.0 * det_patch_tokens[layer] @ text_features).unsqueeze(0)\n",
    "                    anomaly_map = torch.softmax(anomaly_map, dim=-1)[:, :, 1]\n",
    "                    anomaly_score += anomaly_map.mean()\n",
    "                det_image_scores_zero.append(anomaly_score.cpu().numpy())\n",
    "\n",
    "            \n",
    "            gt_mask_list.append(mask.squeeze().cpu().detach().numpy())\n",
    "            gt_list.extend(y.cpu().detach().numpy())\n",
    "            \n",
    "\n",
    "    gt_list = np.array(gt_list)\n",
    "    gt_mask_list = np.asarray(gt_mask_list)\n",
    "    gt_mask_list = (gt_mask_list>0).astype(np.int_)\n",
    "\n",
    "\n",
    "    if CLASS_INDEX[args.obj] > 0:\n",
    "\n",
    "        seg_score_map_zero = np.array(seg_score_map_zero)\n",
    "        seg_score_map_few = np.array(seg_score_map_few)\n",
    "\n",
    "        seg_score_map_zero = (seg_score_map_zero - seg_score_map_zero.min()) / (seg_score_map_zero.max() - seg_score_map_zero.min())\n",
    "        seg_score_map_few = (seg_score_map_few - seg_score_map_few.min()) / (seg_score_map_few.max() - seg_score_map_few.min())\n",
    "\n",
    "        print(\"seg_score_map_zero.shape:\", seg_score_map_zero.shape)\n",
    "        print('seg_score_map_few.shape:', seg_score_map_few.shape)\n",
    "        segment_scores = 0.5 * seg_score_map_zero + 0.5 * seg_score_map_few\n",
    "        seg_roc_auc = roc_auc_score(gt_mask_list.flatten(), segment_scores.flatten())\n",
    "        print(f'{args.obj} pAUC : {round(seg_roc_auc,4)}')\n",
    "\n",
    "        segment_scores_flatten = segment_scores.reshape(segment_scores.shape[0], -1)\n",
    "        roc_auc_im = roc_auc_score(gt_list, np.max(segment_scores_flatten, axis=1))\n",
    "        print(f'{args.obj} AUC : {round(roc_auc_im, 4)}')\n",
    "\n",
    "        return seg_roc_auc + roc_auc_im\n",
    "\n",
    "    else:\n",
    "\n",
    "        det_image_scores_zero = np.array(det_image_scores_zero)\n",
    "        det_image_scores_few = np.array(det_image_scores_few)\n",
    "\n",
    "        det_image_scores_zero = (det_image_scores_zero - det_image_scores_zero.min()) / (det_image_scores_zero.max() - det_image_scores_zero.min())\n",
    "        det_image_scores_few = (det_image_scores_few - det_image_scores_few.min()) / (det_image_scores_few.max() - det_image_scores_few.min())\n",
    "    \n",
    "        image_scores = 0.5 * det_image_scores_zero + 0.5 * det_image_scores_few\n",
    "        img_roc_auc_det = roc_auc_score(gt_list, image_scores)\n",
    "        print(f'{args.obj} AUC : {round(img_roc_auc_det,4)}')\n",
    "\n",
    "        return img_roc_auc_det\n",
    "    \n",
    "    # load test dataset\n",
    "kwargs = {'num_workers': 12, 'pin_memory': True} if use_cuda else {}\n",
    "test_dataset = MedDataset(args.data_path, args.obj, args.img_size, args.shot, args.iterate)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "# few-shot image augmentation\n",
    "augment_abnorm_img, augment_abnorm_mask = augment(test_dataset.fewshot_abnorm_img, test_dataset.fewshot_abnorm_mask)\n",
    "augment_normal_img, augment_normal_mask = augment(test_dataset.fewshot_norm_img)\n",
    "\n",
    "augment_fewshot_img = torch.cat([augment_abnorm_img, augment_normal_img], dim=0)\n",
    "augment_fewshot_mask = torch.cat([augment_abnorm_mask, augment_normal_mask], dim=0)\n",
    "\n",
    "augment_fewshot_label = torch.cat([torch.Tensor([1] * len(augment_abnorm_img)), torch.Tensor([0] * len(augment_normal_img))], dim=0)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(augment_fewshot_img, augment_fewshot_mask, augment_fewshot_label)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "# memory bank construction\n",
    "support_dataset = torch.utils.data.TensorDataset(augment_normal_img)\n",
    "support_loader = torch.utils.data.DataLoader(support_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "best_result = 0\n",
    "\n",
    "for epoch in range(args.epoch):\n",
    "        print('epoch ', epoch, ':')\n",
    "\n",
    "        # loss_list = []\n",
    "        # for (image, gt, label) in train_loader:\n",
    "        #     image = image.to(device)\n",
    "        #     with torch.cuda.amp.autocast():\n",
    "        #         _, seg_patch_tokens, det_patch_tokens = model(image)\n",
    "        #         # seg_patch_tokens size { [batch_size,196,512] * 4} \n",
    "        #         seg_patch_tokens = [p[0, 1:, :] for p in seg_patch_tokens]\n",
    "        #         det_patch_tokens = [p[0, 1:, :] for p in det_patch_tokens]\n",
    "\n",
    "        #         # det loss\n",
    "        #         det_loss = 0\n",
    "        #         image_label = label.to(device)\n",
    "        #         for layer in range(len(det_patch_tokens)):\n",
    "        #             det_patch_tokens[layer] = det_patch_tokens[layer] / det_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "        #             anomaly_map = (100.0 * det_patch_tokens[layer] @ text_features).unsqueeze(0)    \n",
    "        #             anomaly_map = torch.softmax(anomaly_map, dim=-1)[:, :, 1]\n",
    "        #             anomaly_score = torch.mean(anomaly_map, dim=-1)\n",
    "        #             det_loss += loss_bce(anomaly_score, image_label)\n",
    "\n",
    "        #         if CLASS_INDEX[args.obj] > 0:\n",
    "        #             # pixel level\n",
    "        #             seg_loss = 0\n",
    "        #             mask = gt.squeeze(0).to(device)\n",
    "        #             mask[mask > 0.5], mask[mask <= 0.5] = 1, 0\n",
    "        #             for layer in range(len(seg_patch_tokens)):\n",
    "        #                 seg_patch_tokens[layer] = seg_patch_tokens[layer] / seg_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "        #                 anomaly_map = (100.0 * seg_patch_tokens[layer] @ text_features).unsqueeze(0)\n",
    "        #                 B, L, C = anomaly_map.shape\n",
    "        #                 H = int(np.sqrt(L))\n",
    "        #                 anomaly_map = F.interpolate(anomaly_map.permute(0, 2, 1).view(B, 2, H, H),\n",
    "        #                                             size=args.img_size, mode='bilinear', align_corners=True)\n",
    "        #                 anomaly_map = torch.softmax(anomaly_map, dim=1)\n",
    "        #                 seg_loss += loss_focal(anomaly_map, mask)\n",
    "        #                 seg_loss += loss_dice(anomaly_map[:, 1, :, :], mask)\n",
    "                    \n",
    "        #             loss = seg_loss + det_loss\n",
    "        #             loss.requires_grad_(True)\n",
    "        #             seg_optimizer.zero_grad()\n",
    "        #             det_optimizer.zero_grad()\n",
    "        #             loss.backward()\n",
    "        #             seg_optimizer.step()\n",
    "        #             det_optimizer.step()\n",
    "\n",
    "        #         else:\n",
    "        #             loss = det_loss\n",
    "        #             loss.requires_grad_(True)\n",
    "        #             det_optimizer.zero_grad()\n",
    "        #             loss.backward()\n",
    "        #             det_optimizer.step()\n",
    "\n",
    "        #         loss_list.append(loss.item())\n",
    "\n",
    "        # print(\"Loss: \", np.mean(loss_list))\n",
    "\n",
    "\n",
    "        seg_features = []\n",
    "        det_features = []\n",
    "        for image in support_loader:\n",
    "            image = image[0].to(device)\n",
    "            with torch.no_grad():\n",
    "                _, seg_patch_tokens, det_patch_tokens = model(image)\n",
    "\n",
    "                seg_patch_tokens = [p[0].contiguous() for p in seg_patch_tokens]\n",
    "                det_patch_tokens = [p[0].contiguous() for p in det_patch_tokens]\n",
    "                seg_features.append(seg_patch_tokens)\n",
    "                det_features.append(det_patch_tokens)\n",
    "        seg_mem_features = [torch.cat([seg_features[j][i] for j in range(len(seg_features))], dim=0) for i in range(len(seg_features[0]))]\n",
    "        det_mem_features = [torch.cat([det_features[j][i] for j in range(len(det_features))], dim=0) for i in range(len(det_features[0]))]\n",
    "        \n",
    "\n",
    "        result = test(args, model, test_loader, text_features, seg_mem_features, det_mem_features)\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            print(\"Best result\\n\")\n",
    "            if args.save_model == 1:\n",
    "                ckp_path = os.path.join(args.save_path, f'{args.obj}.pth')\n",
    "                torch.save({'seg_adapters': model.seg_adapters.state_dict(),\n",
    "                            'det_adapters': model.det_adapters.state_dict()}, \n",
    "                            ckp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数组形状: (4, 4, 1, 2)\n",
      "原始数组内容:\n",
      " [[[[ 0  1]]\n",
      "\n",
      "  [[ 2  3]]\n",
      "\n",
      "  [[ 4  5]]\n",
      "\n",
      "  [[ 6  7]]]\n",
      "\n",
      "\n",
      " [[[ 8  9]]\n",
      "\n",
      "  [[10 11]]\n",
      "\n",
      "  [[12 13]]\n",
      "\n",
      "  [[14 15]]]\n",
      "\n",
      "\n",
      " [[[16 17]]\n",
      "\n",
      "  [[18 19]]\n",
      "\n",
      "  [[20 21]]\n",
      "\n",
      "  [[22 23]]]\n",
      "\n",
      "\n",
      " [[[24 25]]\n",
      "\n",
      "  [[26 27]]\n",
      "\n",
      "  [[28 29]]\n",
      "\n",
      "  [[30 31]]]]\n",
      "\n",
      "沿 axis=0 求和后形状: (4, 1, 2)\n",
      "结果:\n",
      " [[[48 52]]\n",
      "\n",
      " [[56 60]]\n",
      "\n",
      " [[64 68]]\n",
      "\n",
      " [[72 76]]]\n",
      "\n",
      "沿 axis=1 求和后形状: (4, 1, 2)\n",
      "结果:\n",
      " [[[ 12  16]]\n",
      "\n",
      " [[ 44  48]]\n",
      "\n",
      " [[ 76  80]]\n",
      "\n",
      " [[108 112]]]\n",
      "\n",
      "沿 axis=2 求和后形状: (4, 4, 2)\n",
      "结果:\n",
      " [[[ 0  1]\n",
      "  [ 2  3]\n",
      "  [ 4  5]\n",
      "  [ 6  7]]\n",
      "\n",
      " [[ 8  9]\n",
      "  [10 11]\n",
      "  [12 13]\n",
      "  [14 15]]\n",
      "\n",
      " [[16 17]\n",
      "  [18 19]\n",
      "  [20 21]\n",
      "  [22 23]]\n",
      "\n",
      " [[24 25]\n",
      "  [26 27]\n",
      "  [28 29]\n",
      "  [30 31]]]\n",
      "\n",
      "沿 axis=3 求和后形状: (4, 4, 1)\n",
      "结果:\n",
      " [[[ 1]\n",
      "  [ 5]\n",
      "  [ 9]\n",
      "  [13]]\n",
      "\n",
      " [[17]\n",
      "  [21]\n",
      "  [25]\n",
      "  [29]]\n",
      "\n",
      " [[33]\n",
      "  [37]\n",
      "  [41]\n",
      "  [45]]\n",
      "\n",
      " [[49]\n",
      "  [53]\n",
      "  [57]\n",
      "  [61]]]\n"
     ]
    }
   ],
   "source": [
    "# 生成一段高维数组（4，4，1，2）的对于各维度的求和示例代码\n",
    "import numpy as np\n",
    "\n",
    "# 生成一个高维数组 (4,4,1,2)，填充有序数字便于观察\n",
    "arr = np.arange(4*4*1*2).reshape(4,4,1,2)\n",
    "print(\"原始数组形状:\", arr.shape)\n",
    "print(\"原始数组内容:\\n\", arr)\n",
    "\n",
    "# ----------------------------\n",
    "# 不同维度的求和示例\n",
    "# ----------------------------\n",
    "\n",
    "# 1. 沿 axis=0 求和（合并第1个维度）\n",
    "sum_axis0 = np.sum(arr, axis=0)\n",
    "print(\"\\n沿 axis=0 求和后形状:\", sum_axis0.shape)\n",
    "print(\"结果:\\n\", sum_axis0)\n",
    "\n",
    "# 2. 沿 axis=1 求和（合并第2个维度）\n",
    "sum_axis1 = np.sum(arr, axis=1)\n",
    "print(\"\\n沿 axis=1 求和后形状:\", sum_axis1.shape)\n",
    "print(\"结果:\\n\", sum_axis1)\n",
    "\n",
    "# 3. 沿 axis=2 求和（合并第3个维度）\n",
    "sum_axis2 = np.sum(arr, axis=2)\n",
    "print(\"\\n沿 axis=2 求和后形状:\", sum_axis2.shape)\n",
    "print(\"结果:\\n\", sum_axis2)\n",
    "\n",
    "# 4. 沿 axis=3 求和（合并第4个维度）\n",
    "sum_axis3 = np.sum(arr, axis=3)\n",
    "print(\"\\n沿 axis=3 求和后形状:\", sum_axis3.shape)\n",
    "print(\"结果:\\n\", sum_axis3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MVFA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
