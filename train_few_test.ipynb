{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qin/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from CLIP.adapter import CLIP_Inplanted\n",
    "from CLIP.clip import create_model\n",
    "\n",
    "from loss import FocalLoss, BinaryDiceLoss\n",
    "\n",
    "from dataset.medical_zero import MedTestDataset, MedTrainDataset\n",
    "from dataset.medical_few import MedDataset\n",
    "\n",
    "from utils import augment, cos_sim, encode_text_with_prompt_ensemble\n",
    "from prompt import REAL_NAME\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "CLASS_INDEX = {'Brain':3, 'Liver':2, 'Retina_RESC':1, 'Retina_OCT2017':-1, 'Chest':-2, 'Histopathology':-3}\n",
    "CLASS_INDEX_INV = {3:'Brain', 2:'Liver', 1:'Retina_RESC', -1:'Retina_OCT2017', -2:'Chest', -3:'Histopathology'}\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, test_loader, text_features, seg_mem_features, det_mem_features):\n",
    "    gt_list = []\n",
    "    gt_mask_list = []\n",
    "\n",
    "    det_image_scores_zero = []\n",
    "    det_image_scores_few = []\n",
    "    \n",
    "    seg_score_map_zero = []\n",
    "    seg_score_map_few= []\n",
    "    \n",
    "    step = 0\n",
    "    for (image, y, mask) in tqdm(test_loader):\n",
    "        step+=1\n",
    "        if step < 100:\n",
    "            continue\n",
    "        image = image.to(device)\n",
    "        mask[mask > 0.5], mask[mask <= 0.5] = 1, 0\n",
    "        # print(\"mask.shape:\", mask.shape)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            _, seg_patch_tokens, det_patch_tokens = model(image)\n",
    "            # 去掉cls token ，由于 biomedclip cls token 位置不同，此处需要对应改变\n",
    "            seg_patch_tokens = [p[:, 1:, :] for p in seg_patch_tokens]\n",
    "            det_patch_tokens = [p[:, 1:, :] for p in det_patch_tokens]\n",
    "   \n",
    "            if CLASS_INDEX[args.obj] > 0:\n",
    "\n",
    "                # few-shot, seg head\n",
    "                anomaly_maps_few_shot = []\n",
    "                for idx, p in enumerate(seg_patch_tokens):\n",
    "                    batch_cos_sim = []\n",
    "                    for b in range(p.shape[0]):\n",
    "                        cos = cos_sim(seg_mem_features[idx][b], p[b])\n",
    "                        height = int(np.sqrt(cos.shape[1]))\n",
    "                        anomaly_map_few_shot = torch.min((1 - cos), 0)[0].reshape(1, 1, height, height)\n",
    "                        anomaly_map_few_shot = F.interpolate(torch.tensor(anomaly_map_few_shot),\n",
    "                                                            size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                        batch_cos_sim.append(anomaly_map_few_shot[0].cpu().numpy())\n",
    "                        # print('batch_cos_sim.shape:', batch_cos_sim[0].shape)\n",
    "                    anomaly_maps_few_shot.append(np.stack(batch_cos_sim, axis=0))\n",
    "                    # print('anomaly_maps_few_shot.shape:', anomaly_maps_few_shot[0].shape)\n",
    "                score_map_few = np.sum(anomaly_maps_few_shot, axis=0)\n",
    "                seg_score_map_few.append(score_map_few)\n",
    "                # print('seg_score_map_few.shape:', seg_score_map_few[0].shape)\n",
    "                \n",
    "                # zero-shot, seg head\n",
    "                anomaly_maps = []\n",
    "                for layer in range(len(seg_patch_tokens)):\n",
    "                    seg_patch_tokens[layer] /= seg_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "                    anomaly_map = (100.0 * seg_patch_tokens[layer] @ text_features)\n",
    "                    B, L, C = anomaly_map.shape\n",
    "                    H = int(np.sqrt(L))\n",
    "                    anomaly_map = F.interpolate(anomaly_map.permute(0, 2, 1).view(B, 2, H, H),\n",
    "                                                size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                    # print('anomaly_map.shape:', anomaly_map.shape)\n",
    "                    # 4 2 224 224\n",
    "                    anomaly_map = torch.softmax(anomaly_map, dim=1)[:, 1:2, :, :]\n",
    "                    # 4 224 224 \n",
    "                    # print('anomaly_map.shape:', anomaly_map.shape)\n",
    "                    anomaly_maps.append(anomaly_map.cpu().numpy())\n",
    "                    # print('anomaly_map.shape:', anomaly_map[0].shape)\n",
    "                    # print(len(anomaly_maps))\n",
    "                    # print('anomaly_map.shape:', anomaly_maps[0].shape)\n",
    "                # print('anomaly_maps.shape:', len(anomaly_maps))\n",
    "                \n",
    "                # print('anomaly_maps[0].shape:', anomaly_maps[0].shape)\n",
    "                score_map_zero = np.sum(anomaly_maps, axis=0)\n",
    "                # print('score_map_zero.shape:', score_map_zero.shape)\n",
    "                # print('score_map_zero.shape:', score_map_zero.shape)\n",
    "                seg_score_map_zero.append(score_map_zero)\n",
    "                # print(len(seg_score_map_zero))\n",
    "                # print('seg_score_map_zero.shape:', seg_score_map_zero[0].shape)\n",
    "                # \n",
    "\n",
    "\n",
    "            else:\n",
    "                # few-shot, det head\n",
    "                anomaly_maps_few_shot = []\n",
    "                for idx, p in enumerate(det_patch_tokens):\n",
    "                    cos = cos_sim(det_mem_features[idx], p)\n",
    "                    height = int(np.sqrt(cos.shape[1]))\n",
    "                    anomaly_map_few_shot = torch.min((1 - cos), 0)[0].reshape(1, 1, height, height)\n",
    "                    anomaly_map_few_shot = F.interpolate(torch.tensor(anomaly_map_few_shot),\n",
    "                                                            size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                    anomaly_maps_few_shot.append(anomaly_map_few_shot[0].cpu().numpy())\n",
    "                anomaly_map_few_shot = np.sum(anomaly_maps_few_shot, axis=0)\n",
    "                score_few_det = anomaly_map_few_shot.mean()\n",
    "                det_image_scores_few.append(score_few_det)\n",
    "\n",
    "                # zero-shot, det head\n",
    "                anomaly_score = 0\n",
    "                for layer in range(len(det_patch_tokens)):\n",
    "                    det_patch_tokens[layer] /= det_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "                    anomaly_map = (100.0 * det_patch_tokens[layer] @ text_features).unsqueeze(0)\n",
    "                    anomaly_map = torch.softmax(anomaly_map, dim=-1)[:, :, 1]\n",
    "                    anomaly_score += anomaly_map.mean()\n",
    "                det_image_scores_zero.append(anomaly_score.cpu().numpy())\n",
    "\n",
    "            # 使用tensor将mask添加到gt_mask_list中\n",
    "            gt_mask_list.append(mask.cpu().detach().numpy())\n",
    "            gt_list.extend(y.cpu().detach().numpy())\n",
    "\n",
    "            \n",
    "    # 问题不在于gt_mask_list的维度，在于seg_score_map_zero的维度被缩减了\n",
    "    gt_list = np.array(gt_list)\n",
    "    # gt-list (932,)\n",
    "    \n",
    "    # print(\"len(gt_mask_list):\", len(gt_mask_list))\n",
    "    # print(\"shape(gt_mask_list):\", gt_mask_list[-1].shape)\n",
    "\n",
    "    # print(\"shape(gt_mask_list[-1]):\", gt_mask_list[0])\n",
    "    # print(\"shape(gt_mask_list[-1]):\", gt_mask_list[-1])\n",
    "    # print(gt_mask_list[-2])\n",
    "    # print(gt_mask_list[-1])\n",
    "    #! 最后只有一个时，维度会被压缩\n",
    "    # asarray batch_size设置使得最后的对象与前面的大小不统一会报错\n",
    "    gt_mask_list = [\n",
    "        gt_mask_list[j][i] if len(gt_mask_list[j].shape) > 2 else gt_mask_list[j]\n",
    "            for j in range(len(gt_mask_list))        # 先遍历元素索引 j\n",
    "            for i in range(gt_mask_list[j].shape[0])  # 再遍历元素内部的样本索引 i\n",
    "            ]\n",
    "    print('gt_mask_list shape:', len(gt_mask_list))\n",
    "    print('gt_mask_list[0].shape:', gt_mask_list[-2].shape)\n",
    "    print('gt_mask_list[-1].shape:', gt_mask_list[-1].shape)\n",
    "\n",
    "    gt_mask_list = np.asarray(gt_mask_list)\n",
    "    gt_mask_list = (gt_mask_list>0).astype(np.int_)\n",
    "    print('gt_mask_list shape:', gt_mask_list.shape)\n",
    "    \n",
    "    # gt_mask_list = gt_mask_list[:len_gt_mask_list]\n",
    "    # gt_mask_list.shape image_nums,batch_size,224,224\n",
    "\n",
    "    if CLASS_INDEX[args.obj] > 0:\n",
    "        print(\"seg_score_map_zero shape:\", len(seg_score_map_zero))\n",
    "        print('seg_score_map_zero[0].shape:', seg_score_map_zero[0].shape)\n",
    "        seg_score_map_zero = [seg_score_map_zero[j][i] if len(seg_score_map_zero[j].shape) > 2 else seg_score_map_zero[j]\n",
    "            for j in range(len(seg_score_map_zero))        # 先遍历元素索引 j\n",
    "            for i in range(seg_score_map_zero[j].shape[0])  # 再遍历元素内部的样本索引 i\n",
    "            ]\n",
    "        seg_score_map_zero = np.array(seg_score_map_zero)\n",
    "        print('seg_score_map_zero shape:', seg_score_map_zero.shape)\n",
    "        \n",
    "        seg_score_map_few = [seg_score_map_few[j][i] if len(seg_score_map_few[j].shape) > 2 else seg_score_map_few[j]\n",
    "            for j in range(len(seg_score_map_few))        # 先遍历元素索引 j\n",
    "            for i in range(seg_score_map_few[j].shape[0])  # 再遍历元素内部的样本索引 i\n",
    "            ]\n",
    "        seg_score_map_few = np.array(seg_score_map_few)\n",
    "\n",
    "        seg_score_map_zero = (seg_score_map_zero - seg_score_map_zero.min()) / (seg_score_map_zero.max() - seg_score_map_zero.min())\n",
    "        seg_score_map_few = (seg_score_map_few - seg_score_map_few.min()) / (seg_score_map_few.max() - seg_score_map_few.min())\n",
    "        segment_scores = 0.5 * seg_score_map_zero + 0.5 * seg_score_map_few\n",
    "\n",
    "        seg_roc_auc = roc_auc_score(gt_mask_list.flatten(), segment_scores.flatten())\n",
    "        print(f'{args.obj} pAUC : {round(seg_roc_auc,4)}')\n",
    "\n",
    "        # segment_scores size (238, 4, 1, 224, 224)\n",
    "        segment_scores_flatten = segment_scores.reshape(segment_scores.shape[0] * segment_scores.shape[1], -1)\n",
    "        # return segment_scores_flatten,gt_list\n",
    "    \n",
    "        roc_auc_im = roc_auc_score(gt_list, np.max(segment_scores_flatten, axis=1))\n",
    "        print(f'{args.obj} AUC : {round(roc_auc_im, 4)}')\n",
    "\n",
    "        return seg_roc_auc + roc_auc_im\n",
    "\n",
    "    else:\n",
    "\n",
    "        det_image_scores_zero = np.array(det_image_scores_zero)\n",
    "        det_image_scores_few = np.array(det_image_scores_few)\n",
    "\n",
    "        det_image_scores_zero = (det_image_scores_zero - det_image_scores_zero.min()) / (det_image_scores_zero.max() - det_image_scores_zero.min())\n",
    "        det_image_scores_few = (det_image_scores_few - det_image_scores_few.min()) / (det_image_scores_few.max() - det_image_scores_few.min())\n",
    "    \n",
    "        image_scores = 0.5 * det_image_scores_zero + 0.5 * det_image_scores_few\n",
    "        img_roc_auc_det = roc_auc_score(gt_list, image_scores)\n",
    "        print(f'{args.obj} AUC : {round(img_roc_auc_det,4)}')\n",
    "\n",
    "        return img_roc_auc_det\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Testing')\n",
    "parser.add_argument('--model_name', type=str, default='biomedclip_local',)\n",
    "parser.add_argument('--pretrain', type=str, default='CLIP/ckpt/open_clip_pytorch_model.bin')\n",
    "parser.add_argument('--obj', type=str, default='Liver')\n",
    "parser.add_argument('--data_path', type=str, default='/root/data/')\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "parser.add_argument('--save_model', type=int, default=1)\n",
    "parser.add_argument('--save_path', type=str, default='./ckpt/few-shot/')\n",
    "parser.add_argument('--img_size', type=int, default=224)\n",
    "parser.add_argument(\"--epoch\", type=int, default=50, help=\"epochs\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.001, help=\"learning rate\")\n",
    "parser.add_argument(\"--features_list\", type=int, nargs=\"+\", default=[3,6,9,12], help=\"features used\")\n",
    "parser.add_argument('--seed', type=int, default=111)\n",
    "parser.add_argument('--shot', type=int, default=4)\n",
    "parser.add_argument('--iterate', type=int, default=0)\n",
    "args = parser.parse_args(args=['--obj', 'Liver',  '--shot', '4', '--batch_size', '1','--data_path','../MVFA-AD/data/'])\n",
    "\n",
    "setup_seed(args.seed)\n",
    "\n",
    "# fixed feature extractor\n",
    "biomedclip_model,tokenizer = create_model(model_name=args.model_name, \n",
    "                            force_image_size=args.img_size, \n",
    "                            device=device, \n",
    "                            pretrained=args.pretrain, \n",
    "                            require_pretrained=True)\n",
    "\n",
    "biomedclip_model.eval()\n",
    "\n",
    "# 模型添加适配器\n",
    "model = CLIP_Inplanted(clip_model=biomedclip_model, features=args.features_list).to(device)\n",
    "model.eval()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# optimizer for only adapters\n",
    "seg_optimizer = torch.optim.Adam(list(model.seg_adapters.parameters()), lr=args.learning_rate, betas=(0.5, 0.999))\n",
    "det_optimizer = torch.optim.Adam(list(model.det_adapters.parameters()), lr=args.learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# losses\n",
    "loss_focal = FocalLoss()\n",
    "loss_dice = BinaryDiceLoss()\n",
    "loss_bce = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# text prompt\n",
    "with torch.cuda.amp.autocast(), torch.no_grad():\n",
    "    text_features = encode_text_with_prompt_ensemble(biomedclip_model, tokenizer, REAL_NAME[args.obj], device)\n",
    "\n",
    "\n",
    "best_result = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 4\n",
    "args.shot = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "kwargs = {'num_workers': 12, 'pin_memory': True} if use_cuda else {}\n",
    "test_dataset = MedDataset(args.data_path, args.obj, args.img_size, args.shot, args.iterate)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "# few-shot image augmentation\n",
    "augment_abnorm_img, augment_abnorm_mask = augment(test_dataset.fewshot_abnorm_img, test_dataset.fewshot_abnorm_mask)\n",
    "augment_normal_img, augment_normal_mask = augment(test_dataset.fewshot_norm_img)\n",
    "\n",
    "augment_fewshot_img = torch.cat([augment_abnorm_img, augment_normal_img], dim=0)\n",
    "augment_fewshot_mask = torch.cat([augment_abnorm_mask, augment_normal_mask], dim=0)\n",
    "\n",
    "augment_fewshot_label = torch.cat([torch.Tensor([1] * len(augment_abnorm_img)), torch.Tensor([0] * len(augment_normal_img))], dim=0)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(augment_fewshot_img, augment_fewshot_mask, augment_fewshot_label)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "# memory bank construction\n",
    "support_dataset = torch.utils.data.TensorDataset(augment_normal_img)\n",
    "support_loader = torch.utils.data.DataLoader(support_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:27<00:00, 13.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_mask_list shape: 1097\n",
      "gt_mask_list[0].shape: (1, 224, 224)\n",
      "gt_mask_list[-1].shape: (1, 224, 224)\n",
      "gt_mask_list shape: (1097, 1, 224, 224)\n",
      "seg_score_map_zero shape: 275\n",
      "seg_score_map_zero[0].shape: (4, 1, 224, 224)\n",
      "seg_score_map_zero shape: (1097, 1, 224, 224)\n",
      "Liver pAUC : 0.9034\n",
      "Liver AUC : 0.5118\n",
      "Best result\n",
      "\n",
      "epoch  1 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 239/374 [00:14<00:08, 16.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m det_mem_features \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mcat([det_features[j][i]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,det_features[j][i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m],det_features[j][i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(det_features))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(det_features[\u001b[38;5;241m0\u001b[39m]))]\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# seg_mem_features size =>  4, (image_nums * 197, embed_size) \u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_mem_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdet_mem_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m>\u001b[39m best_result:\n\u001b[1;32m     81\u001b[0m     best_result \u001b[38;5;241m=\u001b[39m result\n",
      "Cell \u001b[0;32mIn[6], line 56\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(args, model, test_loader, text_features, seg_mem_features, det_mem_features)\u001b[0m\n\u001b[1;32m     52\u001b[0m anomaly_map \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(anomaly_map\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;241m2\u001b[39m, H, H),\n\u001b[1;32m     53\u001b[0m                             size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mimg_size, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# print('anomaly_map.shape:', anomaly_map.shape)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# 4 2 224 224\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m anomaly_map \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43manomaly_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m, :, :]\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# 4 224 224 \u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# print('anomaly_map.shape:', anomaly_map.shape)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m anomaly_maps\u001b[38;5;241m.\u001b[39mappend(anomaly_map\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epoch):\n",
    "    print('epoch ', epoch, ':')\n",
    "\n",
    "    loss_list = []\n",
    "    for (image, gt, label) in train_loader:\n",
    "        image = image.to(device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            _, seg_patch_tokens, det_patch_tokens = model(image)\n",
    "            # seg_patch_tokens size { [batch_size,196,512] * 4} \n",
    "            seg_patch_tokens = [p[:, 1:, :] for p in seg_patch_tokens]\n",
    "            det_patch_tokens = [p[:, 1:, :] for p in det_patch_tokens]\n",
    "\n",
    "            # det loss\n",
    "            det_loss = 0\n",
    "            image_label = label.to(device)\n",
    "            for layer in range(len(det_patch_tokens)):\n",
    "                det_patch_tokens[layer] = det_patch_tokens[layer] / det_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "                anomaly_map = (100.0 * det_patch_tokens[layer] @ text_features)   \n",
    "                anomaly_map = torch.softmax(anomaly_map, dim=-1)[:, :, 1]\n",
    "                anomaly_score = torch.mean(anomaly_map, dim=-1)\n",
    "                det_loss += loss_bce(anomaly_score, image_label)\n",
    "\n",
    "            if CLASS_INDEX[args.obj] > 0:\n",
    "                # pixel level\n",
    "                seg_loss = 0\n",
    "                mask = gt.squeeze(0).to(device)\n",
    "                mask[mask > 0.5], mask[mask <= 0.5] = 1, 0\n",
    "                for layer in range(len(seg_patch_tokens)):\n",
    "                    seg_patch_tokens[layer] = seg_patch_tokens[layer] / seg_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "                    anomaly_map = (100.0 * seg_patch_tokens[layer] @ text_features)\n",
    "                    B, L, C = anomaly_map.shape\n",
    "                    H = int(np.sqrt(L))\n",
    "                    anomaly_map = F.interpolate(anomaly_map.permute(0, 2, 1).view(B, 2, H, H),\n",
    "                                                size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                    anomaly_map = torch.softmax(anomaly_map, dim=1)\n",
    "                    seg_loss += loss_focal(anomaly_map, mask)\n",
    "                    seg_loss += loss_dice(anomaly_map[:, 1, :, :], mask)\n",
    "                \n",
    "                loss = seg_loss + det_loss\n",
    "                loss.requires_grad_(True)\n",
    "                seg_optimizer.zero_grad()\n",
    "                det_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                seg_optimizer.step()\n",
    "                det_optimizer.step()\n",
    "\n",
    "            else:\n",
    "                loss = det_loss\n",
    "                loss.requires_grad_(True)\n",
    "                det_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                det_optimizer.step()\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "    print(\"Loss: \", np.mean(loss_list))\n",
    "\n",
    "\n",
    "    seg_features = []\n",
    "    det_features = []\n",
    "    for image in support_loader:\n",
    "        image = image[0].to(device)\n",
    "        with torch.no_grad():\n",
    "            _, seg_patch_tokens, det_patch_tokens = model(image)\n",
    "            #? seg_patch_tokens size { [batch_size,197,512] * 4}\n",
    "            \n",
    "            #! 0 -> : , 仅改变batch_size维度， 不会改变其他维度\n",
    "            seg_patch_tokens = [p.contiguous() for p in seg_patch_tokens]\n",
    "            det_patch_tokens = [p.contiguous() for p in det_patch_tokens]\n",
    "            seg_features.append(seg_patch_tokens)\n",
    "            det_features.append(det_patch_tokens)\n",
    "    # batch_size = 1时， seg_features  image_nums， 4 ， [197,embed_size]\n",
    "    # batch_size = 2时， seg_features  {image_nums * { 4 * [2 ,197,embed_size] }  }\n",
    "    #! batch_size > 1 时， seg_features 维度会缩减！\n",
    "    seg_mem_features = [torch.cat([seg_features[j][i].view(-1,seg_features[j][i].shape[-2],seg_features[j][i].shape[-1]) for j in range(len(seg_features))], dim=0) for i in range(len(seg_features[0]))]\n",
    "    det_mem_features = [torch.cat([det_features[j][i].view(-1,det_features[j][i].shape[-2],det_features[j][i].shape[-1]) for j in range(len(det_features))], dim=0) for i in range(len(det_features[0]))]\n",
    "    # seg_mem_features size =>  4, (image_nums * 197, embed_size) \n",
    "    \n",
    "    result = test(args, model, test_loader, text_features, seg_mem_features, det_mem_features)\n",
    "    if result > best_result:\n",
    "        best_result = result\n",
    "        print(\"Best result\\n\")\n",
    "        if args.save_model == 1:\n",
    "            ckp_path = os.path.join(args.save_path, f'{args.obj}.pth')\n",
    "            torch.save({'seg_adapters': model.seg_adapters.state_dict(),\n",
    "                        'det_adapters': model.det_adapters.state_dict()}, \n",
    "                        ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 :\n",
      "Loss:  4.277325229211287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1493 [00:00<05:44,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1493 [00:00<04:15,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/1493 [00:01<02:53,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1493 [00:01<02:42,  9.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1493 [00:01<02:35,  9.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 13/1493 [00:01<02:32,  9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 15/1493 [00:01<02:30,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/1493 [00:02<02:28,  9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 20/1493 [00:02<02:28,  9.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 22/1493 [00:02<02:53,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "1\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "2\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "3\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "augment_normal_imgs.shape: torch.Size([1, 2, 224, 224])\n",
      "anomaly_map.shape: torch.Size([1, 224, 224])\n",
      "4\n",
      "anomaly_maps.shape: (1, 224, 224)\n",
      "score_map_zero.shape: (1, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 228\u001b[0m\n\u001b[1;32m    224\u001b[0m seg_mem_features \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mcat([seg_features[j][i] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seg_features))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seg_features[\u001b[38;5;241m0\u001b[39m]))]\n\u001b[1;32m    225\u001b[0m det_mem_features \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mcat([det_features[j][i] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(det_features))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(det_features[\u001b[38;5;241m0\u001b[39m]))]\n\u001b[0;32m--> 228\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_mem_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdet_mem_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m>\u001b[39m best_result:\n\u001b[1;32m    230\u001b[0m     best_result \u001b[38;5;241m=\u001b[39m result\n",
      "Cell \u001b[0;32mIn[20], line 19\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(args, model, test_loader, text_features, seg_mem_features, det_mem_features)\u001b[0m\n\u001b[1;32m     16\u001b[0m mask[mask \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m], mask[mask \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 19\u001b[0m     _, seg_patch_tokens, det_patch_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     seg_patch_tokens \u001b[38;5;241m=\u001b[39m [p[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m:, :] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m seg_patch_tokens]\n\u001b[1;32m     21\u001b[0m     det_patch_tokens \u001b[38;5;241m=\u001b[39m [p[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m:, :] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m det_patch_tokens]\n",
      "File \u001b[0;32m~/miniconda3/envs/MVFA/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/BioMed_adapter/CLIP/adapter.py:54\u001b[0m, in \u001b[0;36mCLIP_Inplanted.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m det_patch_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_encoder\u001b[38;5;241m.\u001b[39mtrunk\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m---> 54\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[1;32m     56\u001b[0m         seg_adapt_med, seg_adapt_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseg_adapters[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mindex(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)](x)\n",
      "File \u001b[0;32m~/miniconda3/envs/MVFA/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/MVFA/lib/python3.8/site-packages/timm/models/vision_transformer.py:170\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    169\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x))))\n\u001b[0;32m--> 170\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/MVFA/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/MVFA/lib/python3.8/site-packages/timm/layers/mlp.py:44\u001b[0m, in \u001b[0;36mMlp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 44\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[1;32m     46\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/MVFA/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/MVFA/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 测试原本\n",
    "args.batch_size = 1\n",
    "args.shot = 2\n",
    "def test(args, model, test_loader, text_features, seg_mem_features, det_mem_features):\n",
    "    gt_list = []\n",
    "    gt_mask_list = []\n",
    "\n",
    "    det_image_scores_zero = []\n",
    "    det_image_scores_few = []\n",
    "    \n",
    "    seg_score_map_zero = []\n",
    "    seg_score_map_few= []\n",
    "\n",
    "    for (image, y, mask) in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        mask[mask > 0.5], mask[mask <= 0.5] = 1, 0\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            _, seg_patch_tokens, det_patch_tokens = model(image)\n",
    "            seg_patch_tokens = [p[0, 1:, :] for p in seg_patch_tokens]\n",
    "            det_patch_tokens = [p[0, 1:, :] for p in det_patch_tokens]\n",
    "\n",
    "            if CLASS_INDEX[args.obj] > 0:\n",
    "\n",
    "                # few-shot, seg head\n",
    "                anomaly_maps_few_shot = []\n",
    "                for idx, p in enumerate(seg_patch_tokens):\n",
    "                    \n",
    "                    cos = cos_sim(seg_mem_features[idx], p)\n",
    "                    height = int(np.sqrt(cos.shape[1]))\n",
    "                    anomaly_map_few_shot = torch.min((1 - cos), 0)[0].reshape(1, 1, height, height)\n",
    "\n",
    "                    anomaly_map_few_shot = F.interpolate(torch.tensor(anomaly_map_few_shot),\n",
    "                                                            size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                    anomaly_maps_few_shot.append(anomaly_map_few_shot[0].cpu().numpy())\n",
    " \n",
    "                score_map_few = np.sum(anomaly_maps_few_shot, axis=0)\n",
    "                seg_score_map_few.append(score_map_few)\n",
    "\n",
    "                # zero-shot, seg head\n",
    "                anomaly_maps = []\n",
    "                for layer in range(len(seg_patch_tokens)):\n",
    "                    \n",
    "                    seg_patch_tokens[layer] /= seg_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "                    anomaly_map = (100.0 * seg_patch_tokens[layer] @ text_features).unsqueeze(0)\n",
    "        \n",
    "                    B, L, C = anomaly_map.shape\n",
    "                    H = int(np.sqrt(L))\n",
    "                    anomaly_map = F.interpolate(anomaly_map.permute(0, 2, 1).view(B, 2, H, H),\n",
    "                                                size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                    print(\"augment_normal_imgs.shape:\", anomaly_map.shape)\n",
    "                    anomaly_map = torch.softmax(anomaly_map, dim=1)[:, 1, :, :]\n",
    "                    print(\"anomaly_map.shape:\", anomaly_map.shape)\n",
    "                    anomaly_maps.append(anomaly_map.cpu().numpy())\n",
    "                    print(len(anomaly_maps))\n",
    "                    print(\"anomaly_maps.shape:\", anomaly_maps[0].shape)\n",
    "                score_map_zero = np.sum(anomaly_maps, axis=0)\n",
    "                print(\"score_map_zero.shape:\", score_map_zero.shape)\n",
    "                seg_score_map_zero.append(score_map_zero)\n",
    "                \n",
    "\n",
    "\n",
    "            else:\n",
    "                # few-shot, det head\n",
    "                anomaly_maps_few_shot = []\n",
    "                for idx, p in enumerate(det_patch_tokens):\n",
    "                    cos = cos_sim(det_mem_features[idx], p)\n",
    "                    height = int(np.sqrt(cos.shape[1]))\n",
    "                    anomaly_map_few_shot = torch.min((1 - cos), 0)[0].reshape(1, 1, height, height)\n",
    "                    anomaly_map_few_shot = F.interpolate(torch.tensor(anomaly_map_few_shot),\n",
    "                                                            size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                    anomaly_maps_few_shot.append(anomaly_map_few_shot[0].cpu().numpy())\n",
    "                anomaly_map_few_shot = np.sum(anomaly_maps_few_shot, axis=0)\n",
    "                score_few_det = anomaly_map_few_shot.mean()\n",
    "                det_image_scores_few.append(score_few_det)\n",
    "\n",
    "                # zero-shot, det head\n",
    "                anomaly_score = 0\n",
    "                for layer in range(len(det_patch_tokens)):\n",
    "                    det_patch_tokens[layer] /= det_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "                    anomaly_map = (100.0 * det_patch_tokens[layer] @ text_features).unsqueeze(0)\n",
    "                    anomaly_map = torch.softmax(anomaly_map, dim=-1)[:, :, 1]\n",
    "                    anomaly_score += anomaly_map.mean()\n",
    "                det_image_scores_zero.append(anomaly_score.cpu().numpy())\n",
    "\n",
    "            \n",
    "            gt_mask_list.append(mask.squeeze().cpu().detach().numpy())\n",
    "            gt_list.extend(y.cpu().detach().numpy())\n",
    "            \n",
    "\n",
    "    gt_list = np.array(gt_list)\n",
    "    gt_mask_list = np.asarray(gt_mask_list)\n",
    "    gt_mask_list = (gt_mask_list>0).astype(np.int_)\n",
    "\n",
    "\n",
    "    if CLASS_INDEX[args.obj] > 0:\n",
    "\n",
    "        seg_score_map_zero = np.array(seg_score_map_zero)\n",
    "        seg_score_map_few = np.array(seg_score_map_few)\n",
    "\n",
    "        seg_score_map_zero = (seg_score_map_zero - seg_score_map_zero.min()) / (seg_score_map_zero.max() - seg_score_map_zero.min())\n",
    "        seg_score_map_few = (seg_score_map_few - seg_score_map_few.min()) / (seg_score_map_few.max() - seg_score_map_few.min())\n",
    "\n",
    "        print(\"seg_score_map_zero.shape:\", seg_score_map_zero.shape)\n",
    "        print('seg_score_map_few.shape:', seg_score_map_few.shape)\n",
    "        segment_scores = 0.5 * seg_score_map_zero + 0.5 * seg_score_map_few\n",
    "        seg_roc_auc = roc_auc_score(gt_mask_list.flatten(), segment_scores.flatten())\n",
    "        print(f'{args.obj} pAUC : {round(seg_roc_auc,4)}')\n",
    "\n",
    "        segment_scores_flatten = segment_scores.reshape(segment_scores.shape[0], -1)\n",
    "        roc_auc_im = roc_auc_score(gt_list, np.max(segment_scores_flatten, axis=1))\n",
    "        print(f'{args.obj} AUC : {round(roc_auc_im, 4)}')\n",
    "\n",
    "        return seg_roc_auc + roc_auc_im\n",
    "\n",
    "    else:\n",
    "\n",
    "        det_image_scores_zero = np.array(det_image_scores_zero)\n",
    "        det_image_scores_few = np.array(det_image_scores_few)\n",
    "\n",
    "        det_image_scores_zero = (det_image_scores_zero - det_image_scores_zero.min()) / (det_image_scores_zero.max() - det_image_scores_zero.min())\n",
    "        det_image_scores_few = (det_image_scores_few - det_image_scores_few.min()) / (det_image_scores_few.max() - det_image_scores_few.min())\n",
    "    \n",
    "        image_scores = 0.5 * det_image_scores_zero + 0.5 * det_image_scores_few\n",
    "        img_roc_auc_det = roc_auc_score(gt_list, image_scores)\n",
    "        print(f'{args.obj} AUC : {round(img_roc_auc_det,4)}')\n",
    "\n",
    "        return img_roc_auc_det\n",
    "    \n",
    "    # load test dataset\n",
    "kwargs = {'num_workers': 12, 'pin_memory': True} if use_cuda else {}\n",
    "test_dataset = MedDataset(args.data_path, args.obj, args.img_size, args.shot, args.iterate)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "# few-shot image augmentation\n",
    "augment_abnorm_img, augment_abnorm_mask = augment(test_dataset.fewshot_abnorm_img, test_dataset.fewshot_abnorm_mask)\n",
    "augment_normal_img, augment_normal_mask = augment(test_dataset.fewshot_norm_img)\n",
    "\n",
    "augment_fewshot_img = torch.cat([augment_abnorm_img, augment_normal_img], dim=0)\n",
    "augment_fewshot_mask = torch.cat([augment_abnorm_mask, augment_normal_mask], dim=0)\n",
    "\n",
    "augment_fewshot_label = torch.cat([torch.Tensor([1] * len(augment_abnorm_img)), torch.Tensor([0] * len(augment_normal_img))], dim=0)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(augment_fewshot_img, augment_fewshot_mask, augment_fewshot_label)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "# memory bank construction\n",
    "support_dataset = torch.utils.data.TensorDataset(augment_normal_img)\n",
    "support_loader = torch.utils.data.DataLoader(support_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "best_result = 0\n",
    "\n",
    "for epoch in range(args.epoch):\n",
    "        print('epoch ', epoch, ':')\n",
    "\n",
    "        loss_list = []\n",
    "        for (image, gt, label) in train_loader:\n",
    "            image = image.to(device)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                _, seg_patch_tokens, det_patch_tokens = model(image)\n",
    "                # seg_patch_tokens size { [batch_size,196,512] * 4} \n",
    "                seg_patch_tokens = [p[0, 1:, :] for p in seg_patch_tokens]\n",
    "                det_patch_tokens = [p[0, 1:, :] for p in det_patch_tokens]\n",
    "\n",
    "                # det loss\n",
    "                det_loss = 0\n",
    "                image_label = label.to(device)\n",
    "                for layer in range(len(det_patch_tokens)):\n",
    "                    det_patch_tokens[layer] = det_patch_tokens[layer] / det_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "                    anomaly_map = (100.0 * det_patch_tokens[layer] @ text_features).unsqueeze(0)    \n",
    "                    anomaly_map = torch.softmax(anomaly_map, dim=-1)[:, :, 1]\n",
    "                    anomaly_score = torch.mean(anomaly_map, dim=-1)\n",
    "                    det_loss += loss_bce(anomaly_score, image_label)\n",
    "\n",
    "                if CLASS_INDEX[args.obj] > 0:\n",
    "                    # pixel level\n",
    "                    seg_loss = 0\n",
    "                    mask = gt.squeeze(0).to(device)\n",
    "                    mask[mask > 0.5], mask[mask <= 0.5] = 1, 0\n",
    "                    for layer in range(len(seg_patch_tokens)):\n",
    "                        seg_patch_tokens[layer] = seg_patch_tokens[layer] / seg_patch_tokens[layer].norm(dim=-1, keepdim=True)\n",
    "                        anomaly_map = (100.0 * seg_patch_tokens[layer] @ text_features).unsqueeze(0)\n",
    "                        B, L, C = anomaly_map.shape\n",
    "                        H = int(np.sqrt(L))\n",
    "                        anomaly_map = F.interpolate(anomaly_map.permute(0, 2, 1).view(B, 2, H, H),\n",
    "                                                    size=args.img_size, mode='bilinear', align_corners=True)\n",
    "                        anomaly_map = torch.softmax(anomaly_map, dim=1)\n",
    "                        seg_loss += loss_focal(anomaly_map, mask)\n",
    "                        seg_loss += loss_dice(anomaly_map[:, 1, :, :], mask)\n",
    "                    \n",
    "                    loss = seg_loss + det_loss\n",
    "                    loss.requires_grad_(True)\n",
    "                    seg_optimizer.zero_grad()\n",
    "                    det_optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    seg_optimizer.step()\n",
    "                    det_optimizer.step()\n",
    "\n",
    "                else:\n",
    "                    loss = det_loss\n",
    "                    loss.requires_grad_(True)\n",
    "                    det_optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    det_optimizer.step()\n",
    "\n",
    "                loss_list.append(loss.item())\n",
    "\n",
    "        print(\"Loss: \", np.mean(loss_list))\n",
    "\n",
    "\n",
    "        seg_features = []\n",
    "        det_features = []\n",
    "        for image in support_loader:\n",
    "            image = image[0].to(device)\n",
    "            with torch.no_grad():\n",
    "                _, seg_patch_tokens, det_patch_tokens = model(image)\n",
    "\n",
    "                seg_patch_tokens = [p[0].contiguous() for p in seg_patch_tokens]\n",
    "                det_patch_tokens = [p[0].contiguous() for p in det_patch_tokens]\n",
    "                seg_features.append(seg_patch_tokens)\n",
    "                det_features.append(det_patch_tokens)\n",
    "        seg_mem_features = [torch.cat([seg_features[j][i] for j in range(len(seg_features))], dim=0) for i in range(len(seg_features[0]))]\n",
    "        det_mem_features = [torch.cat([det_features[j][i] for j in range(len(det_features))], dim=0) for i in range(len(det_features[0]))]\n",
    "        \n",
    "\n",
    "        result = test(args, model, test_loader, text_features, seg_mem_features, det_mem_features)\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            print(\"Best result\\n\")\n",
    "            if args.save_model == 1:\n",
    "                ckp_path = os.path.join(args.save_path, f'{args.obj}.pth')\n",
    "                torch.save({'seg_adapters': model.seg_adapters.state_dict(),\n",
    "                            'det_adapters': model.det_adapters.state_dict()}, \n",
    "                            ckp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MVFA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
